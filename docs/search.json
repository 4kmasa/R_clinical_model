[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R语言实战临床预测模型",
    "section": "",
    "text": "前言\n谨以此书献给我不务正业的研究生生涯。"
  },
  {
    "objectID": "index.html#本书缘起",
    "href": "index.html#本书缘起",
    "title": "R语言实战临床预测模型",
    "section": "本书缘起",
    "text": "本书缘起\n我在2019年接触到R语言和临床预测模型，彼时还是“简单的纯生信也能随便发SCI的上古时代”，当时的临床预测模型并不像现在这么火爆，连培训班也只有零星的几个，收费不过1000块左右。\n然而随着大家愈加内卷，各种各样的培训班也越来越多，价格也是水涨船高，竟然都要3000+，甚至8000+了，真是离谱！但是内容并没有什么新意，无非就是列线图/ROC曲线/C-index/NRI/IDI/校准曲线/决策曲线等等。\n回想自己的学习经历，这些东西无非就是各种R语言操作而已，本人的公众号也一直向大家免费提供这些教程。这部分内容已经积累了几十篇推文，是时候做个总结了，我把这些内容整理在一起，方便有需要的人学习。\n本书github地址：https://github.com/ayueme/R_clinical_model"
  },
  {
    "objectID": "index.html#书籍简介",
    "href": "index.html#书籍简介",
    "title": "R语言实战临床预测模型",
    "section": "书籍简介",
    "text": "书籍简介\n本书主要介绍R语言在临床预测模型中的应用，重实践，少理论，全书只有少量内容是理论，其余部分都是R语言实操。\n临床预测模型和统计学以及机器学习交叉很多，本书虽然是R语言实现临床预测模型的入门书籍，但在阅读本书前，还需要你已经对临床预测模型、统计学、机器学习具有一定的了解。\n\n\n\n\n\n\n提醒\n\n\n\n本书不适合R语言零基础的人。\n如果你是刚入门的小白，我首先推荐你了解下R语言的基础知识，比如R语言和R包安装（初学者可参考附录）、Rstudio的界面、R语言中数据类型（向量、矩阵、数据框、列表等）、R语言中的数据导入导出、R语言的基础数据操作等。\n对于医学生/医生，我比较推荐先看《R语言实战》，再看《R数据科学》，无需全部了解，只需要熟悉其中的基础知识即可。\n\n\n本书内容主要涉及模型建立、模型评价、模型比较3部分内容，其中模型建立和模型评价内容占比较多，模型比较部分主要是几个综合性R包的使用，简化多模型比较的流程。变量筛选内容较多，我把它单独放在一个章节中。对于临床预测模型中常见的列线图、C-index、ROC曲线、校准曲线、决策曲线、临床影响曲线、NRI、IDI等内容，皆进行了详细的操作演示，同时提供多种实现方法。\n对于一些比较火爆的机器学习方法，如随机生存森林、生存支持向量机、提升法、神经网络等内容，本书并未进行介绍，公众号会逐步更新这部分内容（随机生存森林、生存支持向量机已完结），需要的朋友可关注公众号：医学和生信笔记。\n\n\n\n\n\n\n注意\n\n\n\n本书实际上是我公众号历史推文的整理和汇总（部分内容有改动），书中涉及的所有数据都可以在相关历史推文中免费获取！推文合集链接：临床预测模型\n我也准备了一个PDF版合集，内容和网页版一致，只是打包了所有的数据，付费获取（10元），介意勿扰！PDF版合集获取链接：R语言实战临床预测模型\n\n\n限于本人水平等问题，难免会有一些错误，欢迎大家以各种方式批评指正，比如公众号留言、粉丝QQ群、github、个人微信等。\n本书会不定期更新，内容和格式都会不断完善。\n更新日志：\n\n20231230：优化内容结构和章节，增加变量筛选；增加列线图和决策曲线相关内容等\n20231015：首次上传"
  },
  {
    "objectID": "index.html#作者简介",
    "href": "index.html#作者简介",
    "title": "R语言实战临床预测模型",
    "section": "作者简介",
    "text": "作者简介\n\n阿越，外科医生，R语言爱好者，长期分享R语言和医学统计学、临床预测模型、生信数据挖掘、R语言机器学习等知识。\n公众号：医学和生信笔记\n知乎：医学和生信笔记\nCSDN：医学和生信笔记\n哔哩哔哩：阿越就是我\nGithub：ayueme"
  },
  {
    "objectID": "clinmodel-definition.html#简单理解",
    "href": "clinmodel-definition.html#简单理解",
    "title": "1  什么是临床预测模型？",
    "section": "1.1 简单理解",
    "text": "1.1 简单理解\n临床预测模型，初听这个名字，或许会觉得很高大上，其实没那么复杂，你可以理解成一种方法，这种方法可以预测一个人到底是有病还是没病，或者预测一个人一段时间后会不会死，或者预测一个人的某种疾病会不会复发，又或者是预测一个样本到底是肿瘤还是正常组织……\n再直白一点，临床预测模型是一个公式，根据这个公式，你提供一些基本信息，比如年龄、性别、体重、血红蛋白量等（或者某个基因的表达量等），就可以计算出这个人到底是有病还是没病！\n目前很多疾病都需要做磁共振、做CT、病理才能确诊，假如你发现了一个公式，只要验个血，得到几个生化指标，就能根据你的公式算出来这个人到底是有病还是没病！这不比CT、磁共振、病理简单多了？值得推广。\n所以，临床预测模型的本质是一种分类方法。通过这种方法，你可以对很多东西进行分类，比如，生和死、有病和没病、肿瘤和非肿瘤、复发和不复发等等。\n既然是一种方法，那肯定就有准确和不准确，看名字也能知道，这只是一种预测，或者叫：猜（有根据的猜）！ 如果你这种方法能和金标准相提并论，那说明你的方法很牛，如果恰好你的方法更加简单方便、经济适用，那你的方法真是太厉害了，非常有希望成为新的金标准！\n那如何评价你的方法好还是不好呢？这就是临床预测模型的评价，通过各种指标（后面会详细介绍）、从各种不同的角度评价。\n说了这么多，我怎么才能得到我的模型（或者叫方法）呢？这就是临床预测模型的另一个主要内容：临床预测模型的建立。\n前面说过，临床预测模型本质上就是一个公式而已！说个最简单的，逻辑回归（logistic），大家应该都知道怎么构建逻辑回归吧？不就是自变量和因变量吗。给你几个自变量，一个二分类的因变量，大家通过SPSS点点点，就可以得到各个自变量的系数，然后就能写出逻辑回归方程了。你的这个逻辑回归方程，这就是一个临床预测模型了！给你几个自变量的值，根据这个方程，你就可以算出因变量的值，然后就可以分类了！\n说到这里，相信你应该明白很多了！但是这还不够，你可能还听过什么机器学习、lasso、随机森林、支持向量机等等，别慌，这就是我们接下来要说的：临床预测模型和机器学习的关系。"
  },
  {
    "objectID": "clinmodel-definition.html#临床预测模型和机器学习",
    "href": "clinmodel-definition.html#临床预测模型和机器学习",
    "title": "1  什么是临床预测模型？",
    "section": "1.2 临床预测模型和机器学习",
    "text": "1.2 临床预测模型和机器学习\n机器学习，是不是听上去也高大上，但是对于学习临床医学的我们来说，不需要知道的太彻底，大概明白是什么就够了。\n逻辑回归也是机器学习的一种，随机森林、决策树、支持向量机、lasso、岭回归、弹性网络、xgboost等等，这些都是和逻辑回归一样，就是不同的方法而已！\n学过医学统计学的都知道（没学过可能也知道），如果因变量是连续性变量，那么我们就用多元线性回归，如果因变量是二分类变量，就用logistic回归（分类）。回归和分类，刚好就是机器学习的两个主要任务。很多方法，比如随机森林，既可以做回归，又可以做分类，而且准确度还很高，这就是为什么大家喜欢用其他方法的原因，主要是为了提高准确性。\n临床预测模型，只是机器学习在医学领域的应用之一，回归和分类，适用于各行各业，所以在很多领域你都听过机器学习这几个字。此外，还有深度学习、人工智能等等，这些都可以简单的理解为更加牛逼的方法！\n这些不同的方法都有各自适合的场景，在合适的场景下才能得到最好的表现，如何让模型表现的更好，那就需要学习一些机器学习的基本知识了，这些东西在bilibili一搜一大堆，大家可以自行学习，不过千万不要太沉迷哟！\n但是你一搜机器学习教程，出来的都是推荐你吴恩达、西瓜书等内容，我是不太推荐的，这些东西不是给生物医药领域的人看的，你看这些，可能就是听天书，毕竟很多医学生，连高数都是不学的！我比较推荐statquest，b站也可以搜到，这是一个国外的生物统计教授的课程，他的风格更适合我们。不过在学习这些这些之前，希望你已经学会了书本中常见的医学统计知识。"
  },
  {
    "objectID": "clinmodel-definition.html#临床预测模型和统计学",
    "href": "clinmodel-definition.html#临床预测模型和统计学",
    "title": "1  什么是临床预测模型？",
    "section": "1.3 临床预测模型和统计学",
    "text": "1.3 临床预测模型和统计学\n我们学过的医学统计学，在某些方面和机器学习是有交集的。比如，逻辑回归、多元线性回归，既是统计学方法，也是机器学习算法，这并不冲突，就像一个人在不同场合有不同身份一样。\n在谈临床预测模型时，我们可能是偏向于机器学习多一点的，毕竟用到的很多方法，都是来自于机器学习领域。\n你可能见到在很多生信文章中，使用一个模型并没有提前检验各种条件，直接就用了。但在医学统计学中，很多方法都是有适用条件的，符合条件才能用。哪种才是正确的呢？\n其实不用纠结，别人能用你也能用，多看文章，你能发现各种用法，但是别人依然发了SCI，你也可以。如果非要说区别，这就涉及到频率学派和贝叶斯学派这些东西了，咱也不是很懂了，如果你有兴趣，可以自己探索。如果就是为了发文章，那就别搞这些没用的了，多看几篇高分SCI，跟着里面的思路模仿吧！\n读到这里，你应该大致了解临床预测模型，不致于云里雾里了。但是光说不练是假把式，还是希望你能多读几篇相关的文献。"
  },
  {
    "objectID": "nomogram-logistic.html#准备r包和数据",
    "href": "nomogram-logistic.html#准备r包和数据",
    "title": "2  logistic回归列线图绘制",
    "section": "2.1 准备R包和数据",
    "text": "2.1 准备R包和数据\n\ninstall.packages(\"rms\")\ninstall.packages(\"DynNom\")\ninstall.packages(\"regplot\")\ndevtools::install_local(\"D:/R/R包/VRPM_1.2.tar.gz\") # 需要下载压缩包本地安装\n\n使用lowbirth数据集，这个数据集是关于低出生体重儿是否会死亡的数据集，其中dead这一列是结果变量，0代表存活，1代表死亡，其余列都是预测变量。\n\n注意：需要把分类变量因子化，对于无序分类变量，需要设置哑变量！\n\n\nrm(list = ls())\nlowbirth &lt;- read.csv(\"./datasets/lowbirth.csv\")\n\n查看一下数据：\n\ndim(lowbirth) # 565行，10列\n## [1] 565  10\nstr(lowbirth) \n## 'data.frame':    565 obs. of  10 variables:\n##  $ birth   : num  81.5 81.6 81.6 81.6 81.6 ...\n##  $ lowph   : num  7.25 7.06 7.25 6.97 7.32 ...\n##  $ pltct   : int  244 114 182 54 282 153 229 182 361 378 ...\n##  $ race    : chr  \"white\" \"black\" \"black\" \"black\" ...\n##  $ bwt     : int  1370 620 1480 925 1255 1350 1310 1110 1180 970 ...\n##  $ delivery: chr  \"abdominal\" \"vaginal\" \"vaginal\" \"abdominal\" ...\n##  $ apg1    : int  7 1 8 5 9 4 6 6 6 2 ...\n##  $ vent    : int  0 1 0 1 0 0 1 0 0 1 ...\n##  $ sex     : chr  \"female\" \"female\" \"male\" \"female\" ...\n##  $ dead    : int  0 1 0 1 0 0 0 0 0 1 ...\n\n简单的把人种分为白色和黑色人种（无序分类变量需要设置哑变量），再去掉race这一列，然后其余分类变量因子化。\n\nlibrary(dplyr)\n\ntmp &lt;- lowbirth %&gt;% \n  mutate(across(where(is.character),as.factor),\n         vent = factor(vent),\n         black = ifelse(race == \"black\",1,0),\n         white = ifelse(race == \"white\",1,0),\n         other = ifelse(race %in% c(\"native American\",\"oriental\"),1,0)\n         ) %&gt;% \n  select(- race)\n\nglimpse(tmp)\n## Rows: 565\n## Columns: 12\n## $ birth    &lt;dbl&gt; 81.514, 81.552, 81.558, 81.593, 81.610, 81.624, 81.626, 81.68…\n## $ lowph    &lt;dbl&gt; 7.250000, 7.059998, 7.250000, 6.969997, 7.320000, 7.160000, 7…\n## $ pltct    &lt;int&gt; 244, 114, 182, 54, 282, 153, 229, 182, 361, 378, 255, 186, 26…\n## $ bwt      &lt;int&gt; 1370, 620, 1480, 925, 1255, 1350, 1310, 1110, 1180, 970, 770,…\n## $ delivery &lt;fct&gt; abdominal, vaginal, vaginal, abdominal, vaginal, abdominal, v…\n## $ apg1     &lt;int&gt; 7, 1, 8, 5, 9, 4, 6, 6, 6, 2, 4, 8, 1, 8, 5, 9, 9, 9, 6, 2, 1…\n## $ vent     &lt;fct&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1…\n## $ sex      &lt;fct&gt; female, female, male, female, female, female, male, male, mal…\n## $ dead     &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n## $ black    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0…\n## $ white    &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1…\n## $ other    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "nomogram-logistic.html#方法1rms",
    "href": "nomogram-logistic.html#方法1rms",
    "title": "2  logistic回归列线图绘制",
    "section": "2.2 方法1：rms",
    "text": "2.2 方法1：rms\n\nlibrary(rms)\n## Loading required package: Hmisc\n## \n## Attaching package: 'Hmisc'\n## The following objects are masked from 'package:dplyr':\n## \n##     src, summarize\n## The following objects are masked from 'package:base':\n## \n##     format.pval, units\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\n然后是打包数据，这一步对于rms包来说是必须的：\n\ndd &lt;- datadist(tmp)\noptions(datadist=\"dd\")\n\n构建模型：\n\nfit1 &lt;- lrm(dead ~ birth + lowph + pltct + bwt + delivery + apg1 +\n            vent + sex + black + white,\n            data = tmp,x=T,y=T)\n\n接下来就是构建列线图模型，然后画图。\n\nnom1 &lt;- nomogram(fit1, fun=plogis,\n                 fun.at=c(0.001,0.1,0.25,0.5,0.75,0.9,0.99),\n                 lp=T, # 是否显示线性概率\n                 funlabel=\"Risk of Death\")  \nplot(nom1) \n\n\n\n\n从这个图来看，sex、delivery、apg1对模型的贡献很小，几乎可以忽略不计，下面我们去掉这两个变量再看看。\n\nfit2 &lt;- lrm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp,x=T,y=T)\nnom2 &lt;- nomogram(fit2, fun=plogis,\n                 fun.at=c(0.001,0.01,0.1,0.25,0.5,0.75,0.9,0.99),\n                 lp=T, \n                 maxscale = 100, # 最大得分数\n                 conf.int = F, # 添加置信区间，很难看，可以不要\n                 funlabel=\"Dead\")  \nplot(nom2,\n     col.grid=c(\"tomato\",\"grey\")\n     #conf.space = c(0.3,0.5) # 置信区间位置\n     )"
  },
  {
    "objectID": "nomogram-logistic.html#方法2dynnom",
    "href": "nomogram-logistic.html#方法2dynnom",
    "title": "2  logistic回归列线图绘制",
    "section": "2.3 方法2：DynNom",
    "text": "2.3 方法2：DynNom\n使用这种方法会在你的Rstudio中弹出一个窗口，你可以自由调节其中给出的选项。\n\nlibrary(DynNom)\n\nfit2 &lt;- glm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp, family = binomial)\nDynNom(fit2,DNtitle = \"nomogram\",DNxlab = \"probability\")\n\n选择好你的参数，点击Predict即可出图：\n\n\n\n\n\n\n\n注意\n\n\n\n仔细看上面这个图其实有错误，black和white作为性别，只有2种可能，不可能为小数，这就提醒我们在建立模型时需要把这样的变量先因子化。"
  },
  {
    "objectID": "nomogram-logistic.html#方法3regplot",
    "href": "nomogram-logistic.html#方法3regplot",
    "title": "2  logistic回归列线图绘制",
    "section": "2.4 方法3：regplot",
    "text": "2.4 方法3：regplot\n\nlibrary(regplot)\n\nfit2 &lt;- lrm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp,x=T,y=T)\nregplot(fit2,\n        #连续性变量形状，\"no plot\"\"density\"\"boxes\"\"ecdf\"\n        #\"bars\"\"boxplot\"\"violin\"\"bean\" \"spikes\"；\n        #分类变量的形状，可选\"no plot\" \"boxes\" \"bars\" \"spikes\"\n        plots = c(\"violin\", \"boxes\"),   \n        observation = tmp[1,], #用哪行观测，或者T F\n        center = T, # 对齐变量\n        subticks = T,\n        droplines = T,#是否画竖线\n        title = \"nomogram\",\n        points = T, # 截距项显示为0-100\n        odds = T, # 是否显示OR值\n        showP = T, # 是否显示变量的显著性标记\n        rank = \"sd\", # 根据sd给变量排序\n        interval=\"confidence\", # 展示可信区间\n        clickable = F # 是否可以交互\n        )\n## Regression  fit2 lrm formula:\n## dead `~` birth + lowph + pltct + bwt + vent + black + white\n## CI: 0.00496(0.00106,0.0233)\n## [[1]]\n##   white Points\n## 1   0.0     26\n## 2   0.4     34\n## 3   0.8     42\n## \n## [[2]]\n##   black Points\n## 1   0.0     17\n## 2   0.4     29\n## 3   0.8     40\n## \n## [[3]]\n##       vent Points\n## vent1    0     34\n## vent2    1     99\n## \n## [[4]]\n##    bwt Points\n## 1  400     87\n## 2  600     72\n## 3  800     57\n## 4 1000     42\n## 5 1200     27\n## 6 1400     12\n## \n## [[5]]\n##   pltct Points\n## 1     0     42\n## 2   300     30\n## 3   600     18\n## \n## [[6]]\n##    lowph Points\n## 1    6.5    103\n## 2    6.6     93\n## 3    6.7     83\n## 4    6.8     74\n## 5    6.9     64\n## 6    7.0     54\n## 7    7.1     44\n## 8    7.2     34\n## 9    7.3     25\n## 10   7.4     15\n## 11   7.5      5\n## 12   7.6     -5\n## \n## [[7]]\n##   birth Points\n## 1  81.5     43\n## 2  84.5     34\n## 3  87.5     26\n## \n## [[8]]\n##   Total Points    Pr(  )\n## 1          100 3.798e-05\n## 2          150 3.144e-04\n## 3          200 2.598e-03\n## 4          250 2.112e-02\n## 5          300 1.516e-01\n## 6          350 5.967e-01\n## 7          400 9.245e-01\n## 8          450 9.902e-01"
  },
  {
    "objectID": "nomogram-logistic.html#方法4vrpm",
    "href": "nomogram-logistic.html#方法4vrpm",
    "title": "2  logistic回归列线图绘制",
    "section": "2.5 方法4：VRPM",
    "text": "2.5 方法4：VRPM\n\nlibrary(VRPM)\n\nfit2 &lt;- glm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp, family = binomial)\n\n# 图片保存在你的目录下\ncolplot(fit2,coloroptions = 3)\n\n\n以上就是4种Cox回归列线图绘制的方法，其中方法1和3是最常用的。\n获取lowbirth数据请在公众号：医学和生信笔记 后台回复20220520。"
  },
  {
    "objectID": "nomogram-cox.html#加载数据和r包",
    "href": "nomogram-cox.html#加载数据和r包",
    "title": "3  Cox回归列线图绘制",
    "section": "3.1 加载数据和R包",
    "text": "3.1 加载数据和R包\nCox回归模型在医学统计中是一个很重要的统计方法，关于Cox比例风险模型，我写过一些实现方法的推文，大家可以参考：\n\nR语言生存分析：Cox回归\nR语言时依系数和时依协变量Cox回归\n\n\n# 加载需要的R包和数据\nlibrary(survival)\nlibrary(rms)\n## Loading required package: Hmisc\n## \n## Attaching package: 'Hmisc'\n## The following objects are masked from 'package:base':\n## \n##     format.pval, units\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\nrm(list = ls())\n\ndim(lung)\n## [1] 228  10\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ..."
  },
  {
    "objectID": "nomogram-cox.html#方法1rms",
    "href": "nomogram-cox.html#方法1rms",
    "title": "3  Cox回归列线图绘制",
    "section": "3.2 方法1：rms",
    "text": "3.2 方法1：rms\n大多数情况下都是使用1代表死亡，0代表删失，下面这个演示数据集用2代表死亡。在这里没有影响，但有的R包会报错，需要注意！\n\n# 使用rms包需要对数据进行“打包”操作\ndd &lt;- datadist(lung)\noptions(datadist = \"dd\")\n\n构建cox比例风险模型：\n\ncoxfit &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n              data = lung, x=T,y=T,surv = T\n              )\n\n# 构建生存函数，注意你的最大生存时间\nsurv &lt;- Survival(coxfit) \nsurv1 &lt;- function(x) surv(365,x) # 1年OS\nsurv2 &lt;- function(x) surv(365*2,x) # 2年OS\n\nnom &lt;- nomogram(coxfit,\n                fun = list(surv1,surv2),\n                lp = T,\n                funlabel = c('1-year survival Probability',\n                         '2-year survival Probability'),\n                maxscale = 100,\n                fun.at = c(0.95,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1))\n\n然后就是画图：\n\nplot(nom, \n     lplabel=\"Linear Predictor\",\n     xfrac = 0.2, # 左侧标签距离坐标轴的距离\n     #varname.label = TRUE, \n     tcl = -0.2, # 刻度长短和方向 \n     lmgp = 0.1, # 坐标轴标签距离坐标轴远近\n     points.label ='Points', \n     total.points.label = 'Total Points',\n     cap.labels = FALSE,\n     cex.var = 1, # 左侧标签字体大小\n     cex.axis = 1, # 坐标轴字体大小\n     col.grid = gray(c(0.8, 0.95))) # 竖线颜色"
  },
  {
    "objectID": "nomogram-cox.html#方法2dynnom",
    "href": "nomogram-cox.html#方法2dynnom",
    "title": "3  Cox回归列线图绘制",
    "section": "3.3 方法2：DynNom",
    "text": "3.3 方法2：DynNom\n使用这种方法会在你的Rstudio中弹出一个窗口，你可以自由调节其中给出的选项。\n\nlibrary(DynNom)\n\ncoxfit &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n              data = lung, x=T,y=T,surv = T\n              )\n\nDynNom(coxfit,\n       DNxlab = \"Survival probability\",\n       KMtitle=\"Kaplan-Meier plot\", \n       KMxlab = \"Time (Days)\", \n       KMylab = \"Survival probability\")\n\n选择好你的参数，点击Predict即可出图：\n\n仔细看上面这个图其实有错误，sex作为性别，只有2种可能：男或女，所以只能是1或者2，不可能为小数，这就提醒我们在建立模型时需要把这样的变量先因子化。"
  },
  {
    "objectID": "nomogram-cox.html#方法3regplot",
    "href": "nomogram-cox.html#方法3regplot",
    "title": "3  Cox回归列线图绘制",
    "section": "3.4 方法3：regplot",
    "text": "3.4 方法3：regplot\n使用regplot包实现，这个包很强大，以后还会经常用到。\n\nlibrary(regplot)\n\ncoxfit &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n              data = lung, x=T,y=T,surv = T\n              )\n\nregplot(coxfit,\n        #连续性变量形状，\"no plot\"\"density\"\"boxes\"\"ecdf\"\n        #\"bars\"\"boxplot\"\"violin\"\"bean\" \"spikes\"；\n        #分类变量的形状，可选\"no plot\" \"boxes\" \"bars\" \"spikes\"\n        plots = c(\"violin\", \"boxes\"), \n        observation = lung[1,], #用哪行观测，或者T F\n        center = T, # 对齐变量\n        subticks = T,\n        droplines = T,#是否画竖线\n        title = \"nomogram\",\n        points = T, # 截距项显示为0-100\n        odds = T, # 是否显示OR值\n        showP = T, # 是否显示变量的显著性标记\n        rank = \"sd\", # 根据sd给变量排序\n        interval=\"confidence\", # 展示可信区间\n        clickable = F # 是否可以交互\n        )\n## Regression  coxfit cph formula:\n## Surv(time, status) `~` age + sex + ph.ecog + ph.karno + pat.karno\n## CI: 0.931(4.82,57.5)\n## [[1]]\n##   pat.karno Points\n## 1        30     61\n## 2        50     49\n## 3        70     37\n## 4        90     25\n## \n## [[2]]\n##   ph.karno Points\n## 1       50      1\n## 2       60     11\n## 3       70     20\n## 4       80     29\n## 5       90     39\n## 6      100     48\n## \n## [[3]]\n##   ph.ecog Points\n## 1     0.0      0\n## 2     0.5     17\n## 3     1.0     33\n## 4     1.5     50\n## 5     2.0     67\n## 6     2.5     83\n## 7     3.0    100\n## \n## [[4]]\n##   sex Points\n## 1 1.0     45\n## 2 1.4     31\n## 3 1.8     18\n## \n## [[5]]\n##   age Points\n## 1  35     13\n## 2  45     20\n## 3  55     26\n## 4  65     33\n## 5  75     40\n## 6  85     47\n## \n## [[6]]\n##    Total Points Pr( time &lt; 267 )\n## 1            60           0.0889\n## 2            80           0.1225\n## 3           100           0.1676\n## 4           120           0.2271\n## 5           140           0.3034\n## 6           160           0.3981\n## 7           180           0.5097\n## 8           200           0.6324\n## 9           220           0.7546\n## 10          240           0.8609\n## 11          260           0.9373"
  },
  {
    "objectID": "nomogram-cox.html#方法4vrpm",
    "href": "nomogram-cox.html#方法4vrpm",
    "title": "3  Cox回归列线图绘制",
    "section": "3.5 方法4：VRPM",
    "text": "3.5 方法4：VRPM\n\nlibrary(VRPM)\nlibrary(survival)\n\ncox_fit &lt;- coxph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n                 data = lung,\n                 model = T)\n\n# 图片保存在你的目录下\ncolplot(cox_fit,coloroptions = 3,filename = \"cox.png\")\n\n\n以上就是4种Cox回归列线图绘制的方法，其中方法1和3是最常用的。"
  },
  {
    "objectID": "nomogram-essential.html#列线图与模型的关系",
    "href": "nomogram-essential.html#列线图与模型的关系",
    "title": "4  列线图的本质",
    "section": "4.1 列线图与模型的关系",
    "text": "4.1 列线图与模型的关系\n列线图又叫诺莫图，nomogram plot，前面我们展示了绘制二分类变量（logistic）和生存资料（Cox）列线图的多种方法，不知道大家有没有考虑过，其他模型可以绘制列线图吗？例如lasso回归、随机森林等。这就涉及到列线图到底是怎么绘制出来的。\n对于一个含有多个自变量和1个因变量的逻辑回归来说，回归方程可以写成类似 y=a + b1x1 + b2x2 + b3x3 这种形式，其中b是回归系数。列线图就是把回归方程用图形的方式展现出来，线段的长短（分数）根据回归系数计算。"
  },
  {
    "objectID": "nomogram-essential.html#列线图分数的计算方法",
    "href": "nomogram-essential.html#列线图分数的计算方法",
    "title": "4  列线图的本质",
    "section": "4.2 列线图分数的计算方法",
    "text": "4.2 列线图分数的计算方法\n以下面这个列线图为例：\n\n上面这个列线图是一个逻辑回归的（之前logistic回归列线图推文中的fit2），它的逻辑回归的结果是这样的：\n\nlrm(formula = dead ~ birth + lowph + pltct + bwt + vent + black + \n     white, data = tmp, x = T, y = T)                    \n \n           Coef    S.E.    Wald Z Pr(&gt;|Z|)\n Intercept 38.3815 11.0303  3.48  0.0005  \n birth     -0.1201  0.0914 -1.31  0.1890  \n lowph     -4.1451  1.1881 -3.49  0.0005  \n pltct     -0.0017  0.0019 -0.91  0.3644  \n bwt       -0.0031  0.0006 -5.14  &lt;0.0001 \n vent=1     2.7526  0.7436  3.70  0.0002  \n black      1.1974  0.8448  1.42  0.1564  \n white      0.8597  0.8655  0.99  0.3206\n\n在最下面列出了每个自变量的回归系数，我们首先 把回归系数绝对值最大的设置为100分，在这个例子中是lowph，它的回归系数绝对值是4.1451,也就是对应的是100分，然后其他自变量都是根据lowph进行转换 ，即可算出其他自变量对应的得分！\n然后根据这个表达式得到的列线图的结果是这样的：\n\nnom2\nPoints per unit of linear predictor: 21.93145 \nLinear predictor units per point   : 0.04559661 \n\n\n birth Points\n 81.5  16    \n 82.0  14    \n 82.5  13    \n 83.0  12    \n 83.5  11    \n 84.0   9    \n 84.5   8    \n 85.0   7    \n 85.5   5    \n 86.0   4    \n 86.5   3    \n 87.0   1    \n 87.5   0    \n\n\n lowph Points\n 6.5   100   \n 6.6    91   \n 6.7    82   \n 6.8    73   \n 6.9    64   \n 7.0    55   \n 7.1    45   \n 7.2    36   \n 7.3    27   \n 7.4    18   \n 7.5     9   \n 7.6     0   \n\n\n pltct Points\n   0   22    \n  50   20    \n 100   18    \n 150   17    \n 200   15    \n 250   13    \n 300   11    \n 350    9    \n 400    7    \n 450    6    \n 500    4    \n 550    2    \n 600    0    \n\n\n bwt  Points\n  400 76    \n  500 69    \n  600 62    \n  700 55    \n  800 48    \n  900 41    \n 1000 34    \n 1100 27    \n 1200 21    \n 1300 14    \n 1400  7    \n 1500  0    \n\n\n vent Points\n 0     0    \n 1    60    \n\n\n black Points\n 0      0    \n 1     26    \n\n\n white Points\n 0      0    \n 1     19    \n\n\n Total Points  Dead\n           53 0.001\n          104 0.010\n          157 0.100\n          181 0.250\n          205 0.500\n          229 0.750\n          253 0.900\n\n上面的 Linear predictor units per point: 0.04559661  意思是每一个小刻度代表的分数是0.04559661分，这实际上是最大回归系数的1/100(这里应该是4.1451/100=0.041451，实际上会有偏差！)。\n\n4.2.1 分类变量分数的计算\n以vent为例，vent=1的时候，它的回归系数是2.7526，那么它对应的分数应该是 2.7526/0.04559661 * 100 ≈ 60，上面得到的结果是60分，一样的！black应该是 1.1974/0.04559661 * 100 ≈ 26，也和我们算的差不多！\n这就是分类变量分数的计算。\n\n\n4.2.2 连续性变量分数的计算\n连续性自变量需要考虑取值范围，它的解释应该是每增加一个单位，因变量变化多少，对于pltct来说，系数是-0.0017，就是每增加1各单位，因变量减少0.0017。\npltct是600，对应的分数是0分，那么如果是100，对应的分数就是 （600-100）* (0.0017/0.04559661) ≈ 18。\n这就是连续性变量分数的计算。\n了解了列线图的分数计算方法，即使没有R语言，你也可以通过手动计算算出来，这样你可以自己画图！（理论上可行，但实际上很难，至少我还没见到过…）\n根据这个思路，理论上凡是有回归系数的、有变量重要性的，都可以画列线图展示出来，并不局限于logistic和cox，但是模型在临床中的解释就很难了。"
  },
  {
    "objectID": "nomogram-rcs.html#建立logistic模型",
    "href": "nomogram-rcs.html#建立logistic模型",
    "title": "5  样条回归列线图绘制",
    "section": "5.1 建立logistic模型",
    "text": "5.1 建立logistic模型\n\nlibrary(rms)\n# 逻辑回归数据\n#getHdata(titanic3)\n#save(titanic3, file = \"./datasets/titanic3.rdata\")\nload(file = \"./datasets/titanic3.rdata\")\n# 使用rms前先把数据打包\ndd &lt;- datadist(titanic3); options(datadist='dd')\n\n# 逻辑回归的立方样条\nf &lt;- lrm(survived ~ rcs(sqrt(age),5) + sex, data=titanic3)\nf\n## Frequencies of Missing Values Due to Each Variable\n## survived      age      sex \n##        0      263        0 \n## \n## Logistic Regression Model\n## \n## lrm(formula = survived ~ rcs(sqrt(age), 5) + sex, data = titanic3)\n## \n## \n##                        Model Likelihood      Discrimination    Rank Discrim.    \n##                              Ratio Test             Indexes          Indexes    \n## Obs          1046    LR chi2     328.06      R2       0.363    C       0.794    \n##  0            619    d.f.             5     R2(5,1046)0.266    Dxy     0.587    \n##  1            427    Pr(&gt; chi2) &lt;0.0001    R2(5,758.1)0.347    gamma   0.593    \n## max |deriv| 2e-07                            Brier    0.168    tau-a   0.284    \n## \n##           Coef     S.E.    Wald Z Pr(&gt;|Z|)\n## Intercept   3.0936  0.5428   5.70 &lt;0.0001 \n## age        -0.6383  0.1771  -3.60 0.0003  \n## age'        1.5544  0.6527   2.38 0.0172  \n## age''     -12.1583  8.8925  -1.37 0.1715  \n## age'''     15.8326 16.9397   0.93 0.3500  \n## sex=male   -2.4944  0.1549 -16.10 &lt;0.0001"
  },
  {
    "objectID": "nomogram-rcs.html#画列线图",
    "href": "nomogram-rcs.html#画列线图",
    "title": "5  样条回归列线图绘制",
    "section": "5.2 画列线图",
    "text": "5.2 画列线图\n下面直接画图即可，没有任何难度，因为rms这个包把一切都给你做好了，不用自己操心，如果你做临床预测模型，是不可能绕开这个包的。\n\nnom &lt;- nomogram(f, fun=plogis,\n                 lp=T,\n                 funlabel=\"Risk of Death\")  \nplot(nom) \n\n\n\n\n这样RCS的列线图就画好了，关于一些参数的意义和细节的美化，可以参考前面的推文，这里就不多说了。\n既然logistic回归没问题，那COX回归自然也是没问题的！"
  },
  {
    "objectID": "nomogram-rcs.html#cox回归rcs的列线图",
    "href": "nomogram-rcs.html#cox回归rcs的列线图",
    "title": "5  样条回归列线图绘制",
    "section": "5.3 COX回归RCS的列线图",
    "text": "5.3 COX回归RCS的列线图\n\nlibrary(survival)\ndd &lt;- datadist(lung)\noptions(datadist = \"dd\")\n\n构建cox比例风险模型：\n\ncoxfit &lt;- cph(Surv(time, status) ~ rcs(sqrt(age),5) + sex,\n              data = lung, x=T,y=T,surv = T\n              )\n\n# 构建生存函数，注意你的最大生存时间\nsurv &lt;- Survival(coxfit) \nsurv1 &lt;- function(x) surv(365,x) # 1年OS\nsurv2 &lt;- function(x) surv(365*2,x) # 2年OS\n\nnom &lt;- nomogram(coxfit,\n                fun = list(surv1,surv2),\n                lp = T,\n                funlabel = c('1-year survival Probability',\n                         '2-year survival Probability')\n                )\n\nplot(nom)\n\n\n\n\n这就是COX回归RCS的列线图，关于一些参数的意义和细节的美化，可以参考前面的推文，这里就不多说了。\n是不是很简单？\n因为是演示数据，所以画出来的图不是很美观，但是实现方法就是这么简单！"
  },
  {
    "objectID": "nomogram-compete-risk.html#加载数据和r包",
    "href": "nomogram-compete-risk.html#加载数据和r包",
    "title": "6  竞争风险模型列线图绘制",
    "section": "6.1 加载数据和R包",
    "text": "6.1 加载数据和R包\n探讨骨髓移植和血液移植治疗白血病的疗效，结局事件定义为复发，某些患者因为移植不良反应死亡，定义为竞争风险事件。\n\nrm(list = ls())\ndata(\"bmtcrr\",package = \"casebase\")\nstr(bmtcrr)\n## 'data.frame':    177 obs. of  7 variables:\n##  $ Sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : Factor w/ 2 levels \"ALL\",\"AML\": 1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : Factor w/ 4 levels \"CR1\",\"CR2\",\"CR3\",..: 4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Status: int  2 1 0 2 2 2 0 2 0 1 ...\n##  $ Source: Factor w/ 2 levels \"BM+PB\",\"PB\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ftime : num  0.67 9.5 131.77 24.03 1.47 ...\n\n这个数据一共7个变量，177行。\n\nSex: 性别，F是女，M是男\nD: 疾病类型，ALL是急性淋巴细胞白血病，AML是急性髓系细胞白血病。\nPhase: 不同阶段，4个水平，CR1，CR2，CR3，Relapse。\nAge: 年龄。\nStatus: 结局变量，0=删失，1=复发，2=竞争风险事件。\nSource: 因子变量，2个水平：BM+PB(骨髓移植+血液移植)，PB(血液移植)。\nftime: 生存时间。\n\n\n# 竞争风险分析需要用的R包\nlibrary(cmprsk)\n## Loading required package: survival"
  },
  {
    "objectID": "nomogram-compete-risk.html#fine-gray检验单因素分析",
    "href": "nomogram-compete-risk.html#fine-gray检验单因素分析",
    "title": "6  竞争风险模型列线图绘制",
    "section": "6.2 Fine-Gray检验（单因素分析）",
    "text": "6.2 Fine-Gray检验（单因素分析）\n在普通的生存分析中，可以用log-rank检验做单因素分析，在竞争风险模型中，使用Fine-Gray检验进行单因素分析。\n\n比如现在我们想要比较不同疾病类型（D）有没有差异，可以进行Fine-Gray检验：\n\nbmtcrr$Status &lt;- factor(bmtcrr$Status)\nf &lt;- cuminc(bmtcrr$ftime, bmtcrr$Status, bmtcrr$D)\nf\n## Tests:\n##        stat         pv df\n## 1 2.8623325 0.09067592  1\n## 2 0.4481279 0.50322531  1\n## Estimates and Variances:\n## $est\n##              20        40        60        80       100       120\n## ALL 1 0.3713851 0.3875571 0.3875571 0.3875571 0.3875571 0.3875571\n## AML 1 0.2414530 0.2663827 0.2810390 0.2810390 0.2810390        NA\n## ALL 2 0.3698630 0.3860350 0.3860350 0.3860350 0.3860350 0.3860350\n## AML 2 0.4439103 0.4551473 0.4551473 0.4551473 0.4551473        NA\n## \n## $var\n##                20          40          60          80         100         120\n## ALL 1 0.003307032 0.003405375 0.003405375 0.003405375 0.003405375 0.003405375\n## AML 1 0.001801156 0.001995487 0.002130835 0.002130835 0.002130835          NA\n## ALL 2 0.003268852 0.003373130 0.003373130 0.003373130 0.003373130 0.003373130\n## AML 2 0.002430406 0.002460425 0.002460425 0.002460425 0.002460425          NA\n\n结果中1代表复发,2代表竞争风险事件。\n第一行统计量=2.8623325, P=0.09067592,表示在控制了竞争风险事件（即第二行计算的统计量和P值）后，两种疾病类型ALL和AML的累计复发风险无统计学差异P=0.09067592。\n第2行说明ALL和AML的累计竞争风险无统计学差异。\n$est表示估计的各时间点ALL和AML组的累计复发率与与累计竞争风险事件发生率（分别用1和2来区分，与第一行第二行一致）。\n$var表示估计的各时间点ALL和AML组的累计复发率与与累计竞争风险事件发生率的方差（分别用1和2来区分，与第一行第二行一致）。\n\n6.2.1 图形展示结果\n对于上述结果可以使用图形展示：\n\nplot(f,xlab = 'Month', ylab = 'CIF',lwd=2,lty=1,\n     col = c('red','blue','black','forestgreen'))\n\n\n\n\n图形解读：\n纵坐标表示累计发生率CIF，横坐标是时间。我们从ALL1对应的红色曲线和AML1对应的蓝色曲线可以得出，ALL组的复发风险较AML 组高，但无统计学意义，P=0.09067592。同理，ALL2对应的黑色曲线在AML2对应的草绿色曲线下方，我们可以得出，ALL组的竞争风险事件发生率较AML组低，同样无统计学意义，P=0.50322531。\n简单来讲，这个图可以用一句话来概括：在控制了竞争风险事件后，ALL和AML累计复发风险无统计学差异P=0.09067592。\n\n\n6.2.2 ggplot2\n这个图不好看，非常的不ggplot，所以我们要用ggplot2重新画它！所以首先要提取数据，因为数就是图，图就是数。但是万能的broom包竟然没有不能提取这个对象的数据，只能手动来，太不优雅了！\n\n# 提取数据\nALL1 &lt;- data.frame(ALL1_t = f[[1]][[1]], ALL1_C = f[[1]][[2]])\nAML1 &lt;- data.frame(AML1_t = f[[2]][[1]], AML1_C = f[[2]][[2]])\nALL2 &lt;- data.frame(ALL2_t = f[[3]][[1]], ALL2_C = f[[3]][[2]])\nAML2 &lt;- data.frame(AML2_t = f[[4]][[1]], AML2_C = f[[4]][[2]])\n\nlibrary(ggplot2)\n\nggplot()+\n  geom_line(data = ALL1, aes(ALL1_t,ALL1_C))+\n  geom_line(data = ALL2, aes(ALL2_t,ALL2_C))+\n  geom_line(data = AML1, aes(AML1_t,AML1_C))+\n  geom_line(data = AML2, aes(AML2_t,AML2_C))+\n  labs(x=\"month\",y=\"cif\")+\n  theme_bw()\n\n\n\n\n但是这种不好上色，所以我们美化一下，变成长数据再画图即可。\n\ntmp &lt;- data.frame(month = c(ALL1$ALL1_t,AML1$AML1_t,ALL2$ALL2_t,AML2$AML2_t),\n                  cif = c(ALL1$ALL1_C,AML1$AML1_C,ALL2$ALL2_C,AML2$AML2_C),\n                  type = rep(c(\"ALL1\",\"AML1\",\"ALL2\",\"AML2\"), c(58,58,58,88))\n                  )\n\nggplot(tmp, aes(month, cif))+\n  geom_line(aes(color=type, group=type),size=1.2)+\n  theme_bw()+\n  theme(legend.position = \"top\")\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead."
  },
  {
    "objectID": "nomogram-compete-risk.html#竞争风险模型多因素分析",
    "href": "nomogram-compete-risk.html#竞争风险模型多因素分析",
    "title": "6  竞争风险模型列线图绘制",
    "section": "6.3 竞争风险模型（多因素分析）",
    "text": "6.3 竞争风险模型（多因素分析）\n做完了单因素分析，再看看竞争风险模型的多因素分析。\n首先要把自变量单独放在一个数据框里，使用中发现一个问题，这里如果把分类变量变为因子型不会自动进行哑变量编码，所以需要手动进行哑变量编码！\n但是我这里偷懒了，并没有进行哑变量设置！实际中是需要的哦！！\n\ncovs &lt;- subset(bmtcrr, select = - c(ftime,Status))\ncovs[,c(1:3,5)] &lt;- lapply(covs[,c(1:3,5)],as.integer)\n\nstr(covs)\n## 'data.frame':    177 obs. of  5 variables:\n##  $ Sex   : int  2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : int  1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : int  4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Source: int  1 1 1 1 1 1 1 1 1 1 ...\n\n指定failcode=1, cencode=0, 分别代表结局事件1与截尾0，其他默认为竞争风险事件2。\n\n# 构建竞争风险模型\nf2 &lt;- crr(bmtcrr$ftime, bmtcrr$Status, covs, failcode=1, cencode=0)\nsummary(f2)\n## Competing Risks Regression\n## \n## Call:\n## crr(ftime = bmtcrr$ftime, fstatus = bmtcrr$Status, cov1 = covs, \n##     failcode = 1, cencode = 0)\n## \n##           coef exp(coef) se(coef)      z p-value\n## Sex     0.0494     1.051   0.2867  0.172 0.86000\n## D      -0.4860     0.615   0.3040 -1.599 0.11000\n## Phase   0.4144     1.514   0.1194  3.470 0.00052\n## Age    -0.0174     0.983   0.0118 -1.465 0.14000\n## Source  0.9526     2.592   0.5469  1.742 0.08200\n## \n##        exp(coef) exp(-coef)  2.5% 97.5%\n## Sex        1.051      0.952 0.599  1.84\n## D          0.615      1.626 0.339  1.12\n## Phase      1.514      0.661 1.198  1.91\n## Age        0.983      1.018 0.960  1.01\n## Source     2.592      0.386 0.888  7.57\n## \n## Num. cases = 177\n## Pseudo Log-likelihood = -267 \n## Pseudo likelihood ratio test = 23.6  on 5 df,\n\n结果解读：在控制了竞争分险事件后，phase变量，即疾病所处阶段是患者复发的独立影响因素(p =0.00052)。"
  },
  {
    "objectID": "nomogram-compete-risk.html#列线图",
    "href": "nomogram-compete-risk.html#列线图",
    "title": "6  竞争风险模型列线图绘制",
    "section": "6.4 列线图",
    "text": "6.4 列线图\nregplot包绘制列线图。但是它目前只适用coxph()、lm()和glm()返回的对象。\n因此我们需要对原数据集加权创建一个新数据集用于为竞争风险模型分析，使用mstate包中的crprep()创建加权数据集,然后使用coxph()对加权数据集进行竞争风险模型拟合，这样就可以画列线图了。\n首先是加载数据和R包：\n\nrm(list = ls())\ndata(\"bmtcrr\",package = \"casebase\") # 还是这个数据\n\nlibrary(mstate) # 加权用到的R包\n\nbmtcrr$id &lt;- 1:nrow(bmtcrr) # 创建id\n\n# phase变为2分类，不然列线图不好解释\nbmtcrr$Phase &lt;- factor(ifelse(bmtcrr$Phase==\"Relapse\",1,0)) \nstr(bmtcrr)\n## 'data.frame':    177 obs. of  8 variables:\n##  $ Sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : Factor w/ 2 levels \"ALL\",\"AML\": 1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : Factor w/ 2 levels \"0\",\"1\": 2 1 1 1 1 2 1 1 1 2 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Status: int  2 1 0 2 2 2 0 2 0 1 ...\n##  $ Source: Factor w/ 2 levels \"BM+PB\",\"PB\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ftime : num  0.67 9.5 131.77 24.03 1.47 ...\n##  $ id    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n然后是对原数据进行加权：\n\ndf.w &lt;- crprep(\"ftime\", \"Status\",\n               data=bmtcrr, \n               trans=c(1,2),# 要加权的变量，1表示结局事件，2表示竞争风险事件\n               cens=0, # 删失\n               id=\"id\",\n               \n               # 要保留的协变量\n               keep=c(\"Age\",\"Sex\",\"D\",\"Source\",\"Phase\"))\n\nhead(df.w)\n##   id Tstart Tstop status weight.cens Age Sex   D Source Phase count failcode\n## 1  1   0.00  0.67      2   1.0000000  48   M ALL  BM+PB     1     1        1\n## 2  1   0.67  9.50      2   1.0000000  48   M ALL  BM+PB     1     2        1\n## 3  1   9.50 13.07      2   0.9679938  48   M ALL  BM+PB     1     3        1\n## 4  1  13.07 17.23      2   0.8730924  48   M ALL  BM+PB     1     4        1\n## 5  1  17.23 20.83      2   0.8536904  48   M ALL  BM+PB     1     5        1\n## 6  1  20.83 28.53      2   0.8120469  48   M ALL  BM+PB     1     6        1\ndf.w$T&lt;- df.w$Tstop - df.w$Tstart\n\n上述代码已经创建一个加权数据集df.w，此时还需要选择failcode == 1的行，然后我们才可以在此数据集上使用coxph()函数进行竞争风险分析，不然最后画列线图会报错。\n\n# 参考资料\n# https://blog.csdn.net/zhongkeyuanchongqing/article/details/124086113\ndf.w2 &lt;- df.w[df.w$failcode == 1,]\n\n构建cox模型：\n\nm.crr&lt;- coxph(Surv(T,status==1)~Age+Sex+D+Source+Phase,\n             data=df.w2,\n             weight=weight.cens,\n             subset=failcode==1)\nsummary(m.crr)\n## Call:\n## coxph(formula = Surv(T, status == 1) ~ Age + Sex + D + Source + \n##     Phase, data = df.w2, weights = weight.cens, subset = failcode == \n##     1)\n## \n##   n= 686, number of events= 56 \n## \n##              coef exp(coef) se(coef) robust se      z Pr(&gt;|z|)    \n## Age      -0.02174   0.97850  0.01172   0.01208 -1.800 0.071914 .  \n## SexM      0.10551   1.11128  0.27981   0.29571  0.357 0.721247    \n## DAML     -0.53163   0.58764  0.29917   0.30613 -1.737 0.082450 .  \n## SourcePB  1.06564   2.90269  0.53453   0.56000  1.903 0.057051 .  \n## Phase1    1.06140   2.89040  0.27870   0.28129  3.773 0.000161 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##          exp(coef) exp(-coef) lower .95 upper .95\n## Age         0.9785     1.0220    0.9556     1.002\n## SexM        1.1113     0.8999    0.6225     1.984\n## DAML        0.5876     1.7017    0.3225     1.071\n## SourcePB    2.9027     0.3445    0.9686     8.699\n## Phase1      2.8904     0.3460    1.6654     5.016\n## \n## Concordance= 0.737  (se = 0.037 )\n## Likelihood ratio test= 28.33  on 5 df,   p=3e-05\n## Wald test            = 27.27  on 5 df,   p=5e-05\n## Score (logrank) test = 30.49  on 5 df,   p=1e-05,   Robust = 20.2  p=0.001\n## \n##   (Note: the likelihood ratio and score tests assume independence of\n##      observations within a cluster, the Wald and robust score tests do not).\n\n接下来，我们可以使用regplot()函数绘制nomogram。其实你可以绘制多种不同的列线图，可以参考之前的推文：生存资料列线图的4种绘制方法\n\nlibrary(regplot)\nregplot(m.crr,\n        observation=df.w2[df.w2$id==25&df.w2$failcode==1,],\n        failtime = c(36, 60), \n        prfail = T, \n        droplines=T)\n## \"observation\" has &gt;1 row. The first  row provides plotted values\n## Regression  m.crr coxph formula:\n## Surv(T, status == 1) `~` Age + Sex + D + Source + Phase\n## Replicate integer weights assumed\n## Note: non-integer weights have been floored\n## [1] \"note: points tables not constructed unless points=TRUE \"\n\n\n在这个列线图中，将数据集中id=25的患者各协变量的取值映射到相应的得分，并计算总得分,并分别计算其在36个月和60个月的累计复发概率，此概率即为控制了竞争风险的累计复发概率，分别为：0.134和0.146。"
  },
  {
    "objectID": "nomogram-lasso.html",
    "href": "nomogram-lasso.html",
    "title": "7  lasso回归列线图绘制",
    "section": "",
    "text": "lasso回归的列线图绘制请参考Chapter 39。"
  },
  {
    "objectID": "nomogram-colorfulbar.html#加载数据和r包",
    "href": "nomogram-colorfulbar.html#加载数据和r包",
    "title": "8  列线图添加彩色风险分层",
    "section": "8.1 加载数据和R包",
    "text": "8.1 加载数据和R包\n\nlibrary(survival)\nlibrary(rms)\n## Loading required package: Hmisc\n## \n## Attaching package: 'Hmisc'\n## The following objects are masked from 'package:base':\n## \n##     format.pval, units\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\ndim(lung)\n## [1] 228  10\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ..."
  },
  {
    "objectID": "nomogram-colorfulbar.html#传统列线图",
    "href": "nomogram-colorfulbar.html#传统列线图",
    "title": "8  列线图添加彩色风险分层",
    "section": "8.2 传统列线图",
    "text": "8.2 传统列线图\n大多数情况下都是使用1代表死亡，0代表删失，这个数据集用2代表死亡。在这里没有影响，但有的R包会报错，需要注意！\n\ndd &lt;- datadist(lung)\noptions(datadist = \"dd\")\n\n构建cox比例风险模型：\n\ncoxfit &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n              data = lung, x=T,y=T,surv = T\n              )\n\n# 构建生存函数，注意你的最大生存时间\nsurv &lt;- Survival(coxfit) \nsurv1 &lt;- function(x) surv(365,x) # 1年OS\nsurv2 &lt;- function(x) surv(365*2,x) # 2年OS\n\nnom &lt;- nomogram(coxfit,\n                fun = list(surv1,surv2),\n                lp = T,\n                funlabel = c('1-year survival Probability',\n                         '2-year survival Probability'),\n                maxscale = 100,\n                fun.at = c(0.95,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1))\n\n然后就是默认的画图，没有任何难度：\n\nplot(nom, \n     lplabel=\"Linear Predictor\",\n     xfrac = 0.2, # 左侧标签距离坐标轴的距离\n     #varname.label = TRUE, \n     tcl = -0.2, # 刻度长短和方向 \n     lmgp = 0.1, # 坐标轴标签距离坐标轴远近\n     points.label ='Points', \n     total.points.label = 'Total Points',\n     cap.labels = FALSE,\n     cex.var = 1, # 左侧标签字体大小\n     cex.axis = 1, # 坐标轴字体大小\n     col.grid = gray(c(0.8, 0.95))) # 竖线颜色"
  },
  {
    "objectID": "nomogram-colorfulbar.html#新型列线图",
    "href": "nomogram-colorfulbar.html#新型列线图",
    "title": "8  列线图添加彩色风险分层",
    "section": "8.3 新型列线图",
    "text": "8.3 新型列线图\n如何给列线图添加风险分层条带呢？其实思路是很简单的，只要在合适的位置插入颜色条即可。\n为了达到这个目的，需要你对base r的绘图语法足够熟悉。\n直接用rect即可在原图形继续添加矩形区域，然后给它一个颜色即可，除此之外，我们还可以用text函数在底部添加文字提示，让这个图形看上去更加美观实用。\n\n#pdf(\"nomogram.pdf\")\nplot(nom, \n     lplabel=\"Risk Stratification\",\n     xfrac = 0.2, # 左侧标签距离坐标轴的距离\n     #varname.label = TRUE, \n     tcl = -0.2, # 刻度长短和方向 \n     lmgp = 0.1, # 坐标轴标签距离坐标轴远近\n     points.label ='Points', \n     total.points.label = 'Total Points',\n     cap.labels = FALSE,\n     cex.var = 1, # 左侧标签字体大小\n     cex.axis = 1, # 坐标轴字体大小\n     col.grid = gray(c(0.8, 0.95))) # 竖线颜色\nrect(0.29,0.20,0.5,0.26,col = \"#01847F\") # 添加彩色条带\nrect(0.5,0.20,0.7,0.26,col = \"#FBD26A\")\nrect(0.7,0.20,0.935,0.26,col = \"#F40002\")\ntext(0.4,0.18,\"Low\")\ntext(0.6,0.18,\"Medium\")\ntext(0.83,0.18,\"High\")\n#dev.off()\n\n\n这样一个新型的带颜色条的列线图就绘制好了。是不是很简单呢？\n我说说我的具体思路，首先用rect函数添加3个彩色条带，其用法是rect(min(x),min(y),max(x),max(y))，前四个参数确定位置。然后使用text函数在合适的位置添加文字即可。\n这个彩色条带刚好覆盖在原来的Linear Predictor的位置，当然这个位置需要你不断的尝试才能确定，而且我这里的风险分层为了演示是随便选的，你需要根据自己的实际情况确定到底什么分数段属于什么分层，然后不断调整位置直到你满意为止。\n但是这个图现在还是有点问题的，主要是左侧遗留了一个-1，没办法去掉。\n当然了，你也可以直接把传统列线图保存为PDF，然后用AI等软件编辑，更加自由！"
  },
  {
    "objectID": "nomogram-colorfulbar.html#继续改进",
    "href": "nomogram-colorfulbar.html#继续改进",
    "title": "8  列线图添加彩色风险分层",
    "section": "8.4 继续改进",
    "text": "8.4 继续改进\n我又去pubmed以及google使用关键词nomogram继续搜索，果然又搜到一篇带有彩色条带的列线图，而且我感觉这个图更加好看！\n\n文献DOI：10.1093/eurheartj/ehab294\n上面这个图不仅有彩色条带展示分层，而且还增加了彩色箭头标识，并在最底部也增加了彩色线条标识。\n下面我们继续学习这个列线图怎么画，思路和上面基本是一样的。\n首先是再添加一个颜色条：\n\n#pdf(\"nomogram.pdf\")\nplot(nom, \n     lplabel=\"Risk Stratification\",#名字就不改了\n     xfrac = 0.2, # 左侧标签距离坐标轴的距离\n     #varname.label = TRUE, \n     tcl = -0.2, # 刻度长短和方向 \n     lmgp = 0.1, # 坐标轴标签距离坐标轴远近\n     points.label ='Points', \n     total.points.label = 'Total Points',\n     cap.labels = FALSE,\n     cex.var = 1, # 左侧标签字体大小\n     cex.axis = 1, # 坐标轴字体大小\n     col.grid = gray(c(0.8, 0.95))) # 竖线颜色\nrect(0.29,0.245,0.5,0.26,col = \"#01847F\") # 添加彩色条带\nrect(0.5,0.245,0.7,0.26,col = \"#FBD26A\")\nrect(0.7,0.245,0.935,0.26,col = \"#F40002\")\ntext(0.4,0.28,\"Low\")\ntext(0.6,0.28,\"Medium\")\ntext(0.83,0.28,\"High\")\n\n#在底部再增加3个彩色条带，高度错开，显得有层次感\nrect(0.37,0.14,0.5,0.144,col = \"#01847F\")\nrect(0.5,0.144,0.7,0.148,col = \"#FBD26A\")\nrect(0.7,0.148,0.835,0.152,col = \"#F40002\")\n#如果你还要继续添加文字说明也可以，我这里就不加了\n#dev.off()\n\n\n彩色箭头如何添加？一模一样的思路，选择一个你想展示的病人，然后计算它每一项的分数，然后使用arrows函数在合适的位置绘制箭头即可。\n下面随便展示下，我这里并没有认真计算这个人的各项分数。如果你需要展示，可以用nomogramformula包计算，或者等我们下次演示。\n\n#pdf(\"nomogram.pdf\")\nplot(nom, \n     lplabel=\"Risk Stratification\",#名字就不改了\n     xfrac = 0.2, # 左侧标签距离坐标轴的距离\n     #varname.label = TRUE, \n     tcl = -0.2, # 刻度长短和方向 \n     lmgp = 0.1, # 坐标轴标签距离坐标轴远近\n     points.label ='Points', \n     total.points.label = 'Total Points',\n     cap.labels = FALSE,\n     cex.var = 1, # 左侧标签字体大小\n     cex.axis = 1, # 坐标轴字体大小\n     col.grid = gray(c(0.8, 0.95))) # 竖线颜色\nrect(0.29,0.245,0.5,0.26,col = \"#01847F\") # 添加彩色条带\nrect(0.5,0.245,0.7,0.26,col = \"#FBD26A\")\nrect(0.7,0.245,0.935,0.26,col = \"#F40002\")\ntext(0.4,0.28,\"Low\")\ntext(0.6,0.28,\"Medium\")\ntext(0.83,0.28,\"High\")\n\n#在底部再增加3个彩色条带，高度错开，显得有层次感\nrect(0.37,0.14,0.5,0.144,col = \"#01847F\")\nrect(0.5,0.144,0.7,0.148,col = \"#FBD26A\")\nrect(0.7,0.148,0.835,0.152,col = \"#F40002\")\n#如果你还要继续添加文字说明也可以，我这里就不加了\n\n# 添加箭头\narrows(0.205,0.86,0.205,0.96,col = \"steelblue\",lwd = 4,length = 0.1)\narrows(0.4,0.76,0.4,0.96,col = \"steelblue\",lwd = 4,length = 0.1)\narrows(0.68,0.655,0.68,0.96,col = \"steelblue\",lwd = 4,length = 0.1)\narrows(0.28,0.55,0.28,0.96,col = \"steelblue\",lwd = 4,length = 0.1)\narrows(0.47,0.45,0.47,0.96,col = \"steelblue\",lwd = 4,length = 0.1)\n\n# 总分箭头，加起来可能不对，单纯演示下\narrows(0.84,0.40,0.84,0.35,col = \"#F40002\",lwd = 4,length = 0.1)\n#dev.off()\n\n\n这样一个非常漂亮的列线图就画好了，层次分明，细节满满，让人耳目一新，大家赶紧用起来吧！\n但是目前这样画太费劲了，需要不断调整位置才能得到最终的效果，我在想，能不能写成一个函数，帮大家简化这件事？\n如果有大佬知道成熟的方法，也欢迎告诉我，这样我就不用重复造轮子了。"
  },
  {
    "objectID": "nomogram-points.html#准备数据",
    "href": "nomogram-points.html#准备数据",
    "title": "9  计算列线图得分及危险分层",
    "section": "9.1 准备数据",
    "text": "9.1 准备数据\n使用R包自带数据。\n\nlibrary(survival)\nlibrary(rms)\n## Loading required package: Hmisc\n## \n## Attaching package: 'Hmisc'\n## The following objects are masked from 'package:base':\n## \n##     format.pval, units\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\nrm(list = ls())\n\ndim(lung)\n## [1] 228  10\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ..."
  },
  {
    "objectID": "nomogram-points.html#建立模型和列线图",
    "href": "nomogram-points.html#建立模型和列线图",
    "title": "9  计算列线图得分及危险分层",
    "section": "9.2 建立模型和列线图",
    "text": "9.2 建立模型和列线图\n使用rms包构建模型和列线图。\n大多数情况下都是使用1代表死亡，0代表删失，这个数据集用2代表死亡。在这里没有影响，但有的R包会报错，需要注意！\n\ndd &lt;- datadist(lung)\noptions(datadist = \"dd\")\n\n构建cox比例风险模型：\n\ncoxfit &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n              data = lung, x=T,y=T,surv = T\n              )\n\n# 构建生存函数，注意你的最大生存时间\nsurv &lt;- Survival(coxfit) \nsurv1 &lt;- function(x) surv(365,x) # 1年OS\nsurv2 &lt;- function(x) surv(365*2,x) # 2年OS\n\nnom &lt;- nomogram(coxfit,\n                fun = list(surv1,surv2),\n                lp = T,\n                funlabel = c('1-year survival Probability',\n                         '2-year survival Probability'),\n                maxscale = 100,\n                fun.at = c(0.95,0.9,0.8,0.7,0.6,0.5,0.4,0.3,0.2,0.1))\n\n然后就是画图：\n\nplot(nom, \n     lplabel=\"Linear Predictor\",\n     xfrac = 0.2, # 左侧标签距离坐标轴的距离\n     #varname.label = TRUE, \n     tcl = -0.2, # 刻度长短和方向 \n     lmgp = 0.1, # 坐标轴标签距离坐标轴远近\n     points.label ='Points', \n     total.points.label = 'Total Points',\n     cap.labels = FALSE,\n     cex.var = 1, # 左侧标签字体大小\n     cex.axis = 1, # 坐标轴字体大小\n     col.grid = gray(c(0.8, 0.95))) # 竖线颜色\n\n\n\n\n到这里都很简单。"
  },
  {
    "objectID": "nomogram-points.html#计算分数",
    "href": "nomogram-points.html#计算分数",
    "title": "9  计算列线图得分及危险分层",
    "section": "9.3 计算分数",
    "text": "9.3 计算分数\n使用nomogramFormula计算每个患者的列线图得分。\n两种方法，其中是使用formula_lp根据线性预测值计算，另一种是使用formula_rd根据原始数据（raw_data）计算，两种方法结果差不多，任选一种即可。\n\nlibrary(nomogramFormula)\nresults &lt;- formula_lp(nomogram = nom)\npoints1 &lt;- points_cal(formula = results$formula, lp = coxfit$linear.predictors)\n\n#或者\n#results &lt;- formula_rd(nomogram = nom2)\n#points1 &lt;- points_cal(formula = results$formula, rd = tmp)\n\nlength(points1)\n## [1] 223\nhead(points1)\n##         1         2         3         4         5         6 \n## 129.96853  98.56938  90.51815 142.40181 102.54570 104.51291\n\n根据这个分数就可以分成高风险组/低风险组了。"
  },
  {
    "objectID": "nomogram-points.html#分层",
    "href": "nomogram-points.html#分层",
    "title": "9  计算列线图得分及危险分层",
    "section": "9.4 分层",
    "text": "9.4 分层\n假如我们想根据列线图得分进行分层，分层后两组的K-M生存分析的p值最小，方法很多，任选一种即可，我这里就用surv_cutpoint演示。\n但是计算出来的分数223个，原始数据是228个，因为数据有缺失值，在建立模型时有5个样本被删了，这时候你回过去找不一定找得到缺失值在哪（我能找到），所以建议一开始就把缺失值处理掉。\n\nlibrary(tidyr)\nlibrary(survminer)\n\n# 去掉缺失值\ntmp &lt;- lung %&gt;% \n  drop_na(ph.ecog,ph.karno,pat.karno)\ndim(tmp)\n## [1] 223  10\n\ntmp$points &lt;- points1\n\n# 分层\nres.cut &lt;- surv_cutpoint(tmp, time = \"time\", event = \"status\",\n                         variables = \"points\"\n                         )\nres.cat &lt;- surv_categorize(res.cut)\n\n绘制生存曲线：\n\nlibrary(\"survival\")\nfit &lt;- survfit(Surv(time, status) ~points, data = res.cat)\nggsurvplot(fit, data = res.cat, pval = T)\n\n\n\n\n中间的数据展示省略了很多，还不熟悉这一套流程的可以一步一步的看，结合之前的推文。"
  },
  {
    "objectID": "nomogram-points.html#扩展",
    "href": "nomogram-points.html#扩展",
    "title": "9  计算列线图得分及危险分层",
    "section": "9.5 扩展",
    "text": "9.5 扩展\n这里是根据列线图的得分进行分层的，其实也可以直击根据模型得到的线性预测值进行分层，就是直接使用predict即可：\n\npredict(coxfit,head(tmp))\n##          1          2          3          4          5          6 \n##  0.3113300 -0.2213878 -0.3579849  0.5222729 -0.1539256 -0.1205499\n\n这个东西就是大家常见的risk-score，当然这只是其中一种计算方式，不同的模型计算方法略有不同。\n而且cox回归得到的这个线性预测值又叫做预后指数（prognosis index, PI）。\n\n预后指数越大，患者风险越大，预后越差。–孙振球医学统计学第4版P293\n\n最早的建模类文章都是这么干的，现在也不少见。优点就是少了计算分数那一步，缺点嘛暂时没发现，毕竟都是模仿，你发文章只要把你的故事说清楚即可~"
  },
  {
    "objectID": "feature-selection.html#常见方法简介",
    "href": "feature-selection.html#常见方法简介",
    "title": "10  常见的变量选择方法",
    "section": "10.1 常见方法简介",
    "text": "10.1 常见方法简介\n变量选择(特征选择,feature selection)，是机器学习领域非常重要的问题，到底哪些变量是有用的，哪些是不重要的，可以删除的，怎么选才能提高模型表现，理论非常复杂，实在不是一个临床医生能完全掌握的，以下简单介绍下，感兴趣的自己看书，后续会推一些相关R包的使用教程。\n在传统的临床预测模型中，比较常见的变量筛选方法有：\n\n先单因素后多因素\n最优子集（全子集回归）\n逐步选择法\nlasso回归筛选变量\n随机森林筛选变量\n…\n\n本文介绍的机器学习中的变量筛选方法（并没有包含在本书中，可在公众号医学和生信笔记后台回复变量筛选获取相关合集）虽然可以用在临床预测模型中，但是和大家常见的“先单因素后多因素”这种完全不是一个概念，虽然它们的目的相同，都是为了提高模型表现。\n当数据的维度增加时，决定模型最终使用哪些预测变量是很关键的问题。数据的维度就是自变量(预测变量)\n特征选择是特征工程中非常重要的一部分内容，特征选择的方法非常多，主要可以分为以下3类，每个大类下又会细分为好多具体的方法，有机会慢慢介绍…\n\n过滤法(filter)\n\n缺失值比例、方差、相关系数、方差分析/t检验/卡方检验、ROC等\n信息增益 information gain\n最小冗余最大相关性mrmr，Minimum Redundancy Maximum Relevance\n…\n\n包装法(wrapper)\n\n向前、向后、逐步\n递归特征消除rfe(也属于向后)\n模拟退火\n遗传算法\n…\n\n嵌入法(embeded)\n\n随机森林\nMARS\nlasso\nGBDT\n…\n\n\n大家经常使用的逐步选择法(step/stepAIC)，也属于包装法的一种，在之前的推文中已有介绍：R语言逻辑回归的细节解读，但是并不局限于逻辑回归。\n3种方法的简单解释如下，以后单独演示时会专门再解释：\n\n过滤法：进行变量选择时不考虑模型表现和变量重要性等，只是通过变量自身的情况、变量间的关系进行选择。\n包装法：变量选择考虑到了模型表现和变量重要性等信息，属于是对每一个模型进行“量身定制”的变量\n嵌入法：变量选择的过程就在模型训练的过程之中"
  },
  {
    "objectID": "feature-selection.html#r语言中的实现",
    "href": "feature-selection.html#r语言中的实现",
    "title": "10  常见的变量选择方法",
    "section": "10.2 R语言中的实现",
    "text": "10.2 R语言中的实现\n后续主要介绍3个包：caret、mlr3、tidymodels\n在caret包中主要可以实现包装法和过滤法。\ncaret包中的封装法有递归特征消除(recursive feature elimination，rfe)算法，遗传算法（genetic algorithms，ga）和模拟退火（Simulated annealing，sa）算法。\n过滤法通过sbf函数实现，但其实部分数据预处理方法属于过滤法的内容。\nmlr3中的变量选择主要包括两种：过滤法和包装法。不过和caret的实现方法略有不同。\n\n\n\n\n\n\n注意\n\n\n\n目前mlr3已经实现了对嵌入法的支持，可以说是R语言中对变量筛选做的最好的综合性R包了。\n\n\n过滤法通过mlr3filters包实现，包装法通过mlr3fselect包实现，关于这两种方法的具体实现，早已在之前的推文介绍过，大家可以参考之前的推文mlr3特征选择\n不过随着mlr3的更新，部分细节稍有不同，以后再给大家慢慢演示。\ntidymodels中的特征选择很不完善，不如mlr3做得好，也不如caret做得好！\n部分过滤法包含在recipes中，部分包装法和嵌入法现在并不成熟，没有完整的实现，部分可通过colina包实现，但是这个包并不属于tidymodels，而是个人开发者贡献的R包。\n已经看到tidymodels的开发者有计划增加特征选择的这部分特性，但不知何时实现…\n总的来说，想要在R中完整实现以上三种方法，一言难尽……"
  },
  {
    "objectID": "feature-selection_unimulti.html#准备数据",
    "href": "feature-selection_unimulti.html#准备数据",
    "title": "11  变量筛选之先单后多",
    "section": "11.1 准备数据",
    "text": "11.1 准备数据\n我们使用TCGA-BLCA的lncRNA数据，其中包括408个样本，time_months是生存时间，event是生存状态，1代表死亡，0代表生存，其余变量都是自变量。\n先简单处理一下数据（数据已放在粉丝QQ群文件）：\n\nrm(list = ls())\nload(file = \"datasets/lnc_expr_clin.RData\")\n#去掉没有生存信息的样本\nlnc_expr_clin1 &lt;- lnc_expr_clin[!is.na(lnc_expr_clin$time_months),]\nlnc_expr_clin1 &lt;- lnc_expr_clin1[lnc_expr_clin1$time_months&gt;0,]\n\n#选择其中一部分数据\ndat.cox &lt;- lnc_expr_clin1[,c(72:73,1:59)]\ndim(dat.cox)\n## [1] 297  61\ndat.cox[1:4,1:6]\n##   event time_months   PGM5-AS1 LINC01082 AC005180.2 AC005180.1\n## 1     0       36.33 0.15064007 0.2642238  0.0000000  0.1547768\n## 2     0       13.87 0.06309362 0.1666554  0.3105983  0.2436603\n## 3     1       21.83 2.16399508 3.5662920  2.2454129  2.0073496\n## 4     0       18.20 2.73075081 1.7314314  0.8609916  0.7323014\n\n现在这个数据一共59个自变量，我们先对每一个自变量都做一遍单因素COX回归，但是要注意，这里的59个自变量都是连续型的，通常基因表达量增加1，死亡风险增加xx倍这种情况是不可能发生的，这样的结果解释也是不合理的，所以我们需要先把这样的变量重新分箱，比如根据中位数分成两组，再进行单因素COX回归。\n\ndat_cox &lt;- dat.cox\ndat_cox[,c(3:ncol(dat_cox))] &lt;- sapply(dat_cox[,c(3:ncol(dat_cox))],function(x){\n  ifelse(x&gt;median(x),\"high\",\"low\")\n})\ndat_cox[,c(3:ncol(dat_cox))] &lt;- lapply(dat_cox[,c(3:ncol(dat_cox))],factor)\ndat_cox[1:4,1:6]\n##   event time_months PGM5-AS1 LINC01082 AC005180.2 AC005180.1\n## 1     0       36.33     high       low        low        low\n## 2     0       13.87      low       low       high       high\n## 3     1       21.83     high      high       high       high\n## 4     0       18.20     high      high       high       high"
  },
  {
    "objectID": "feature-selection_unimulti.html#批量单因素cox",
    "href": "feature-selection_unimulti.html#批量单因素cox",
    "title": "11  变量筛选之先单后多",
    "section": "11.2 批量单因素cox",
    "text": "11.2 批量单因素cox\n然后就可以对每个变量进行单因素COX分析了：\n\nlibrary(survival)\n\ngene &lt;- colnames(dat_cox)[-c(1:2)]\ncox.result &lt;- list()\nfor (i in 1:length(gene)) {\n      #print(i)\n      group &lt;- dat_cox[, i + 2]\n      if (length(table(group)) == 1) next\n      #if (length(grep(\"high\", group)) &lt; min_sample_size) next\n      #if (length(grep(\"low\", group)) &lt; min_sample_size) next\n      x &lt;- survival::coxph(survival::Surv(time_months, event) ~ group, \n                           data = dat_cox)\n      tmp1 &lt;- broom::tidy(x, exponentiate = T, conf.int = T)\n      cox.result[[i]] &lt;- c(gene[i], tmp1)\n    }\n\nres.cox &lt;- data.frame(do.call(rbind, cox.result))\n\n筛选出P值小于0.1的变量：\n\nlibrary(dplyr)\n\nunifea &lt;- res.cox %&gt;% \n  filter(p.value&lt;0.1) %&gt;% \n  pull(V1) %&gt;% \n  unlist()\nunifea\n##  [1] \"AC005180.2\"   \"AC005180.1\"   \"AC053503.3\"   \"MIR100HG\"     \"AP001107.5\"  \n##  [6] \"C5orf66-AS1\"  \"AL162424.1\"   \"ADAMTS9-AS1\"  \"MIR200CHG\"    \"AC093010.3\"  \n## [11] \"AC079313.2\"   \"SNHG25\"       \"AL049555.1\"   \"MIR1-1HG-AS1\" \"SPINT1-AS1\"  \n## [16] \"KRT7-AS\"      \"HAND2-AS1\"    \"AC025575.2\"   \"MAFG-DT\"      \"AL390719.2\"  \n## [21] \"AC002398.2\"   \"AL161431.1\"   \"U62317.1\"     \"AL023284.4\"   \"AATBC\""
  },
  {
    "objectID": "feature-selection_unimulti.html#多因素cox",
    "href": "feature-selection_unimulti.html#多因素cox",
    "title": "11  变量筛选之先单后多",
    "section": "11.3 多因素cox",
    "text": "11.3 多因素cox\n把这些变量进行多因素COX回归\n\nsub_dat &lt;- dat_cox[,c(\"time_months\",\"event\",unifea)]\ndim(sub_dat)\n## [1] 297  27\nsub_dat[1:4,1:6]\n##   time_months event AC005180.2 AC005180.1 AC053503.3 MIR100HG\n## 1       36.33     0        low        low        low      low\n## 2       13.87     0       high       high       high      low\n## 3       21.83     1       high       high       high     high\n## 4       18.20     0       high       high       high     high\n\n拟合多因素cox回归模型并查看结果：\n\nfinal.fit &lt;- coxph(Surv(time_months,event)~., data = sub_dat)\nres &lt;- broom::tidy(final.fit)\nres\n## # A tibble: 25 × 5\n##    term             estimate std.error statistic p.value\n##    &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n##  1 AC005180.2low      0.146      0.413     0.354 0.723  \n##  2 AC005180.1low     -0.343      0.399    -0.859 0.390  \n##  3 AC053503.3low     -0.139      0.391    -0.355 0.723  \n##  4 MIR100HGlow       -0.365      0.366    -0.997 0.319  \n##  5 AP001107.5low      0.284      0.344     0.825 0.409  \n##  6 `C5orf66-AS1`low  -0.538      0.284    -1.89  0.0587 \n##  7 AL162424.1low      0.0418     0.335     0.125 0.901  \n##  8 `ADAMTS9-AS1`low  -0.947      0.360    -2.63  0.00853\n##  9 MIR200CHGlow       0.0336     0.329     0.102 0.919  \n## 10 AC093010.3low      0.905      0.323     2.80  0.00505\n## # ℹ 15 more rows\n\n查看P值小于0.05的变量：结果只有5个\n\nres %&gt;% filter(p.value&lt;0.05)\n## # A tibble: 5 × 5\n##   term             estimate std.error statistic p.value\n##   &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n## 1 `ADAMTS9-AS1`low   -0.947     0.360     -2.63 0.00853\n## 2 AC093010.3low       0.905     0.323      2.80 0.00505\n## 3 SNHG25low           0.592     0.280      2.11 0.0346 \n## 4 AC025575.2low       0.618     0.287      2.16 0.0311 \n## 5 AL161431.1low      -0.704     0.309     -2.28 0.0226\n\n这5个变量可以用于最终的模型中，但是考虑到不同变量之间的交互作用等情况，此时再拟合多因素cox模型，可能还会出现某个变量的P值大于0.05的情况，属于正常现象~\n\nfit5 &lt;- coxph(Surv(time_months,event)~`ADAMTS9-AS1`+AC093010.3+SNHG25+\n                AC025575.2+AL161431.1,data = sub_dat)\nlibrary(survminer)\n## Loading required package: ggplot2\n## Loading required package: ggpubr\n## \n## Attaching package: 'survminer'\n## The following object is masked from 'package:survival':\n## \n##     myeloma\nsurvminer::ggforest(fit5)\n## Warning in .get_data(model, data = data): The `data` argument is not provided.\n## Data will be extracted from model fit."
  },
  {
    "objectID": "feature-selection_unimulti.html#行代码实现",
    "href": "feature-selection_unimulti.html#行代码实现",
    "title": "11  变量筛选之先单后多",
    "section": "11.4 1行代码实现",
    "text": "11.4 1行代码实现\n手动实现的过程就是为了告诉大家思路是怎样的，这样大家有一定的基础就可以自己做，不管是什么数据，都是一样的思路，用什么方法和工具不重要，思路才是最重要的。\n下面再给大家介绍一个R包，可以实现1行代码完成先单后多cox分析，得到的结果和我们的手动实现的结果是一样的。\n首先安装R包：\n\n#install.packages(\"devtools\")\ndevtools::install_github(\"cardiomoon/autoReg\")\n\n\nlibrary(autoReg)\n\n使用autoReg函数可以实现先单后多cox分析，首先先建立cox模型，此时是多因素cox的形式，但是这个函数会自动帮我们提取数据，然后先批量对每个变量做cox。\n但是！乱七八糟的变量名字是不行的，比如我们演示用的这个lncRNA数据集，变量名字中有-，导致函数报错：\nfit &lt;- coxph(Surv(time_months,event)~., data = dat_cox)\n\nautoReg(fit,\n        threshold = 0.1,\n        uni = T,\n        multi = F\n        )\n\n# 报错\nError in parse(text = eq) : &lt;text&gt;:1:23: unexpected symbol\n1: df[['MIR1-1HG-AS1']]+1HG\n^\n我们给这个数据集的变量名字修改一下即可，我这里直接把-去掉了：\n\ncolnames(dat_cox)&lt;- gsub(\"-\",\"\",colnames(dat_cox))\ndat_cox[1:4,1:6]\n##   event time_months PGM5AS1 LINC01082 AC005180.2 AC005180.1\n## 1     0       36.33    high       low        low        low\n## 2     0       13.87     low       low       high       high\n## 3     1       21.83    high      high       high       high\n## 4     0       18.20    high      high       high       high\n\n这样变量名字中就没有乱七八糟的符号了，此时再进行分析就不会报错了。\n而且结果直接给出了三线表的格式，看起来非常整洁：\n\nfit &lt;- coxph(Surv(time_months,event)~., data = dat_cox)\n\nft &lt;- autoReg(fit,\n        threshold = 0.1,\n        uni = T, # 单因素分析\n        multi = T, # 多因素分析\n        final = F # 逐步法，向后\n        )\nft\n\n表格太长了，只展示部分：\n\n这个结果是可以导出为Word或者Excel格式的：\n\nlibrary(rrtable)\n\ntable2docx(ft)\n\n除此之外，这个包还是一个非常强大的三线表绘制R包，可以1行代码实现多种精美的三线表、回归分析（线性回归、逻辑回归、生存分析）结果表格，大家感兴趣的可以去官网学习：https://cardiomoon.github.io/autoReg/index.html"
  },
  {
    "objectID": "feature-selection_stepwise.html#加载数据",
    "href": "feature-selection_stepwise.html#加载数据",
    "title": "12  筛选变量逐步回归",
    "section": "12.1 加载数据",
    "text": "12.1 加载数据\n我们使用TCGA-BLCA的lncRNA数据（数据已放在粉丝qq群文件），其中包括408个样本，time_months是生存时间，event是生存状态，1代表死亡，0代表生存，其余变量都是自变量。\n\nrm(list = ls())\nload(file = \"datasets/lnc_expr_clin.RData\")\n#去掉没有生存信息的样本\nlnc_expr_clin1 &lt;- lnc_expr_clin[!is.na(lnc_expr_clin$time_months),]\nlnc_expr_clin1 &lt;- lnc_expr_clin1[lnc_expr_clin1$time_months&gt;0,]\n\n#选择其中一部分数据\ndat.cox &lt;- lnc_expr_clin1[,c(72:73,1:59)]\ndim(dat.cox)\n## [1] 297  61\ndat.cox[1:4,1:6]\n##   event time_months   PGM5-AS1 LINC01082 AC005180.2 AC005180.1\n## 1     0       36.33 0.15064007 0.2642238  0.0000000  0.1547768\n## 2     0       13.87 0.06309362 0.1666554  0.3105983  0.2436603\n## 3     1       21.83 2.16399508 3.5662920  2.2454129  2.0073496\n## 4     0       18.20 2.73075081 1.7314314  0.8609916  0.7323014\n\n现在这个数据一共59个自变量，我们先使用所有自变量建立cox回归模型。"
  },
  {
    "objectID": "feature-selection_stepwise.html#建立模型",
    "href": "feature-selection_stepwise.html#建立模型",
    "title": "12  筛选变量逐步回归",
    "section": "12.2 建立模型",
    "text": "12.2 建立模型\n我们这个是生存数据，使用cox回归。如果你的数据是其他类型，使用逻辑回归或者线性回归都是可以的。\n\nlibrary(survival)\n\nfit.cox &lt;- coxph(Surv(time_months,event)~.,data = dat.cox)\nfit.cox\n## Call:\n## coxph(formula = Surv(time_months, event) ~ ., data = dat.cox)\n## \n##                     coef exp(coef)  se(coef)      z       p\n## `PGM5-AS1`     -0.008183  0.991850  0.222738 -0.037 0.97069\n## LINC01082       0.345614  1.412858  0.403674  0.856 0.39190\n## AC005180.2      0.977584  2.658027  0.906672  1.078 0.28094\n## AC005180.1      0.846348  2.331118  1.193460  0.709 0.47823\n## FENDRR         -0.653451  0.520247  0.548215 -1.192 0.23328\n## AC053503.3     -0.589548  0.554578  0.553104 -1.066 0.28647\n## MIR100HG        0.902471  2.465687  0.386229  2.337 0.01946\n## AP001107.5     -0.812922  0.443560  0.677700 -1.200 0.23032\n## `C5orf66-AS1`   0.094286  1.098873  0.295767  0.319 0.74989\n## NR4A1AS         0.874631  2.397990  0.336703  2.598 0.00939\n## AL162424.1      0.049223  1.050455  0.324490  0.152 0.87943\n## AF001548.1      0.949594  2.584659  0.788709  1.204 0.22860\n## AC099850.4      0.205430  1.228053  0.252745  0.813 0.41634\n## `MBNL1-AS1`    -0.798659  0.449932  0.675864 -1.182 0.23733\n## `ADAMTS9-AS1`  -0.065160  0.936917  1.776167 -0.037 0.97074\n## MIR22HG         0.108393  1.114486  0.228433  0.475 0.63514\n## MIR200CHG      -0.070914  0.931542  0.215534 -0.329 0.74214\n## AC093010.3     -0.658117  0.517825  0.347044 -1.896 0.05791\n## LINC00865      -0.282616  0.753809  0.284716 -0.993 0.32089\n## AP003071.4     -0.506228  0.602765  0.675734 -0.749 0.45377\n## PCAT6           0.347869  1.416047  0.199663  1.742 0.08146\n## LINC02657      -0.175082  0.839388  0.122482 -1.429 0.15287\n## `PPP1R14B-AS1`  0.144657  1.155643  0.238269  0.607 0.54377\n## AC012085.2     -2.267686  0.103552  1.671128 -1.357 0.17479\n## `ACTA2-AS1`    -0.084816  0.918681  0.760755 -0.111 0.91123\n## AC036108.3      1.405644  4.078154  1.413552  0.994 0.32003\n## AC079313.2      0.167743  1.182633  1.020173  0.164 0.86940\n## AC020916.1      0.037921  1.038649  0.201470  0.188 0.85070\n## SNHG25         -0.151295  0.859594  0.330536 -0.458 0.64715\n## AL049555.1      0.398962  1.490277  0.207738  1.921 0.05479\n## `MIR1-1HG-AS1` -1.851131  0.157059  1.991975 -0.929 0.35274\n## AC018904.1      0.026484  1.026838  0.233038  0.114 0.90952\n## SNHG12          0.144329  1.155264  0.344430  0.419 0.67519\n## `SPINT1-AS1`    0.676775  1.967522  0.374980  1.805 0.07110\n## `KRT7-AS`      -0.137828  0.871248  0.129775 -1.062 0.28821\n## MIR205HG       -0.076826  0.926051  0.161261 -0.476 0.63378\n## `HAND2-AS1`     1.853233  6.380415  1.742430  1.064 0.28751\n## AL445524.1     -0.255737  0.774345  0.220099 -1.162 0.24527\n## LINC01980      -0.171282  0.842584  0.128515 -1.333 0.18260\n## `ZNF710-AS1`   -0.959290  0.383165  0.459985 -2.085 0.03703\n## AC092718.4      0.010577  1.010633  0.272577  0.039 0.96905\n## AC008735.2     -0.002696  0.997308  0.291545 -0.009 0.99262\n## LINC01133       0.122659  1.130499  0.120170  1.021 0.30739\n## AC025575.2      0.158544  1.171804  0.135523  1.170 0.24205\n## `MAFG-DT`       0.343519  1.409901  0.242305  1.418 0.15627\n## CASC9          -0.118071  0.888633  0.154923 -0.762 0.44598\n## AL390719.2      0.177187  1.193854  0.225404  0.786 0.43182\n## AC002398.2     -1.089318  0.336446  1.662224 -0.655 0.51225\n## AC008736.1     -0.117863  0.888818  0.163847 -0.719 0.47193\n## AL161431.1      0.158805  1.172110  0.112004  1.418 0.15623\n## `PCCA-DT`      -0.456381  0.633572  0.254320 -1.795 0.07273\n## AC245041.2      0.243686  1.275944  0.200371  1.216 0.22392\n## U62317.1       -0.162513  0.850005  0.205601 -0.790 0.42928\n## U62317.2       -0.131903  0.876426  0.315579 -0.418 0.67597\n## `VPS9D1-AS1`   -0.044547  0.956431  0.174172 -0.256 0.79813\n## AL023284.4     -0.335339  0.715095  0.245641 -1.365 0.17220\n## AATBC           0.136654  1.146432  0.180919  0.755 0.45005\n## LINC00641       0.383262  1.467062  0.474750  0.807 0.41950\n## AC015912.3     -0.562875  0.569569  0.296322 -1.900 0.05749\n## \n## Likelihood ratio test=78.86  on 59 df, p=0.04311\n## n= 297, number of events= 71\n\n下面就是用逐步法选择变量。"
  },
  {
    "objectID": "feature-selection_stepwise.html#逐步选择",
    "href": "feature-selection_stepwise.html#逐步选择",
    "title": "12  筛选变量逐步回归",
    "section": "12.3 逐步选择",
    "text": "12.3 逐步选择\n我们使用逐步选择法进行变量筛选：\n\nfit.step &lt;- step(fit.cox,direction = \"both\")\n#save(fit.step,file = \"./datasets/fit.step.edata\")\n\nStart:  AIC=794.12\nSurv(time_months, event) ~ `PGM5-AS1` + LINC01082 + AC005180.2 + \n    AC005180.1 + FENDRR + AC053503.3 + MIR100HG + AP001107.5 + \n    `C5orf66-AS1` + NR4A1AS + AL162424.1 + AF001548.1 + AC099850.4 + \n    `MBNL1-AS1` + `ADAMTS9-AS1` + MIR22HG + MIR200CHG + AC093010.3 + \n    LINC00865 + AP003071.4 + PCAT6 + LINC02657 + `PPP1R14B-AS1` + \n    AC012085.2 + `ACTA2-AS1` + AC036108.3 + AC079313.2 + AC020916.1 + \n    SNHG25 + AL049555.1 + `MIR1-1HG-AS1` + AC018904.1 + SNHG12 + \n    `SPINT1-AS1` + `KRT7-AS` + MIR205HG + `HAND2-AS1` + AL445524.1 + \n    LINC01980 + `ZNF710-AS1` + AC092718.4 + AC008735.2 + LINC01133 + \n    AC025575.2 + `MAFG-DT` + CASC9 + AL390719.2 + AC002398.2 + \n    AC008736.1 + AL161431.1 + `PCCA-DT` + AC245041.2 + U62317.1 + \n    U62317.2 + `VPS9D1-AS1` + AL023284.4 + AATBC + LINC00641 + \n    AC015912.3\n\n                 Df    AIC\n- AC008735.2      1 792.12\n- `ADAMTS9-AS1`   1 792.12\n- `PGM5-AS1`      1 792.12\n- AC092718.4      1 792.12\n- `ACTA2-AS1`     1 792.13\n- AC018904.1      1 792.13\n- AL162424.1      1 792.14\n- AC079313.2      1 792.15\n- AC020916.1      1 792.15\n- `VPS9D1-AS1`    1 792.18\n- `C5orf66-AS1`   1 792.22\n- MIR200CHG       1 792.23\n- U62317.2        1 792.29\n- SNHG12          1 792.29\n- SNHG25          1 792.33\n- MIR22HG         1 792.34\n- MIR205HG        1 792.35\n- `PPP1R14B-AS1`  1 792.49\n- AC002398.2      1 792.56\n- AC005180.1      1 792.63\n- AC008736.1      1 792.65\n- AATBC           1 792.69\n- AP003071.4      1 792.69\n- CASC9           1 792.70\n- AL390719.2      1 792.74\n- U62317.1        1 792.75\n- LINC00641       1 792.76\n- AC099850.4      1 792.79\n- LINC01082       1 792.86\n- `MIR1-1HG-AS1`  1 793.03\n- AC036108.3      1 793.10\n- LINC00865       1 793.13\n- LINC01133       1 793.13\n- `HAND2-AS1`     1 793.23\n- `KRT7-AS`       1 793.25\n- AC005180.2      1 793.25\n- AC053503.3      1 793.29\n- AL445524.1      1 793.49\n- AC025575.2      1 793.50\n- AF001548.1      1 793.55\n- FENDRR          1 793.61\n- AC245041.2      1 793.61\n- `MBNL1-AS1`     1 793.62\n- AP001107.5      1 793.84\n- LINC01980       1 793.87\n- AL023284.4      1 794.01\n- AL161431.1      1 794.11\n&lt;none&gt;              794.12\n- `MAFG-DT`       1 794.13\n- AC012085.2      1 794.14\n- LINC02657       1 794.20\n- PCAT6           1 795.17\n- `PCCA-DT`       1 795.43\n- `SPINT1-AS1`    1 795.45\n- AC093010.3      1 795.84\n- AL049555.1      1 795.88\n- AC015912.3      1 795.94\n- `ZNF710-AS1`    1 796.86\n- MIR100HG        1 797.13\n- NR4A1AS         1 798.62\n\nStep:  AIC=792.12\n## 省略巨多中间过程\nStep:  AIC=790.12\n## 省略巨多中间过程\nStep:  AIC=734.72\nSurv(time_months, event) ~ AC005180.2 + MIR100HG + AP001107.5 + \n    NR4A1AS + AC093010.3 + PCAT6 + AC036108.3 + AL049555.1 + \n    `MIR1-1HG-AS1` + `SPINT1-AS1` + LINC01980 + `ZNF710-AS1` + \n    AL161431.1 + `PCCA-DT` + U62317.1 + AL023284.4 + AC015912.3\n\n                 Df    AIC\n&lt;none&gt;              734.72\n+ LINC00641       1 734.91\n+ AC012085.2      1 735.01\n- AL049555.1      1 735.02\n+ AC002398.2      1 735.06\n- AC036108.3      1 735.12\n+ `MAFG-DT`       1 735.17\n- `ZNF710-AS1`    1 735.28\n+ AF001548.1      1 735.44\n+ AL445524.1      1 735.44\n- `MIR1-1HG-AS1`  1 735.54\n- U62317.1        1 735.76\n+ `C5orf66-AS1`   1 735.89\n+ `MBNL1-AS1`     1 735.91\n+ MIR205HG        1 735.92\n+ AP003071.4      1 735.99\n- `PCCA-DT`       1 736.08\n+ AATBC           1 736.14\n+ LINC01133       1 736.25\n+ AC099850.4      1 736.27\n- AL161431.1      1 736.29\n+ AC245041.2      1 736.30\n+ AC008735.2      1 736.34\n+ AC025575.2      1 736.36\n+ SNHG12          1 736.36\n+ MIR200CHG       1 736.42\n+ LINC02657       1 736.46\n- AL023284.4      1 736.47\n+ `KRT7-AS`       1 736.49\n+ SNHG25          1 736.50\n+ `PPP1R14B-AS1`  1 736.52\n+ `ADAMTS9-AS1`   1 736.53\n+ U62317.2        1 736.53\n+ FENDRR          1 736.56\n+ `ACTA2-AS1`     1 736.60\n+ AC008736.1      1 736.60\n+ `HAND2-AS1`     1 736.60\n+ `PGM5-AS1`      1 736.61\n+ MIR22HG         1 736.61\n+ AL390719.2      1 736.66\n+ `VPS9D1-AS1`    1 736.67\n+ CASC9           1 736.67\n+ AC053503.3      1 736.67\n+ AC005180.1      1 736.68\n+ AL162424.1      1 736.68\n+ LINC01082       1 736.71\n+ AC079313.2      1 736.71\n+ LINC00865       1 736.71\n+ AC092718.4      1 736.71\n+ AC020916.1      1 736.72\n+ AC018904.1      1 736.72\n- NR4A1AS         1 736.74\n- MIR100HG        1 736.79\n- LINC01980       1 736.86\n- `SPINT1-AS1`    1 736.89\n- AP001107.5      1 737.71\n- PCAT6           1 738.00\n- AC015912.3      1 738.51\n- AC093010.3      1 739.96\n- AC005180.2      1 739.97\n\n查看下结果：\n\nsummary(fit.step)\n\nCall:\ncoxph(formula = Surv(time_months, event) ~ AC005180.2 + MIR100HG + \n    AP001107.5 + NR4A1AS + AC093010.3 + PCAT6 + AC036108.3 + \n    AL049555.1 + `MIR1-1HG-AS1` + `SPINT1-AS1` + LINC01980 + \n    `ZNF710-AS1` + AL161431.1 + `PCCA-DT` + U62317.1 + AL023284.4 + \n    AC015912.3, data = dat.cox)\n\n  n= 297, number of events= 71 \n\n                   coef exp(coef) se(coef)      z Pr(&gt;|z|)   \nAC005180.2      0.83728   2.31007  0.30399  2.754  0.00588 **\nMIR100HG        0.59783   1.81818  0.28762  2.079  0.03766 * \nAP001107.5     -1.63414   0.19512  0.80039 -2.042  0.04118 * \nNR4A1AS         0.48095   1.61761  0.22753  2.114  0.03453 * \nAC093010.3     -0.65126   0.52139  0.24377 -2.672  0.00755 **\nPCAT6           0.34371   1.41017  0.14756  2.329  0.01985 * \nAC036108.3      1.48666   4.42231  0.96894  1.534  0.12495   \nAL049555.1      0.23994   1.27117  0.15931  1.506  0.13203   \n`MIR1-1HG-AS1` -1.95396   0.14171  1.21419 -1.609  0.10756   \n`SPINT1-AS1`    0.48314   1.62116  0.23919  2.020  0.04340 * \nLINC01980      -0.17331   0.84087  0.08564 -2.024  0.04300 * \n`ZNF710-AS1`   -0.55489   0.57414  0.35644 -1.557  0.11953   \nAL161431.1      0.17192   1.18758  0.08854  1.942  0.05217 . \n`PCCA-DT`      -0.35372   0.70207  0.19499 -1.814  0.06966 . \nU62317.1       -0.20670   0.81326  0.12168 -1.699  0.08937 . \nAL023284.4     -0.30170   0.73956  0.15674 -1.925  0.05425 . \nAC015912.3     -0.43266   0.64878  0.18428 -2.348  0.01888 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n               exp(coef) exp(-coef) lower .95 upper .95\nAC005180.2        2.3101     0.4329   1.27311    4.1916\nMIR100HG          1.8182     0.5500   1.03470    3.1949\nAP001107.5        0.1951     5.1251   0.04065    0.9367\nNR4A1AS           1.6176     0.6182   1.03563    2.5266\nAC093010.3        0.5214     1.9180   0.32334    0.8407\nPCAT6             1.4102     0.7091   1.05601    1.8831\nAC036108.3        4.4223     0.2261   0.66204   29.5404\nAL049555.1        1.2712     0.7867   0.93025    1.7370\n`MIR1-1HG-AS1`    0.1417     7.0566   0.01312    1.5308\n`SPINT1-AS1`      1.6212     0.6168   1.01443    2.5908\nLINC01980         0.8409     1.1892   0.71094    0.9946\n`ZNF710-AS1`      0.5741     1.7417   0.28551    1.1546\nAL161431.1        1.1876     0.8420   0.99839    1.4126\n`PCCA-DT`         0.7021     1.4244   0.47908    1.0288\nU62317.1          0.8133     1.2296   0.64070    1.0323\nAL023284.4        0.7396     1.3522   0.54395    1.0055\nAC015912.3        0.6488     1.5413   0.45211    0.9310\n\nConcordance= 0.735  (se = 0.029 )\nLikelihood ratio test= 54.26  on 17 df,   p=9e-06\nWald test            = 47.59  on 17 df,   p=1e-04\nScore (logrank) test = 51.16  on 17 df,   p=3e-05\n\n最终59个变量剩下17个，筛选效果还不错。\n这个筛选过程是根据AIC进行的，一般会选择AIC最小的结果。AIC全称赤池信息量准则(Akaike information criterion，AIC)，是评估统计模型的复杂度和衡量统计模型”拟合优度”（Goodness of Fit）的一种标准，是由日本统计学家赤池弘次创立和发展的。赤池信息量准则的方法是寻找可以最好地解释数据但包含最少自由参数的模型。\n查看最终的AIC和BIC：\n\n# 初始模型的AIC\nAIC(fit.cox)\n## [1] 794.1182\n\n# 筛选后的AIC和BIC\nAIC(fit.step)\n## [1] 734.7167\nBIC(fit.step)\n## [1] 773.1823\n\n查看回归系数：\n\nstep.coef &lt;- coef(fit.step)\nstep.coef\n##     AC005180.2       MIR100HG     AP001107.5        NR4A1AS     AC093010.3 \n##      0.8372759      0.5978335     -1.6341403      0.4809485     -0.6512620 \n##          PCAT6     AC036108.3     AL049555.1 `MIR1-1HG-AS1`   `SPINT1-AS1` \n##      0.3437133      1.4866631      0.2399385     -1.9539623      0.4831391 \n##      LINC01980   `ZNF710-AS1`     AL161431.1      `PCCA-DT`       U62317.1 \n##     -0.1733137     -0.5548853      0.1719177     -0.3537250     -0.2067033 \n##     AL023284.4     AC015912.3 \n##     -0.3017031     -0.4326573\n\n提取这17个变量的名字：\n\nstep.lnc &lt;- names(coef(fit.step))\nstep.lnc\n##  [1] \"AC005180.2\"     \"MIR100HG\"       \"AP001107.5\"     \"NR4A1AS\"       \n##  [5] \"AC093010.3\"     \"PCAT6\"          \"AC036108.3\"     \"AL049555.1\"    \n##  [9] \"`MIR1-1HG-AS1`\" \"`SPINT1-AS1`\"   \"LINC01980\"      \"`ZNF710-AS1`\"  \n## [13] \"AL161431.1\"     \"`PCCA-DT`\"      \"U62317.1\"       \"AL023284.4\"    \n## [17] \"AC015912.3\"\n\n简单。\n当然使用broom也是可以提取这个结果的：\n\nbroom::tidy(fit.step)\n## # A tibble: 17 × 5\n##    term           estimate std.error statistic p.value\n##    &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n##  1 AC005180.2        0.837    0.304       2.75 0.00588\n##  2 MIR100HG          0.598    0.288       2.08 0.0377 \n##  3 AP001107.5       -1.63     0.800      -2.04 0.0412 \n##  4 NR4A1AS           0.481    0.228       2.11 0.0345 \n##  5 AC093010.3       -0.651    0.244      -2.67 0.00755\n##  6 PCAT6             0.344    0.148       2.33 0.0198 \n##  7 AC036108.3        1.49     0.969       1.53 0.125  \n##  8 AL049555.1        0.240    0.159       1.51 0.132  \n##  9 `MIR1-1HG-AS1`   -1.95     1.21       -1.61 0.108  \n## 10 `SPINT1-AS1`      0.483    0.239       2.02 0.0434 \n## 11 LINC01980        -0.173    0.0856     -2.02 0.0430 \n## 12 `ZNF710-AS1`     -0.555    0.356      -1.56 0.120  \n## 13 AL161431.1        0.172    0.0885      1.94 0.0522 \n## 14 `PCCA-DT`        -0.354    0.195      -1.81 0.0697 \n## 15 U62317.1         -0.207    0.122      -1.70 0.0894 \n## 16 AL023284.4       -0.302    0.157      -1.92 0.0542 \n## 17 AC015912.3       -0.433    0.184      -2.35 0.0189\n\n一目了然，简洁清晰，broom真的是神包~\n还可以1行代码查看模型的各种统计值，包括P值、R^2、AIC、BIC、C-index等等：\n\nbroom::glance(fit.step)\n## # A tibble: 1 × 18\n##       n nevent statistic.log p.value.log statistic.sc p.value.sc statistic.wald\n##   &lt;int&gt;  &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;\n## 1   297     71          54.3  0.00000900         51.2  0.0000279           47.6\n## # ℹ 11 more variables: p.value.wald &lt;dbl&gt;, statistic.robust &lt;dbl&gt;,\n## #   p.value.robust &lt;dbl&gt;, r.squared &lt;dbl&gt;, r.squared.max &lt;dbl&gt;,\n## #   concordance &lt;dbl&gt;, std.error.concordance &lt;dbl&gt;, logLik &lt;dbl&gt;, AIC &lt;dbl&gt;,\n## #   BIC &lt;dbl&gt;, nobs &lt;int&gt;"
  },
  {
    "objectID": "feature-selection_stepwise.html#自助法stepwise",
    "href": "feature-selection_stepwise.html#自助法stepwise",
    "title": "12  筛选变量逐步回归",
    "section": "12.4 自助法stepwise",
    "text": "12.4 自助法stepwise\n这里再给大家介绍下一种自助法stepwise，可以通过自助法重抽样进行逐步筛选变量，比如进行1000次bootstrap。\n该方法借助bootStepAIC实现，是基于stepAIC()函数的，支持”lm”, “aov”,“glm”, “negbin”, “polr”, “survreg”,以及”coxph”。\n使用方法也很简单，下面是一个10次bootstrap的逐步选择法(因为耗时太长了，我只用了10次) ：\n\nlibrary(bootStepAIC)\n\n# 10次bootstrap\nfit.boot &lt;- boot.stepAIC(fit.cox,data=dat.cox,direction=\"both\",B=10,seed=123)\n#fit.boot\n\n\nFinal Model:\nSurv(time_months, event) ~ AC005180.2 + MIR100HG + AP001107.5 + \n    NR4A1AS + AC093010.3 + PCAT6 + AC036108.3 + AL049555.1 + \n    `MIR1-1HG-AS1` + `SPINT1-AS1` + LINC01980 + `ZNF710-AS1` + \n    AL161431.1 + `PCCA-DT` + U62317.1 + AL023284.4 + AC015912.3\n\n提取变量名字：\n\nnames(coef(fit.boot$OrigStepAIC))\n\n [1] \"AC005180.2\"     \"MIR100HG\"       \"AP001107.5\"     \"NR4A1AS\"       \n [5] \"AC093010.3\"     \"PCAT6\"          \"AC036108.3\"     \"AL049555.1\"    \n [9] \"`MIR1-1HG-AS1`\" \"`SPINT1-AS1`\"   \"LINC01980\"      \"`ZNF710-AS1`\"  \n[13] \"AL161431.1\"     \"`PCCA-DT`\"      \"U62317.1\"       \"AL023284.4\"    \n[17] \"AC015912.3\"  \n\n和不进行bootstrap的方法得到的结果是一样的：\n\nidentical(sort(step.lnc),sort(names(coef(fit.boot$OrigStepAIC))))\n\n[1] TRUE"
  },
  {
    "objectID": "feature-selection_bestsubset.html#准备数据",
    "href": "feature-selection_bestsubset.html#准备数据",
    "title": "13  变量筛选之最优子集",
    "section": "13.1 准备数据",
    "text": "13.1 准备数据\n使用孙振球版医学统计学例15-1的数据。\n\ndf &lt;- data.frame(\n  cho = c(5.68,3.79,6.02,4.85,4.60,6.05,4.90,7.08,3.85,4.65,4.59,4.29,7.97,\n      6.19,6.13,5.71,6.40,6.06,5.09,6.13,5.78,5.43,6.50,7.98,11.54,5.84,\n      3.84),\n  tg = c(1.90,1.64,3.56,1.07,2.32,0.64,8.50,3.00,2.11,0.63,1.97,1.97,1.93,\n      1.18,2.06,1.78,2.40,3.67,1.03,1.71,3.36,1.13,6.21,7.92,10.89,0.92,\n      1.20),\n  ri = c(4.53, 7.32,6.95,5.88,4.05,1.42,12.60,6.75,16.28,6.59,3.61,6.61,7.57,\n      1.42,10.35,8.53,4.53,12.79,2.53,5.28,2.96,4.31,3.47,3.37,1.20,8.61,\n      6.45),\n  hba = c(8.2,6.9,10.8,8.3,7.5,13.6,8.5,11.5,7.9,7.1,8.7,7.8,9.9,6.9,10.5,8.0,\n      10.3,7.1,8.9,9.9,8.0,11.3,12.3,9.8,10.5,6.4,9.6),\n  fpg = c(11.2,8.8,12.3,11.6,13.4,18.3,11.1,12.1,9.6,8.4,9.3,10.6,8.4,9.6,10.9,\n     10.1,14.8,9.1,10.8,10.2,13.6,14.9,16.0,13.2,20.0,13.3,10.4)\n  )"
  },
  {
    "objectID": "feature-selection_bestsubset.html#建立模型",
    "href": "feature-selection_bestsubset.html#建立模型",
    "title": "13  变量筛选之最优子集",
    "section": "13.2 建立模型",
    "text": "13.2 建立模型\n使用全部的4个变量建立回归方程：\n\nf &lt;- lm(fpg ~ cho + tg + ri + hba, data = df)\n\nsummary(f)\n## \n## Call:\n## lm(formula = fpg ~ cho + tg + ri + hba, data = df)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -3.6268 -1.2004 -0.2276  1.5389  4.4467 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)  \n## (Intercept)   5.9433     2.8286   2.101   0.0473 *\n## cho           0.1424     0.3657   0.390   0.7006  \n## tg            0.3515     0.2042   1.721   0.0993 .\n## ri           -0.2706     0.1214  -2.229   0.0363 *\n## hba           0.6382     0.2433   2.623   0.0155 *\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 2.01 on 22 degrees of freedom\n## Multiple R-squared:  0.6008, Adjusted R-squared:  0.5282 \n## F-statistic: 8.278 on 4 and 22 DF,  p-value: 0.0003121"
  },
  {
    "objectID": "feature-selection_bestsubset.html#最优子集法",
    "href": "feature-selection_bestsubset.html#最优子集法",
    "title": "13  变量筛选之最优子集",
    "section": "13.3 最优子集法",
    "text": "13.3 最优子集法\n使用最优子集法筛选变量，借助leaps包实现。使用起来其实就是1行代码而已：\n\nlibrary(leaps)\nleaps &lt;- regsubsets(fpg ~ cho + tg + ri + hba, data = df)\nsummary(leaps)\n## Subset selection object\n## Call: regsubsets.formula(fpg ~ cho + tg + ri + hba, data = df)\n## 4 Variables  (and intercept)\n##     Forced in Forced out\n## cho     FALSE      FALSE\n## tg      FALSE      FALSE\n## ri      FALSE      FALSE\n## hba     FALSE      FALSE\n## 1 subsets of each size up to 4\n## Selection Algorithm: exhaustive\n##          cho tg  ri  hba\n## 1  ( 1 ) \" \" \" \" \" \" \"*\"\n## 2  ( 1 ) \"*\" \" \" \" \" \"*\"\n## 3  ( 1 ) \" \" \"*\" \"*\" \"*\"\n## 4  ( 1 ) \"*\" \"*\" \"*\" \"*\"\n\n*表示变量被包含在模型中，这个结果看起来并不是很直观，下面会结合图进行解释。"
  },
  {
    "objectID": "feature-selection_bestsubset.html#子集选择",
    "href": "feature-selection_bestsubset.html#子集选择",
    "title": "13  变量筛选之最优子集",
    "section": "13.4 子集选择",
    "text": "13.4 子集选择\n我们首先查看rss(Residual sum of squares，残差平方和)最小的结果，因为RSS越小说明模型拟合结果越好：\n\nwhich.min(summary(leaps)$rss)\n## [1] 4\n\n结果表明有4个特征的模型具有最小的RSS，这是很明显的哈，因为这里就有4个自变量。。。\n增加特征数量必然会减少RSS！而且必然会增加R方。我们即使添加一个完全不相关的特征，比如洛杉矶湖人队的胜场数，模型的RSS也会减少，R方也会增加。\n所以只看RSS并不能帮助我们很好的选择变量。\n我们这里讨论4种用于特征选择的统计方法：\n\n赤池信息量准则\n马洛斯的Cp\n贝叶斯准则\n调整的R方\n\n赤池信息量准则(Akaike information criterion，AIC)，是评估统计模型的复杂度和衡量统计模型”拟合优度”（Goodness of Fit）的一种标准，是由日本统计学家赤池弘次创立和发展的。赤池信息量准则的方法是寻找可以最好地解释数据但包含最少自由参数的模型。\nAIC的计算方法如下，其中p是模型中的特征数量(也就是自变量数量)，n是样本大小：\n\\[\n\\mathrm{AIC}=n*\\log\\Bigg(\\frac{\\mathrm{RSS}_p}{n}\\Bigg)+2*p\n\\]\n贝叶斯信息量准则(Bayesian information criterion，BIC)和AIC类似，只不过BIC比AIC的惩罚力度更大。\nBIC的计算方法如下，BIC的前半部分计算和AIC是完全一样的：\n\\[\n\\mathrm{BIC}=n*\\log\\left(\\frac{\\mathrm{RSS}_p}n\\right)+p*\\log(n)\n\\]\nAIC和BIC的不同点：\n\nBIC的惩罚项比AIC大，考虑了样本个数，样本数量多，可以防止模型精度过高造成的模型复杂度过高。 AIC和BIC前半部分是一样的，BIC考虑了样本数量，样本数量过多时，可有效防止模型精度过高造成的模型复杂度过高。–参考资料2\n\n马洛斯的Cp(Mallows’Cp)与AIC的计算也是类似，其计算公式如下，其中MSE(mean-square error, MSE)是均方误差：\n\\[\n\\mathrm{CP}=\\frac{\\mathrm{RSS}_{p}}{\\mathrm{MSE}_{f}}-n+2*p\n\\]\n调整的R^2(修正R方)的计算公式如下：\n\\[\n\\text{修正R方}=1-\\left(\\frac{\\mathrm{RSS}}{n-p-1}\\right)/\\left(\\frac{\\mathrm{R}\\text{方}}{ n - 1 }\\right)\n\\]\n前三种方法的目标是追求统计量的值最小化，调整的R方的目标是追求统计量的值最大化。这些统计方法的目的是建立一个尽可能简约的模型，换句话说，要对模型复杂性进行”惩罚”。\n在线性模型中，AIC和Cp成正比，所以我们只需关注Cp即可。\n下面以CP为纵坐标进行可视化：\n\npar(mfrow=c(1,2))\nplot(summary(leaps)$cp,type = \"l\",xlab = \"number of features\",ylab = \"cp\")\nplot(leaps, scale = \"Cp\") # 通过Cp判断\n\n\n\n\n左图横坐标是自变量数量，纵坐标是CP值大小，可以看到在变量数量为3时，CP是最小的；右边的图也是一样的意思，先看纵坐标，CP最小是3.2，此时对应的黑色块是tg,ri,hba这3个变量。\n也可以用其他指标作为纵坐标进行可视化，比如调整的R2adjr2:\n\nplot(leaps, scale = \"adjr2\")\n\n\n\n\n或者BIC，BIC和AIC意思是一样的：\n\nplot(leaps, scale = \"bic\")\n\n\n\n\n可以看到3种方法得到的结果都是一样的，都是选出了tg,ri,hba这3个变量，但有时结果也是不太一样的，不用纠结。\n上面是比较传统的方法，但是说实话得到的图不是很好看，如果没有教程完全不知道如何解读，非常的不优雅。\n神包broom可以用于这个筛选结果，结果会返回一个tibble，看起来非常清稀易懂：\n\nbroom::tidy(leaps)\n## # A tibble: 4 × 9\n##   `(Intercept)` cho   tg    ri    hba   r.squared adj.r.squared    BIC\n##   &lt;lgl&gt;         &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;     &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;\n## 1 TRUE          FALSE FALSE FALSE TRUE      0.372         0.347  -5.95\n## 2 TRUE          TRUE  FALSE FALSE TRUE      0.484         0.441  -7.99\n## 3 TRUE          FALSE TRUE  TRUE  TRUE      0.598         0.546 -11.4 \n## 4 TRUE          TRUE  TRUE  TRUE  TRUE      0.601         0.528  -8.32\n## # ℹ 1 more variable: mallows_cp &lt;dbl&gt;\n\n可以看到如果是根据R方来选择，则R方最大时4个变量都在，如果是根据BIC最小选择，那结果是tg,ri,hba这3个变量入选。这样我们不用画图也可以很直观的看出在不同指标下应该选择哪几个变量。\n如果还想更加炫酷一点，可以把这个结果变成一个图表（参考文章）：\n\nlibrary(gt)\nlibrary(tidyverse)\nlibrary(scales)\n\nbroom::tidy(leaps) %&gt;%\n  select(-`(Intercept)`) %&gt;%\n  rownames_to_column(var = \"n_vars\") %&gt;%\n  gt(rowname_col = \"n_vars\") %&gt;%\n  gt::data_color(\n    columns = cho:hba,\n    fn = col_numeric(\n      palette = c(\"#fdae61\", \"#abdda4\"),\n      domain = c(0, 1)) \n  ) %&gt;%\n  gt::fmt_number(r.squared:mallows_cp, n_sigfig = 4)\n\n\n\n\n\n  \n    \n    \n      \n      cho\n      tg\n      ri\n      hba\n      r.squared\n      adj.r.squared\n      BIC\n      mallows_cp\n    \n  \n  \n    1\nFALSE\nFALSE\nFALSE\nTRUE\n0.3717\n0.3465\n−5.955\n11.63\n    2\nTRUE\nFALSE\nFALSE\nTRUE\n0.4843\n0.4414\n−7.995\n7.419\n    3\nFALSE\nTRUE\nTRUE\nTRUE\n0.5981\n0.5456\n−11.43\n3.152\n    4\nTRUE\nTRUE\nTRUE\nTRUE\n0.6008\n0.5282\n−8.315\n5.000\n  \n  \n  \n\n\n\n\n这样一来更加直观了，这里我用绿色表示留下的变量，黄色表示被剔除的变量，后面几列是对应的各种指标。\n做到这一步的时候，我真的觉得R语言太牛逼了，大神的可视化思路太强了，我差的太远了。\n如果只是想要看一下不同指标下保留的变量个数，我们可以根据上面的结果用ggplot2画图。\n\nbroom::tidy(leaps) %&gt;%\n  select(r.squared:mallows_cp) %&gt;%\n  mutate(n_vars = 1:n()) %&gt;%\n  pivot_longer(cols = -n_vars, names_to = \"metric\") %&gt;%\n  ggplot(aes(x = n_vars, y = value)) +\n  geom_point(size = 2) +\n  geom_line(linewidth = 1) +\n  geom_vline(\n    data = . %&gt;%\n      group_by(metric) %&gt;%\n      filter(value == ifelse(str_detect(metric, \"r.squared\"),\n                             max(value), min(value))),\n    aes(xintercept = n_vars), lty = 2) +\n  theme_bw()+\n  facet_wrap(~ metric, scales = \"free_y\")\n\n\n\n\n强大！牛B！专业！"
  },
  {
    "objectID": "feature-selection_bestsubset.html#参考资料",
    "href": "feature-selection_bestsubset.html#参考资料",
    "title": "13  变量筛选之最优子集",
    "section": "13.5 参考资料",
    "text": "13.5 参考资料\n\n维基百科：AIC\nhttps://zhuanlan.zhihu.com/p/293315874\nhttps://bookdown.org/taylordunn/islr-tidy-1655226885741/"
  },
  {
    "objectID": "feature-selection_lasso.html#安装",
    "href": "feature-selection_lasso.html#安装",
    "title": "14  lasso回归筛选变量",
    "section": "14.1 安装",
    "text": "14.1 安装\n\ninstall.packages(\"glmnet\")"
  },
  {
    "objectID": "feature-selection_lasso.html#建模",
    "href": "feature-selection_lasso.html#建模",
    "title": "14  lasso回归筛选变量",
    "section": "14.2 建模",
    "text": "14.2 建模\n\nlibrary(glmnet)\n## Loading required package: Matrix\n## Loaded glmnet 4.1-8\n\n用一个二分类数据进行演示，因为大家最常用的就是二分类数据和生存数据了。\n\ndata(BinomialExample)\nx &lt;- BinomialExample$x\ny &lt;- BinomialExample$y\n\ndim(x)\n## [1] 100  30\nclass(x)\n## [1] \"matrix\" \"array\"\nx[1:4,1:4]\n##            [,1]       [,2]       [,3]       [,4]\n## [1,] -0.6192614 0.01624409 -0.6260683  0.4126846\n## [2,]  1.0942728 0.47257285 -1.3371470 -0.6405813\n## [3,] -0.3567040 0.30121334  0.1905619  0.2340268\n## [4,] -2.4690701 2.84771447  1.6602435  1.5688130\n\nclass(y)\n## [1] \"integer\"\nhead(y)\n## [1] 0 1 1 0 1 0\n\n注意glmnet需要的自变量格式，需要是matrix或者稀疏矩阵格式！\nfamily用来指定不同的模型类型，对于二分类数据，应该选择binomial。\nfamily的其他选项如下：“gaussian”（默认）, “poisson”, “multinomial”, “cox”, “mgaussian”。\n建立模型就是1句代码，非常简单：\n\nfit &lt;- glmnet(x, y, family = \"binomial\")\n\n官方不建议直接提取fit中的元素，因为提供了plot，print，coef，predict方法帮助大家探索结果。"
  },
  {
    "objectID": "feature-selection_lasso.html#可视化",
    "href": "feature-selection_lasso.html#可视化",
    "title": "14  lasso回归筛选变量",
    "section": "14.3 可视化",
    "text": "14.3 可视化\n可视化各个变量系数的变化，这个图是大家最常见的图形之一：\n\nplot(fit,label = T)\n\n\n\n\n这个图形中的每一条线都代表1个变量，并且展示了在不同的L1范数（L1 Norm）下该变量的系数变化。这个图下面的横坐标是L1范数，上面的横坐标是L1范数下对应的非零系数的个数，比如当L1范数是20时，对应的非零系数有27个，也就是此时可以有27个变量保留下来。左侧纵坐标是变量的系数值。\n这里的plot()函数还有一个xvar参数，可以用于指定不同的横坐标：\n\nnorm：横坐标是L1 norm，这个是默认值；\nlambda：横坐标是log-lambda；\ndev：横坐标是模型解释的%deviance\n\n\nplot(fit, xvar = \"lambda\")\n\n\n\n\n这里的横坐标是log-lambda，可以看做是正则化程度。\n上面这幅图展示了随着lambda值的变化，每个变量系数的变化，可以看到随着lambda值变大，系数值逐渐减小，直至为0，上面的横坐标也显示随着lambda值变大，保留的变量数量也越来越少。\n\nplot(fit, xvar = \"dev\", label = TRUE)\n\n\n\n\n这幅图和上面图的解释是一样的，只有下面的横坐标不一样。\n最后一幅图下面的横坐标是模型解释的偏差百分比，也可以用来衡量模型复杂度。可以看出在图形的右侧部分，模型能够解释的偏差百分比基本变化不大，但是模型系数基本都是往上或往下“飘”的很厉害。\n虽然官方不建议提取数据，但是很明显大家都喜欢提取数据再自己美化图片，我之前也介绍过一种简便方法，可以实现自定义美化图形：lasso回归结果美化"
  },
  {
    "objectID": "feature-selection_lasso.html#打印结果",
    "href": "feature-selection_lasso.html#打印结果",
    "title": "14  lasso回归筛选变量",
    "section": "14.4 打印结果",
    "text": "14.4 打印结果\n使用print(fit)可以查看不同lambda值对应的自由度和模型能够解释的偏差百分比：\n\nprint(fit) # 直接fit也可\n## \n## Call:  glmnet(x = x, y = y, family = \"binomial\") \n## \n##    Df  %Dev   Lambda\n## 1   0  0.00 0.240500\n## 2   1  2.90 0.219100\n## 3   1  5.34 0.199600\n## 4   2  8.86 0.181900\n## 5   2 11.95 0.165800\n## 6   2 14.59 0.151000\n## 7   2 16.88 0.137600\n## 8   3 18.95 0.125400\n## 9   7 22.38 0.114200\n## 10  8 26.26 0.104100\n## 11  8 29.73 0.094850\n## 12  8 32.77 0.086420\n## 13  9 35.58 0.078750\n## 14 11 38.98 0.071750\n## 15 12 42.23 0.065380\n## 16 12 45.29 0.059570\n## 17 13 48.09 0.054280\n## 18 13 50.63 0.049450\n## 19 14 53.00 0.045060\n## 20 14 55.19 0.041060\n## 21 15 57.33 0.037410\n## 22 15 59.43 0.034090\n## 23 16 61.36 0.031060\n## 24 17 63.15 0.028300\n## 25 17 64.85 0.025790\n## 26 18 66.42 0.023490\n## 27 19 67.98 0.021410\n## 28 20 69.44 0.019510\n## 29 20 70.80 0.017770\n## 30 21 72.10 0.016190\n## 31 21 73.33 0.014760\n## 32 23 74.52 0.013440\n## 33 23 75.65 0.012250\n## 34 24 76.72 0.011160\n## 35 24 77.77 0.010170\n## 36 25 78.77 0.009267\n## 37 25 79.73 0.008444\n## 38 26 80.66 0.007693\n## 39 26 81.57 0.007010\n## 40 27 82.48 0.006387\n## 41 27 83.39 0.005820\n## 42 27 84.30 0.005303\n## 43 27 85.21 0.004832\n## 44 27 86.12 0.004402\n## 45 27 87.05 0.004011\n## 46 28 87.96 0.003655\n## 47 28 88.87 0.003330\n## 48 28 89.76 0.003034\n## 49 28 90.61 0.002765\n## 50 28 91.41 0.002519\n## 51 28 92.16 0.002295\n## 52 28 92.86 0.002092\n## 53 28 93.50 0.001906\n## 54 28 94.08 0.001736\n## 55 29 94.61 0.001582\n## 56 29 95.10 0.001442\n## 57 29 95.54 0.001314\n## 58 29 95.95 0.001197\n## 59 29 96.31 0.001091\n## 60 29 96.64 0.000994\n## 61 29 96.94 0.000905\n## 62 29 97.22 0.000825\n## 63 29 97.47 0.000752\n## 64 29 97.69 0.000685\n## 65 29 97.90 0.000624\n## 66 29 98.09 0.000569\n## 67 29 98.26 0.000518\n## 68 29 98.41 0.000472\n## 69 29 98.55 0.000430\n## 70 29 98.68 0.000392\n## 71 29 98.80 0.000357\n## 72 30 98.91 0.000325\n## 73 30 99.00 0.000296\n## 74 30 99.09 0.000270\n## 75 30 99.17 0.000246\n## 76 30 99.25 0.000224\n## 77 30 99.31 0.000204\n## 78 30 99.37 0.000186\n## 79 30 99.43 0.000170\n## 80 30 99.48 0.000155\n## 81 30 99.52 0.000141\n## 82 30 99.57 0.000128\n## 83 30 99.61 0.000117\n## 84 30 99.64 0.000106\n## 85 30 99.67 0.000097\n## 86 30 99.70 0.000088\n## 87 30 99.73 0.000081\n## 88 30 99.75 0.000073\n## 89 30 99.77 0.000067\n## 90 30 99.79 0.000061\n## 91 30 99.81 0.000056\n## 92 30 99.83 0.000051\n## 93 30 99.84 0.000046\n## 94 30 99.86 0.000042\n## 95 30 99.87 0.000038\n## 96 30 99.88 0.000035\n## 97 30 99.89 0.000032\n## 98 30 99.90 0.000029\n\n左侧的df是非零系数的个数，中间的%Dev是模型解释的偏差百分比，右侧的Lambda是总惩罚值大小。\n默认情况下，glmnet()函数中的nlambda参数的取值是100，也就是会取100个不同的Lambda值，但是如果%Dev变化不大或者不再变化，它可能会提前停止，取不到100个值，比如我们这个例子就是这样。"
  },
  {
    "objectID": "feature-selection_lasso.html#查看变量系数",
    "href": "feature-selection_lasso.html#查看变量系数",
    "title": "14  lasso回归筛选变量",
    "section": "14.5 查看变量系数",
    "text": "14.5 查看变量系数\n我们可以通过coef()查看某个Lambda值下的变量系数：\n\ncoef(fit, s = 0.065380)\n## 31 x 1 sparse Matrix of class \"dgCMatrix\"\n##                       s1\n## (Intercept)  0.210158382\n## V1           .          \n## V2           0.193006823\n## V3          -0.069820214\n## V4          -0.606741531\n## V5          -0.081962193\n## V6          -0.285761723\n## V7           .          \n## V8          -0.165879158\n## V9           0.092678665\n## V10         -0.595865115\n## V11          .          \n## V12          .          \n## V13          .          \n## V14          .          \n## V15          .          \n## V16          .          \n## V17          .          \n## V18          .          \n## V19          .          \n## V20          .          \n## V21          .          \n## V22          0.054956208\n## V23          0.001474751\n## V24          .          \n## V25          0.187112112\n## V26         -0.113782733\n## V27          .          \n## V28          .          \n## V29          .          \n## V30          .\n\n可以看到此时一共有12个变量的系数不是0，和上面print(fit)的结果是一样的。\n这里使用了s表示lambda，为什么不直接用lambda呢？这是作者为了以后的某些功能做准备，但是这一点在tidymodels中大受诟病…\n也可以同时指定多个lambda值：\n\ncoef(fit, s = c(0.065380,0.078750))\n## 31 x 2 sparse Matrix of class \"dgCMatrix\"\n##                       s1          s2\n## (Intercept)  0.210158382  0.22467551\n## V1           .            .         \n## V2           0.193006823  0.13578915\n## V3          -0.069820214  .         \n## V4          -0.606741531 -0.55088786\n## V5          -0.081962193 -0.08588769\n## V6          -0.285761723 -0.18303729\n## V7           .            .         \n## V8          -0.165879158 -0.12710236\n## V9           0.092678665  .         \n## V10         -0.595865115 -0.50054790\n## V11          .            .         \n## V12          .            .         \n## V13          .            .         \n## V14          .            .         \n## V15          .            .         \n## V16          .            .         \n## V17          .            .         \n## V18          .            .         \n## V19          .            .         \n## V20          .            .         \n## V21          .            .         \n## V22          0.054956208  0.01466017\n## V23          0.001474751  .         \n## V24          .            .         \n## V25          0.187112112  0.13534486\n## V26         -0.113782733 -0.08255906\n## V27          .            .         \n## V28          .            .         \n## V29          .            .         \n## V30          .            .\n\n除此之外，coef()还有一个exact参数，如果exact = TRUE，那么当一个lambda不在默认的lambda值中时，函数会重新使用这个lambda值拟合模型然后给出结果，如果exact = FALSE（默认值），那么会使用线性插值给出结果。\n举个例子，0.08并不在lambda值向量中：\n\n# 可以看前面的print(fit)的结果，看看lambda的取值有哪些\nany(fit$lambda == 0.08)\n## [1] FALSE\n\n此时两种情况下的系数是不太一样的：\n\ncoef.apprx &lt;- coef(fit, s = 0.08, exact = FALSE)\ncoef.exact &lt;- coef(fit, s = 0.08, exact = TRUE, x=x, y=y)\ncbind2(coef.exact[which(coef.exact != 0)], \n       coef.apprx[which(coef.apprx != 0)])\n##              [,1]        [,2]\n##  [1,]  0.22549572  0.22541853\n##  [2,]  0.13138628  0.13159475\n##  [3,] -0.54737500 -0.54723674\n##  [4,] -0.08464614 -0.08430109\n##  [5,] -0.17544453 -0.17586695\n##  [6,] -0.12334038 -0.12323991\n##  [7,] -0.49261301 -0.49314684\n##  [8,]  0.01036968  0.01227180\n##  [9,]  0.13183895  0.13169100\n## [10,] -0.07909589 -0.07914430\n\n注意在使用exact = TRUE时，需要提供x和y，因为需要重新拟合模型。"
  },
  {
    "objectID": "feature-selection_lasso.html#预测新数据",
    "href": "feature-selection_lasso.html#预测新数据",
    "title": "14  lasso回归筛选变量",
    "section": "14.6 预测新数据",
    "text": "14.6 预测新数据\n对于新数据，可直接使用predict()进行预测，此时也是可以指定lambda值的：\n\nnx &lt;- head(x) #随便准备的新的测试数据\n\npredict(fit, newx = nx, s = c(0.065380,0.078750))\n##              s1         s2\n## [1,] -0.7609757 -0.5755105\n## [2,]  1.4563904  1.1266031\n## [3,]  0.4415409  0.3981256\n## [4,] -1.1676684 -0.9923334\n## [5,]  0.5730604  0.5612494\n## [6,]  0.3064590  0.1926588\n\n由于glmnet包可以用于线性回归、逻辑回归、cox回归、泊松回归、多项式回归等（通过参数family指定即可，默认值是gaussian，可通过?glmnet查看帮助文档），所以在predict()时，type参数略有不同，对于逻辑回归，type可以是以下3种：\n\nlink：线性预测值，默认是这个\nresponse：预测概率\nclass：预测类别\n\n如果要获得预测概率：\n\npredict(fit, newx = nx, s = c(0.065380,0.078750), type = \"response\")\n##             s1        s2\n## [1,] 0.3184345 0.3599663\n## [2,] 0.8109800 0.7552115\n## [3,] 0.6086261 0.5982372\n## [4,] 0.2372767 0.2704514\n## [5,] 0.6394690 0.6367416\n## [6,] 0.5760207 0.5480163\n\n可以通过?predict.glmnet查看帮助文档。"
  },
  {
    "objectID": "feature-selection_lasso.html#交叉验证",
    "href": "feature-selection_lasso.html#交叉验证",
    "title": "14  lasso回归筛选变量",
    "section": "14.7 交叉验证",
    "text": "14.7 交叉验证\nglmnet()函数会返回多个模型（因为会使用多个lambda值），但是很多情况下，用户并不知道到底选择哪一个lambda值，即不知道到底保留哪些变量，或者希望函数能自动给出结果。\n所以glmnet包提供了交叉验证法，帮助用户做出选择，使用方法也非常简单：\n\ncvfit &lt;- cv.glmnet(x, y)\n\n除了glmnet()中的参数之外，cv.glmnet()还有一些独有的参数：\n\nnfolds：交叉验证的折数，默认是10折交叉验证；\nfoldid：指定哪个观测在哪一折中，一般用不到；\ntype.measure：模型性能指标，对于不同的family，也是略有不同，可查看帮助文档\n\n对于逻辑回归，type.measure可以是以下取值：\n\nmse：均方误差；\ndeviance：偏差；\nmae：平均绝对误差，mean absolute error；\nclass：错分率；\nauc：只能用于二分类逻辑回归\n\n\n14.7.1 plot方法\n对于cv.glmnet()的结果，也提供了plot，print，coef，predict方法。\n\nplot(cvfit)\n\n\n\n\n该图形下面的横坐标是log10(lambda)，上面的横坐标是非零系数的数量，左侧的纵坐标是MSE（均方误差），改图展示了不同lambda取值下MSE的变化以及MSE±1倍标准差的置信区间。\n图中的两条竖线就是函数帮你挑选的两个结果，一个是lambda.min，此时的lambda值可以使得MSE最小，另外一个是lambda.1se，此时的lambda值可以使得MSE在最小MSE的1倍标准误区间内，但是同时可以使模型的复杂度降低。（在模型误差之间的差距不是很大的时候，我们肯定是喜欢更简单的模型啦，这个不难理解吧？）\n查看这两个lambda值：\n\ncvfit$lambda.min\n## [1] 0.02829953\ncvfit$lambda.1se\n## [1] 0.04945423\n\n换一个type.measure试试看：\n\ncvfit1 &lt;- cv.glmnet(x, y, family = \"binomial\", type.measure = \"auc\")\nplot(cvfit1)\n\n\n\n\n这个图的解读和上面那个图的解读也是一样的，只不过左侧纵坐标不一样而已。\n交叉验证的图形也是可以自己美化的，参考推文：lasso回归结果美化\n\n\n14.7.2 coef方法\n查看这两个取值下保留的非零系数情况：\n\n# 此时s不能同时使用多个值\ncoef(cvfit, s = \"lambda.min\")\n## 31 x 1 sparse Matrix of class \"dgCMatrix\"\n##                       s1\n## (Intercept)  0.538475144\n## V1           .          \n## V2           0.054881645\n## V3          -0.044528447\n## V4          -0.145609945\n## V5          -0.023218071\n## V6          -0.104593647\n## V7           .          \n## V8          -0.057762149\n## V9           0.073669351\n## V10         -0.148264046\n## V11         -0.009489879\n## V12          .          \n## V13         -0.002021430\n## V14          .          \n## V15          .          \n## V16          0.013778043\n## V17          .          \n## V18          .          \n## V19          .          \n## V20          .          \n## V21          .          \n## V22          0.032149957\n## V23          0.034431329\n## V24          .          \n## V25          0.069884641\n## V26         -0.050479757\n## V27          .          \n## V28          0.021367958\n## V29         -0.021118533\n## V30          .\ncoef(cvfit, s = \"lambda.1se\") # 这个是默认值\n## 31 x 1 sparse Matrix of class \"dgCMatrix\"\n##                       s1\n## (Intercept)  0.541225753\n## V1           .          \n## V2           0.046087719\n## V3          -0.027102998\n## V4          -0.132840898\n## V5          -0.019424141\n## V6          -0.079980759\n## V7           .          \n## V8          -0.044011217\n## V9           0.043650956\n## V10         -0.133222487\n## V11          .          \n## V12          .          \n## V13          .          \n## V14          .          \n## V15          .          \n## V16          .          \n## V17          .          \n## V18          .          \n## V19          .          \n## V20          .          \n## V21          .          \n## V22          0.022463353\n## V23          0.015349808\n## V24          .          \n## V25          0.052132213\n## V26         -0.037104017\n## V27          .          \n## V28          0.002725018\n## V29         -0.005935917\n## V30          .\n\n可以看到coef()的结果都是稀疏矩阵格式，这种格式计算效率更高，但是不方便后续使用，可以使用as.matrix()转换为矩阵格式：\n\nas.matrix(coef(cvfit))\n##                       s1\n## (Intercept)  0.541225753\n## V1           0.000000000\n## V2           0.046087719\n## V3          -0.027102998\n## V4          -0.132840898\n## V5          -0.019424141\n## V6          -0.079980759\n## V7           0.000000000\n## V8          -0.044011217\n## V9           0.043650956\n## V10         -0.133222487\n## V11          0.000000000\n## V12          0.000000000\n## V13          0.000000000\n## V14          0.000000000\n## V15          0.000000000\n## V16          0.000000000\n## V17          0.000000000\n## V18          0.000000000\n## V19          0.000000000\n## V20          0.000000000\n## V21          0.000000000\n## V22          0.022463353\n## V23          0.015349808\n## V24          0.000000000\n## V25          0.052132213\n## V26         -0.037104017\n## V27          0.000000000\n## V28          0.002725018\n## V29         -0.005935917\n## V30          0.000000000\n\n\n\n14.7.3 predict方法\n对新数据进行预测也是一样的用法：\n\npredict(cvfit, newx = x[1:5,], s = \"lambda.min\")\n##      lambda.min\n## [1,]  0.2880810\n## [2,]  0.9411606\n## [3,]  0.6169352\n## [4,]  0.1604069\n## [5,]  0.5976043"
  },
  {
    "objectID": "feature-selection_lasso.html#一些参数解释",
    "href": "feature-selection_lasso.html#一些参数解释",
    "title": "14  lasso回归筛选变量",
    "section": "14.8 一些参数解释",
    "text": "14.8 一些参数解释\n\nalpha：可以看做是L1正则化的比例，当alpha=1时，就是lasso，当alpha=0时，就是岭回归，当0&lt;alpha&lt;1时，就是弹性网络。\nweights：不同观测的权重，默认都是1。（glmnet会自动对权重进行重新标准化，使得所有观测的权重相加等于样本数量）。\nnlambda：lambda的取值个数，默认是100。\nlambda：用户可以通过这个参数自己指定lambda的取值。\nstandardize：逻辑值，是否在拟合模型前对自变量进行标准化，默认是TRUE。\n\n下面是一个对不同观测自定义权重的示例。\n我们这个示例中，样本量是100，所以我们为100个观测自定义以下权重：\n\n# 简单定义一下，前50个是1，后50个是2\nwts &lt;-  c(rep(1,50), rep(2,50))\nfit1 &lt;- glmnet(x, y, alpha = 0.2, weights = wts, nlambda = 20)\n\nprint(fit1)\n## \n## Call:  glmnet(x = x, y = y, weights = wts, alpha = 0.2, nlambda = 20) \n## \n##    Df  %Dev  Lambda\n## 1   0  0.00 1.18600\n## 2   2 11.40 0.73050\n## 3  10 31.21 0.44990\n## 4  11 48.89 0.27710\n## 5  15 59.86 0.17060\n## 6  21 66.72 0.10510\n## 7  26 71.32 0.06471\n## 8  28 73.71 0.03985\n## 9  29 74.84 0.02454\n## 10 29 75.37 0.01512\n## 11 29 75.58 0.00931\n## 12 29 75.66 0.00573\n## 13 30 75.70 0.00353\n## 14 30 75.71 0.00217\n## 15 30 75.72 0.00134\n## 16 30 75.72 0.00082\n## 17 30 75.72 0.00051\n\n可以看到结果中只有17个lambda值，少于我们指定的20个，原因已经在前面解释过了。"
  },
  {
    "objectID": "feature-selection_lasso.html#在测试集评估模型",
    "href": "feature-selection_lasso.html#在测试集评估模型",
    "title": "14  lasso回归筛选变量",
    "section": "14.9 在测试集评估模型",
    "text": "14.9 在测试集评估模型\n模型建立后，我们可能会使用测试集检测模型性能，glmnet包为我们提供了assess.glmnet，roc.glmnet，confusion.glmnet，帮助我们快速在衡量模型性能。\n\n14.9.1 assess.glmnet()\n还是使用这个二分类数据，我们把前70个观测作为训练集，用来建模，后30个观测作为测试集。\n\ndata(BinomialExample)\nx &lt;- BinomialExample$x\ny &lt;- BinomialExample$y\nitrain &lt;- 1:70 # 前70个作为训练集\nfit &lt;- glmnet(x[itrain, ], y[itrain], family = \"binomial\", nlambda = 6)\n\n# 在测试集评估模型\nassess.glmnet(fit, newx = x[-itrain, ], newy = y[-itrain])\n## $deviance\n##        s0        s1        s2        s3        s4        s5 \n## 1.3877348 0.8547044 1.2031017 2.3732041 3.1831559 3.7565310 \n## attr(,\"measure\")\n## [1] \"Binomial Deviance\"\n## \n## $class\n##        s0        s1        s2        s3        s4        s5 \n## 0.4666667 0.1666667 0.2000000 0.2000000 0.1666667 0.1666667 \n## attr(,\"measure\")\n## [1] \"Misclassification Error\"\n## \n## $auc\n## [1] 0.5000000 0.8973214 0.8794643 0.8214286 0.8169643 0.8303571\n## attr(,\"measure\")\n## [1] \"AUC\"\n## \n## $mse\n##        s0        s1        s2        s3        s4        s5 \n## 0.5006803 0.2620161 0.3157726 0.3570313 0.3500126 0.3482634 \n## attr(,\"measure\")\n## [1] \"Mean-Squared Error\"\n## \n## $mae\n##        s0        s1        s2        s3        s4        s5 \n## 0.9904762 0.5650890 0.4609257 0.4227314 0.3865725 0.3745569 \n## attr(,\"measure\")\n## [1] \"Mean Absolute Error\"\n\n结果是一个列表，里面是family = \"binomial\"时，5个性能指标在不同lambda值下的结果，由于我们这里指定了只使用6个lambda值，所以结果就是6个，你指定几个，结果就会有几个。\n不同的family对应着不同的性能指标，可以通过glmnet.measures()查看每个family对应的性能指标：\n\nglmnet.measures()\n## $gaussian\n## [1] \"mse\" \"mae\"\n## \n## $binomial\n## [1] \"deviance\" \"class\"    \"auc\"      \"mse\"      \"mae\"     \n## \n## $poisson\n## [1] \"deviance\" \"mse\"      \"mae\"     \n## \n## $cox\n## [1] \"deviance\" \"C\"       \n## \n## $multinomial\n## [1] \"deviance\" \"class\"    \"mse\"      \"mae\"     \n## \n## $mgaussian\n## [1] \"mse\" \"mae\"\n## \n## $GLM\n## [1] \"deviance\" \"mse\"      \"mae\"\n\n交叉验证同样也是适用的：\n\ncfit &lt;- cv.glmnet(x[itrain, ], y[itrain], family = \"binomial\", nlambda = 30)\nassess.glmnet(cfit, newx = x[-itrain, ], newy = y[-itrain])\n## $deviance\n## lambda.1se \n##   1.062957 \n## attr(,\"measure\")\n## [1] \"Binomial Deviance\"\n## \n## $class\n## lambda.1se \n##  0.2333333 \n## attr(,\"measure\")\n## [1] \"Misclassification Error\"\n## \n## $auc\n## [1] 0.8392857\n## attr(,\"measure\")\n## [1] \"AUC\"\n## \n## $mse\n## lambda.1se \n##  0.3509219 \n## attr(,\"measure\")\n## [1] \"Mean-Squared Error\"\n## \n## $mae\n## lambda.1se \n##  0.7730458 \n## attr(,\"measure\")\n## [1] \"Mean Absolute Error\"\n\n不过此时默认使用的lambda值是lambda.1se，也可以使用lambda.min：\n\nassess.glmnet(cfit, newx = x[-itrain, ],newy = y[-itrain], s = \"lambda.min\")\n## $deviance\n## lambda.min \n##   0.877155 \n## attr(,\"measure\")\n## [1] \"Binomial Deviance\"\n## \n## $class\n## lambda.min \n##  0.1666667 \n## attr(,\"measure\")\n## [1] \"Misclassification Error\"\n## \n## $auc\n## [1] 0.8973214\n## attr(,\"measure\")\n## [1] \"AUC\"\n## \n## $mse\n## lambda.min \n##   0.273071 \n## attr(,\"measure\")\n## [1] \"Mean-Squared Error\"\n## \n## $mae\n## lambda.min \n##  0.6069619 \n## attr(,\"measure\")\n## [1] \"Mean Absolute Error\"\n\n当然也可以获取训练集的各种指标，只要在建模时使用keep=TRUE参数即可：\n\ncfit &lt;- cv.glmnet(x, y, family = \"binomial\", keep = TRUE, nlambda = 3)\nassess.glmnet(cfit$fit.preval, newy = y, family = \"binomial\")\n## $deviance\n##       s0       s1       s2 \n## 1.344523 1.701907 3.167967 \n## attr(,\"measure\")\n## [1] \"Binomial Deviance\"\n## \n## $class\n##   s0   s1   s2 \n## 0.41 0.20 0.21 \n## attr(,\"measure\")\n## [1] \"Misclassification Error\"\n## \n## $auc\n## [1] 0.6225649 0.8668831 0.8620130\n## attr(,\"measure\")\n## [1] \"AUC\"\n## \n## $mse\n##        s0        s1        s2 \n## 0.4792227 0.3204998 0.3901110 \n## attr(,\"measure\")\n## [1] \"Mean-Squared Error\"\n## \n## $mae\n##        s0        s1        s2 \n## 0.9635782 0.4058027 0.4178002 \n## attr(,\"measure\")\n## [1] \"Mean Absolute Error\"\n\n\n\n14.9.2 roc.glmnet()\n对于二分类数据，ROC曲线是非常重要的模型衡量工具。\nroc.glmnet()可以快速计算出画ROC曲线需要的数据，然后使用plot()画图即可。\n\nfit &lt;- glmnet(x[itrain,], y[itrain], family = \"binomial\")\n\nrocs &lt;- roc.glmnet(fit, newx = x[-itrain,], newy=y[-itrain])\n\n这个rocs是一个列表，其长度就是lambda值的数量，对于每一个lambda取值，它都计算了可以用来画ROC曲线的数据。\n我们随便取其中一个画出来：\n\nplot(rocs[[3]],type = \"l\",xlim=c(0,1),ylim=c(0,1))\ninvisible(sapply(rocs, lines)) # 把所有的ROC都画出来\nabline(0,1,col=\"grey\")\n\n\n\n\n交叉验证的结果当然也是可以的：\n\n# 建立模型\ncfit &lt;- cv.glmnet(x, y, family = \"binomial\", type.measure = \"auc\", \n                  keep = TRUE)\n\n# 计算画ROC曲线需要的数据\nrocs &lt;- roc.glmnet(cfit$fit.preval, newy = y)\n\nclass(rocs)\n## [1] \"list\"\nlength(rocs)\n## [1] 98\ndim(rocs[[1]])\n## [1] 43  2\nhead(rocs[[1]])\n##                          FPR TPR\n## 0.515726153922901 0.02272727   0\n## 0.493415443261052 0.04545455   0\n## 0.455710371846811 0.06818182   0\n## 0.454710952490737 0.09090909   0\n## 0.440189664756163 0.11363636   0\n## 0.436170361317587 0.13636364   0\n\n这个rocs也是一个列表，其长度就是lambda值的数量，对于每一个lambda取值，它都计算了可以用来画ROC曲线的数据。\n下面我们把AUC最大的ROC曲线画出来，用红色标记，并把其他ROC曲线也画在一起：\n\nbest &lt;- cvfit$index[\"min\",] # 提取AUC最大的lambda值\nplot(rocs[[best]], type = \"l\") # 画出AUC最大的ROC曲线\ninvisible(sapply(rocs, lines, col=\"grey\")) # 把所有的ROC都画出来\nlines(rocs[[best]], lwd = 2,col = \"red\") # 把AUC最大的标红\n\n\n\n\n\n\n14.9.3 confusion.glmnet()\n混淆矩阵作为分类数据必不可少的工具，可以通过confusion.glmnet()实现。\n用一个多分类数据进行演示。\n\ndata(MultinomialExample)\nx &lt;- MultinomialExample$x\ny &lt;- MultinomialExample$y\nset.seed(101)\nitrain &lt;- sample(1:500, 400, replace = FALSE)\ncfit &lt;- cv.glmnet(x[itrain, ], y[itrain], family = \"multinomial\")\n\n# 默认lambda值是lambda.1se\ncnf &lt;- confusion.glmnet(cfit, newx = x[-itrain, ], newy = y[-itrain]) \n\nprint(cnf)\n##          True\n## Predicted  1  2  3 Total\n##     1     13  6  4    23\n##     2      7 25  5    37\n##     3      4  3 33    40\n##     Total 24 34 42   100\n## \n##  Percent Correct:  0.71\n\n如果使用keep=TRUE，那么结果也是多个混淆矩阵，此时也可以选择任意一个进行展示：\n\ncfit &lt;- cv.glmnet(x, y, family = \"multinomial\", type = \"class\", keep = TRUE)\ncnf &lt;- confusion.glmnet(cfit$fit.preval, newy = y, family = \"multinomial\")\nbest &lt;- cfit$index[\"min\",]\nprint(cnf[[best]])\n##          True\n## Predicted   1   2   3 Total\n##     1      76  22  14   112\n##     2      39 129  23   191\n##     3      27  23 147   197\n##     Total 142 174 184   500\n## \n##  Percent Correct:  0.704\n\n虽然glmnet包提供了这3个函数帮助我们查看模型性能，但是很明显不能满足大家的需求，所以一般情况下我们都用其他的R包来代替这几个函数了，比如caret，yardstick，pROC等。"
  },
  {
    "objectID": "feature-selection_lasso.html#其他功能",
    "href": "feature-selection_lasso.html#其他功能",
    "title": "14  lasso回归筛选变量",
    "section": "14.10 其他功能",
    "text": "14.10 其他功能\n\n14.10.1 拟合非正则化的广义线性模型\nglmnet包提供了bigGlm()函数，可以对大型数据拟合非正则化的广义线性模型，类似于常规的glm()，但是支持glmnet中的所有参数。其实此时的lambda=0，也就是不进行正则化。如果你的数据巨大，使用glm很慢，或者你需要其他参数，可以尝试一下bigGlm()。\n以下是一个使用示例：\n\ndata(BinomialExample)\nx &lt;- BinomialExample$x\ny &lt;- BinomialExample$y\nfit &lt;- bigGlm(x, y, family = \"binomial\", lower.limits = -1)\nprint(fit)\n## \n## Call:  bigGlm(x = x, y = y, family = \"binomial\", lower.limits = -1) \n## \n##   Df  %Dev Lambda\n## 1 30 77.57      0\n\n\n\n14.10.2 修改自变量矩阵格式\nglmnet包提供了一个makeX()函数，可以对自变量的格式进行修改，比如，如果你提供了1个数据框data.frame，这个格式是不行的，它可以帮你转换为matrix格式，除此之外，还可以进行如下操作：\n\n对因子型或字符型变量进行独热编码；\n使用均值填补缺失值；\n可以直接变为稀疏矩阵，适合大数据；\n可以直接提供训练集和测试集两个数据集，这样可以保证两个数据集的因子水平对应，以及使用训练集中的均值对测试集进行插补\n\n先展示下独热编码转换功能，我们建立一个数据框，其中两列是字符型，makeX可以帮我们进行独热编码，并把数据变为稀疏矩阵格式：\n\nset.seed(101)\nX &lt;- matrix(rnorm(5), nrow = 5)\nX2 &lt;- sample(letters[1:3], 5, replace = TRUE)\nX3 &lt;- sample(LETTERS[1:3], 5, replace = TRUE)\ndf &lt;- data.frame(X, X2, X3)\ndf\n##            X X2 X3\n## 1 -0.3260365  c  C\n## 2  0.5524619  b  C\n## 3 -0.6749438  c  B\n## 4  0.2143595  c  C\n## 5  0.3107692  a  C\nmakeX(df) # 转换\n##            X X2a X2b X2c X3B X3C\n## 1 -0.3260365   0   0   1   0   1\n## 2  0.5524619   0   1   0   0   1\n## 3 -0.6749438   0   0   1   1   0\n## 4  0.2143595   0   0   1   0   1\n## 5  0.3107692   1   0   0   0   1\n\n添加sparse=T可以返回稀疏矩阵格式：\n\nmakeX(df, sparse = TRUE)\n## 5 x 6 sparse Matrix of class \"dgCMatrix\"\n##            X X2a X2b X2c X3B X3C\n## 1 -0.3260365   .   .   1   .   1\n## 2  0.5524619   .   1   .   .   1\n## 3 -0.6749438   .   .   1   1   .\n## 4  0.2143595   .   .   1   .   1\n## 5  0.3107692   1   .   .   .   1\n\n下面我们对原数据框添加一些缺失值，用来演示makeX的缺失值插补功能：\n\nXn  &lt;- X ; Xn[3,1] &lt;- NA\nX2n &lt;- X2; X2n[1]  &lt;- NA\nX3n &lt;- X3; X3n[5]  &lt;- NA\ndfn &lt;- data.frame(Xn, X2n, X3n)\ndfn\n##           Xn  X2n  X3n\n## 1 -0.3260365 &lt;NA&gt;    C\n## 2  0.5524619    b    C\n## 3         NA    c    B\n## 4  0.2143595    c    C\n## 5  0.3107692    a &lt;NA&gt;\n\n通过添加na.impute=T可以进行插补：\n\nmakeX(dfn,na.impute = T)\n##           Xn X2na X2nb X2nc X3nB X3nC\n## 1 -0.3260365 0.25 0.25  0.5 0.00 1.00\n## 2  0.5524619 0.00 1.00  0.0 0.00 1.00\n## 3  0.1878885 0.00 0.00  1.0 1.00 0.00\n## 4  0.2143595 0.00 0.00  1.0 0.00 1.00\n## 5  0.3107692 1.00 0.00  0.0 0.25 0.75\n## attr(,\"means\")\n##        Xn      X2na      X2nb      X2nc      X3nB      X3nC \n## 0.1878885 0.2500000 0.2500000 0.5000000 0.2500000 0.7500000\n\n这个函数总体来说还是挺方便的。\n\n\n14.10.3 添加进度条\nglmnet()和cv.glmnet()都可以通过添加trace.it=TRUE实现进度条功能：\n\n# 结果太长不展示了\nfit &lt;- glmnet(x, y, trace.it = TRUE)\n\n\n# 结果太长不展示了\nfit &lt;- cv.glmnet(x, y, trace.it = TRUE)\n\n也可以通过以下方式实现：\n\nglmnet.control(itrace = 1) # 变成0就不显示了~"
  },
  {
    "objectID": "feature-selection_lasso.html#正则化cox回归",
    "href": "feature-selection_lasso.html#正则化cox回归",
    "title": "14  lasso回归筛选变量",
    "section": "14.11 正则化Cox回归",
    "text": "14.11 正则化Cox回归\n正则化的COX回归，也就是glmnet在生存分析中的应用，这里我们还是以lasso为例进行演示。\nglmnet包的详细使用介绍已经在前面都介绍过了，正则化的COX回归并没有太大的不同，所以这里简单介绍一下。\n下面是一些理论解释，大家随便看看就好。\n在glmnet中，我们使用弹性网络（elastic net）方法对部分似然的负对数进行惩罚。\n部分似然（partial-likelihood）是一种用于处理生存分析（survival-analysis）中右侧截尾（right-censored）观测的方法。而负对数部分似然（negative-log-partial-likelihood）则是对部分似然取反并求对数，目的是将最大化似然函数的问题转化为最小化负对数似然函数的问题。\n为了进一步约束模型的复杂度和提高模型的泛化能力，我们在负对数部分似然的基础上引入了弹性网络惩罚（elastic-net-penalty）。弹性网惩罚结合了L1正则化（L1-regularization）和L2正则化（L2-regularization）的特性，从而既能产生稀疏解，又能保留一些高度相关的特征。这样我们可以在建立模型时在部分似然的基础上，使用弹性网惩罚来进行模型的优化和参数选择，以提高模型的性能和泛化能力。\n\n14.11.1 基础使用\nglmnet对数据格式是有要求的，之前也说过，x必须是由自变量组成的matrix，y可以是一个两列的matrix，两列的列名必须是time和status，分别表示生存时间和生存状态，其中status必须使用0和1组成，0表示删失，1表示发生终点事件（又叫失效事件，比如死亡）。除此之外，y还可以是由Surv()函数生成的对象。\n下面是一个示例数据：\n\nlibrary(glmnet)\nlibrary(survival)\n\ndata(CoxExample)\nx &lt;- CoxExample$x\ny &lt;- CoxExample$y\n\n# 查看y的数据格式\ny[1:5, ]\n##            time status\n## [1,] 1.76877757      1\n## [2,] 0.54528404      1\n## [3,] 0.04485918      0\n## [4,] 0.85032298      0\n## [5,] 0.61488426      1\n\n建立模型，只需要使用family = \"cox\"即可：\n\nfit &lt;- glmnet(x, y, family = \"cox\")\n\n其中的一些参数比如alpha，weights，nlambda等，在前面已经介绍过了，这里就不再多介绍了。\n可视化、提取系数、预测新数据和之前介绍的用法也是一模一样，这里也不再多说了。\n\n\n14.11.2 交叉验证\n对于正则化的cox来说，cv.glmnet()中的type.measure只能是\"deviance\"（默认值，给出部分似然），或者\"C\"，给出 Harrell-C-index。\n\nset.seed(1)\ncvfit &lt;- cv.glmnet(x, y, family = \"cox\", type.measure = \"C\")\n\nprint(cvfit)\n## \n## Call:  cv.glmnet(x = x, y = y, type.measure = \"C\", family = \"cox\") \n## \n## Measure: C-index \n## \n##      Lambda Index Measure       SE Nonzero\n## min 0.03058    23  0.7304 0.005842      11\n## 1se 0.05865    16  0.7267 0.005993      10\n\n画图也是一样的，下面这幅图的解释在前面也已经详细介绍过了，这里就不再多做解释了：\n\nplot(cvfit)\n\n\n\n\n\n在glmnet中，对于生存时间的排列相同（ties）情况，使用的是Breslow近似（Breslow approximation）。这与survival软件包中的coxph函数的默认排列处理方法（tie-handling method）——Efron近似（Efron approximation）不同。\n当存在相同的生存时间观测时，例如多个个体在同一时间发生事件，排列的处理方法对估计结果和推断的准确性至关重要。Breslow近似与Efron近似是最常见的两种处理方法。\n在glmnet中，使用Breslow近似处理排列，该方法假设所有的排列发生在后一事件之前的所有时间上。这种近似方法在计算效率上比较高，但可能会导致估计的偏差。\n而在survival软件包中的coxph函数，默认使用的是Efron近似处理排列。Efron近似方法基于考虑排列发生的时间顺序进行调整，更接近真实的结果，但在计算过程中稍微耗时一些。\n因此，当在glmnet和survival软件包中处理生存分析时，需要注意到在处理排列的方法上的差异，以确保得到准确和一致的结果。\n\n\n\n14.11.3 分层COX\ncoxph()支持strata()函数，因为它是使用公式形式的，但是glmnet不支持公式形式，只能使用x/y形式的输入，所以如果要实现分层，需要使用stratifySurv()。\n继续使用上面的示例数据，我们把1000个观测分成5层：\n\n# 把1000个观测分5层\nstrata &lt;- rep(1:5, length.out = 1000)\ny2 &lt;- stratifySurv(y, strata) # 对y进行分层\nstr(y2[1:6])\n##  'stratifySurv' num [1:6, 1:2] 1.7688  0.5453  0.0449+ 0.8503+ 0.6149  0.2986+\n##  - attr(*, \"dimnames\")=List of 2\n##   ..$ : NULL\n##   ..$ : chr [1:2] \"time\" \"status\"\n##  - attr(*, \"type\")= chr \"right\"\n##  - attr(*, \"strata\")= int [1:6] 1 2 3 4 5 1\n\n接下来把y2提供给glmnet()或者cv.glmnet()就可以实现正则化的分层COX了。\n\nfit &lt;- glmnet(x, y2, family = \"cox\")\n\ncv.fit &lt;- cv.glmnet(x, y2, family = \"cox\", nfolds = 5)\nplot(cv.fit)\n\n\n\n\n\n\n14.11.4 生存曲线\nglmnet的结果可以直接提供给survfit()使用，可以用来画生存曲线。这里简单介绍一下，大家知道即可，因为大家在平时写文章时根本不会这么用……\n以下是一个示例。\n\ndata(CoxExample)\nx &lt;- CoxExample$x\ny &lt;- CoxExample$y\n\ny &lt;- Surv(y[,1],y[,2]) # 需要用Surv转换格式\n\nfit &lt;- glmnet(x, y, family = \"cox\")\nsurvival::survfit(fit, s = 0.05, x = x, y = y)\n## Call: survfit.coxnet(formula = fit, s = 0.05, x = x, y = y)\n## \n##         n events median\n## [1,] 1000    692  0.922\n\n直接画图即可：\n\nplot(survival::survfit(fit, s = 0.05, x = x, y = y))\n\n\n\n\n这个生存曲线有些奇怪，因为数据原因，大家可以自己尝试下。\n基于新的数据画生存曲线也是可以的：\n\nplot(survival::survfit(fit, s = 0.05, x = x, y = y, newx = x[1:3, ]))"
  },
  {
    "objectID": "feature-selection_lasso.html#其他",
    "href": "feature-selection_lasso.html#其他",
    "title": "14  lasso回归筛选变量",
    "section": "14.12 其他",
    "text": "14.12 其他\n我知道大家想看的肯定不是这些，所以后期会安排一些glmnet实战的推文，用几个详细的示例进行演示，并展示lasso回归相关的列线图、校准曲线、决策曲线、ROC曲线等内容，结合一些文章进行讲解。\n敬请期待~"
  },
  {
    "objectID": "feature-selection_randomforest.html#准备数据",
    "href": "feature-selection_randomforest.html#准备数据",
    "title": "15  变量筛选之随机森林",
    "section": "15.1 准备数据",
    "text": "15.1 准备数据\n我们使用TCGA-BLCA的lncRNA数据(数据在粉丝QQ群文件，需要的加群下载即可)，其中包括408个样本，time_months是生存时间，event是生存状态，1代表死亡，0代表生存，其余变量都是自变量。\n先简单处理一下数据：\n\nrm(list = ls())\nload(file = \"datasets/lnc_expr_clin.RData\")\n#去掉没有生存信息的样本\nlnc_expr_clin &lt;- lnc_expr_clin[!is.na(lnc_expr_clin$time_months),]\nlnc_expr_clin &lt;- lnc_expr_clin[lnc_expr_clin$time_months&gt;0,]\n\n#选择其中一部分数据\ndat.cox &lt;- lnc_expr_clin[,c(72,1:59)]\n\n#把变量命中的“-”去掉\ncolnames(dat.cox)&lt;- gsub(\"-\",\"\",colnames(dat.cox))\n\n#结果变量变为因子型\ndat.cox$event &lt;- factor(dat.cox$event)\ndim(dat.cox)\n## [1] 297  60\ndat.cox[1:4,1:6]\n##   event    PGM5AS1 LINC01082 AC005180.2 AC005180.1    FENDRR\n## 1     0 0.15064007 0.2642238  0.0000000  0.1547768 0.7802599\n## 2     0 0.06309362 0.1666554  0.3105983  0.2436603 0.7239329\n## 3     1 2.16399508 3.5662920  2.2454129  2.0073496 2.8409939\n## 4     0 2.73075081 1.7314314  0.8609916  0.7323014 1.0531249"
  },
  {
    "objectID": "feature-selection_randomforest.html#建立模型",
    "href": "feature-selection_randomforest.html#建立模型",
    "title": "15  变量筛选之随机森林",
    "section": "15.2 建立模型",
    "text": "15.2 建立模型\n使用经典的randomForest建立随机森林模型：\n\nlibrary(randomForest)\n\nset.seed(124)\nfit &lt;- randomForest(event~., data = dat.cox)\n\nfit\n## \n## Call:\n##  randomForest(formula = event ~ ., data = dat.cox) \n##                Type of random forest: classification\n##                      Number of trees: 500\n## No. of variables tried at each split: 7\n## \n##         OOB estimate of  error rate: 23.91%\n## Confusion matrix:\n##     0 1 class.error\n## 0 225 1 0.004424779\n## 1  70 1 0.985915493\n\n结果给出了树的数量：500颗；OOB错误率：23.91%；还给出了混淆矩阵。"
  },
  {
    "objectID": "feature-selection_randomforest.html#结果探索",
    "href": "feature-selection_randomforest.html#结果探索",
    "title": "15  变量筛选之随机森林",
    "section": "15.3 结果探索",
    "text": "15.3 结果探索\n下面是可视化整体错误率和树的数量的关系，可以看到随着树的数量增加，错误率逐渐降低并渐趋平稳，中间的黑色线条是整体的错误率，上下两条是结果变量中两个类别的错误率。\n\nplot(fit)\n\n\n\n\n可以看到结果有一个类别的错误率竟然是逐渐增加的，因为我们这个数据的存在严重的类不平衡问题，也就是结果变量中的两种类别差异很大：\n\ntable(dat.cox$event)\n## \n##   0   1 \n## 226  71\n\n类别0有226个，类别1只有71个，模型为了提高整体准确率，就会牺牲掉类别为1的准确性~\n查看整体错误率最小时有几棵树：\n\nwhich.min(fit$err.rate[,1])\n## [1] 252\n\n查看各个变量的重要性，这里给出了mean decrease gini，数值越大说明变量越重要：\n\nimportance(fit)\n##             MeanDecreaseGini\n## PGM5AS1            0.9352510\n## LINC01082          1.3866068\n## AC005180.2         1.6168145\n## AC005180.1         1.3488141\n## FENDRR             1.3961145\n## AC053503.3         1.1375051\n## MIR100HG           1.8389235\n## AP001107.5         1.4790429\n## C5orf66AS1         1.9776753\n## NR4A1AS            1.5676917\n## AL162424.1         1.4297710\n## AF001548.1         1.4044652\n## AC099850.4         1.9485050\n## MBNL1AS1           1.7969564\n## ADAMTS9AS1         2.6993467\n## MIR22HG            1.7904790\n## MIR200CHG          2.0379430\n## AC093010.3         2.8789159\n## LINC00865          1.4885762\n## AP003071.4         1.6418555\n## PCAT6              2.0033471\n## LINC02657          2.2435347\n## PPP1R14BAS1        1.8262720\n## AC012085.2         1.3986750\n## ACTA2AS1           1.3608811\n## AC036108.3         1.6392733\n## AC079313.2         1.1854198\n## AC020916.1         1.5358508\n## SNHG25             3.2320090\n## AL049555.1         3.2795519\n## MIR11HGAS1         1.1433175\n## AC018904.1         1.5742751\n## SNHG12             2.9166977\n## SPINT1AS1          2.2857854\n## KRT7AS             2.1031232\n## MIR205HG           1.5659665\n## HAND2AS1           1.9955215\n## AL445524.1         2.2827356\n## LINC01980          1.4518537\n## ZNF710AS1          1.7042160\n## AC092718.4         1.7060641\n## AC008735.2         1.5337512\n## LINC01133          1.2802464\n## AC025575.2         1.5706176\n## MAFGDT             1.9238538\n## CASC9              2.6039129\n## AL390719.2         2.1003651\n## AC002398.2         0.7629889\n## AC008736.1         1.9839028\n## AL161431.1         3.0891759\n## PCCADT             1.5235273\n## AC245041.2         1.9197363\n## U62317.1           1.6662262\n## U62317.2           1.4415305\n## VPS9D1AS1          2.0524334\n## AL023284.4         1.8027226\n## AATBC              2.1357139\n## LINC00641          1.6734872\n## AC015912.3         2.4443188\n\n可视化变量重要性：\n\nvarImpPlot(fit)\n\n\n\n\n通过变量重要性，大家就可以选择比较重要的变量了。你可以选择前5个，前10个，或者大于所有变量性平均值(中位数，百分位数等)的变量等等。"
  },
  {
    "objectID": "feature-selection_randomforest.html#交叉验证变量筛选",
    "href": "feature-selection_randomforest.html#交叉验证变量筛选",
    "title": "15  变量筛选之随机森林",
    "section": "15.4 交叉验证变量筛选",
    "text": "15.4 交叉验证变量筛选\nrandomForest还提供了使用交叉验证法进行递归特征消除，筛选变量的方法：rfcv，下面是使用5折交叉验证进行递归特征消除：\n\nset.seed(647)\nres &lt;- rfcv(trainx = dat.cox[,-1],trainy = dat.cox[,1],\n            cv.fold = 5,\n            recursive = T\n            )\nres$n.var #变量个数\n## [1] 59 30 15  7  4  1\nres$error.cv #错误率\n##        59        30        15         7         4         1 \n## 0.2558923 0.2693603 0.2592593 0.2558923 0.2929293 0.3333333\n\n可以看到在变量个数为7的时候，错误率是最小的(和59一样，但是肯定选简单的)。\n可视化这个结果，很明显变量个数为7(和59)的时候错误率最小：\n\nwith(res, plot(n.var, error.cv, type=\"o\", lwd=2))\n\n\n\n\n结合上面的变量重要性，你可以选择前7个最重要的变量。\n\n\n\n\n\n\n注意\n\n\n\nrfcv得出的变量重要性和randomForest得出的变量重要性并不一样，而且rfcv的结果也并没有明确给出到底哪几个变量才是被选中的。这个方法并不常用，也不推荐大家用~\n参考：stackoverflow"
  },
  {
    "objectID": "feature-selection_randomforest.html#boruta筛选变量",
    "href": "feature-selection_randomforest.html#boruta筛选变量",
    "title": "15  变量筛选之随机森林",
    "section": "15.5 Boruta筛选变量",
    "text": "15.5 Boruta筛选变量\nBoruta是基于随机森林的一种变量筛选方法，它可以基于随机森林的变量重要性计算z-score，然后对每个变量标记确认 or 待定 or 拒绝，从而实现变量筛选。\n这种方法倾向于找到所有与结果变量最相关的变量，所以结果有可能是冗余的(或者存在共线性、相关性)。\n参考文献：Kursa M B, Rudnicki W R. Feature selection with the Boruta package[J]. Journal of statistical software, 2010, 36: 1-13.\n我们还是以这个数据集为例进行演示。\n\nlibrary(Boruta)\n\nset.seed(23)\n\nfs &lt;- Boruta(event ~ ., data = dat.cox, doTrace=1)\n## After 13 iterations, +0.91 secs:\n##  rejected 35 attributes: AATBC, AC002398.2, AC008735.2, AC008736.1, AC012085.2 and 30 more;\n##  still have 24 attributes left.\n## After 17 iterations, +0.99 secs:\n##  rejected 2 attributes: KRT7AS, MAFGDT;\n##  still have 22 attributes left.\n## After 21 iterations, +1.1 secs:\n##  confirmed 1 attribute: AL161431.1;\n##  rejected 3 attributes: AC099850.4, AL162424.1, FENDRR;\n##  still have 18 attributes left.\n## After 24 iterations, +1.1 secs:\n##  rejected 1 attribute: AP001107.5;\n##  still have 17 attributes left.\n## After 30 iterations, +1.2 secs:\n##  confirmed 1 attribute: ADAMTS9AS1;\n##  still have 16 attributes left.\n## After 45 iterations, +1.5 secs:\n##  confirmed 1 attribute: SNHG25;\n##  still have 15 attributes left.\n## After 48 iterations, +1.6 secs:\n##  confirmed 2 attributes: AC093010.3, MIR100HG;\n##  still have 13 attributes left.\n## After 63 iterations, +1.9 secs:\n##  confirmed 1 attribute: SPINT1AS1;\n##  still have 12 attributes left.\n## After 71 iterations, +2 secs:\n##  confirmed 1 attribute: HAND2AS1;\n##  still have 11 attributes left.\n\n查看筛选结果：变量确认，变量待定，变量拒绝：\n\ntable(fs$finalDecision)\n## \n## Tentative Confirmed  Rejected \n##        11         7        41\n\n获取确认变量的名字：\n\ngetSelectedAttributes(fs)\n## [1] \"MIR100HG\"   \"ADAMTS9AS1\" \"AC093010.3\" \"SNHG25\"     \"SPINT1AS1\" \n## [6] \"HAND2AS1\"   \"AL161431.1\"\n\n这样就搞定了！用这几个变量重新建立模型即可，当然也可以用这几个变量建立其他你喜欢的模型，都是可以的。\n公众号后台回复变量筛选即可获取相关推文合集，回复随机森林也可获取相关合集。"
  },
  {
    "objectID": "clinmodel-evalution.html#什么样的模型是好的模型",
    "href": "clinmodel-evalution.html#什么样的模型是好的模型",
    "title": "16  临床预测模型的评价方法",
    "section": "16.1 什么样的模型是好的模型？",
    "text": "16.1 什么样的模型是好的模型？\n一个理想的模型应该是这样的：它能正确区分所有的患者和非患者，能正确预测每一个个体是否会发生危险事件，并且不会出错。很明显这样的模型是不存在的，我们只能朝这个方向努力。\n评价一个模型的好坏可以从区分度（Discrimination）和校准度（Calibration）两个方面进行。\n区分度指的是一个模型能正确把人群分为患者/非患者，或者正确区分个体是处于低风险、还是处于高风险，或者正确预测患者是存活、还是死亡等的能力。\n但是一个模型只是有良好的区分度是不够的，因为临床是很复杂的，并不是只要正确分类就行了。对于不同的患者，可能他们都处于高风险组，但是对于50%的风险和80%的风险，我们的处理是不一样的！\n这就引出了校准度的概念，校准度指的是结局实际发生的概率和模型预测出的概率之间的一致性，所以校准度又叫一致性、拟合优度（goodness of fit），校准度体现了一个模型对绝对风险预测的准确性。\n一个具有良好区分度的模型，校准度不一定也很好。比如，一个模型把A、B两个患者（实际上就是低风险）都分为低风险组，并且B的风险是A的3倍，我们可以说这个模型的区分度很好，因为分类分的很准！然后这个模型预测出A的风险是1%，B的风险是3%，但实际上A的风险是10%，B的风险是30%！你看，这个模型的校准度就很差（世纪概率和预测概率相差很大）。如果使用这样的模型预测出来的结果处理病人的话，可能就会碰到很多问题。\n区分度和校准度虽然都有不足，但是一般来说，具有较差区分度的模型，其校准度也不会很好。当一个模型区分度和校准度都很差的时候，我们可以先从提高模型区分度的角度继续。"
  },
  {
    "objectID": "clinmodel-evalution.html#区分度的评价",
    "href": "clinmodel-evalution.html#区分度的评价",
    "title": "16  临床预测模型的评价方法",
    "section": "16.2 区分度的评价",
    "text": "16.2 区分度的评价\n说了这么多，那我们应该用哪些指标评价模型的分类能力的强弱呢？\n我们可以使用ROC曲线和C-Statistic评价。二分类变量的AUC（曲线下面积）和C-Statistic是一个东西，都是越接近1，模型的区分度越好。一般认为，AUC或者C-Statistic在0.6以下是低区分度，在0.6~0.75之间是中区分度，高于0.75是高区分度。\n关于ROC曲线和AUC的概念，大家自行了解。\n\nROC曲线的横坐标是1-特异度，纵坐标是灵敏度，也就是横坐标是假阳性，纵坐标是真阳性，这两个指标刚好是相反的，一个大另一个就小，所以一般我们会找一个阈值，使得曲线下面积最大。但是如果考虑到临床，这样做有时也是不妥的。比如，在患者具有某些高危疾病（比如心梗、肺梗等）的风险时，我们应该采取更加激进的措施进行预防，这时我们就需要更高的灵敏度，因为不想漏诊。\nROC和C-Statistic也存在明显的局限性，比如在模型中新增一个预测变量可能对于预后非常重要，但是反应在ROC和C-Statistic上却变化不大，特别是在ROC和C-Statistic已经很高的时候。"
  },
  {
    "objectID": "clinmodel-evalution.html#校准度的评价",
    "href": "clinmodel-evalution.html#校准度的评价",
    "title": "16  临床预测模型的评价方法",
    "section": "16.3 校准度的评价",
    "text": "16.3 校准度的评价\n目前校准度的评价最好的方式还是使用Calibration curve(Calibration plot)。通过校准曲线可以非常直观地看到预测概率和真实概率的关系。\n\n校准度的评价可以在很多水平进行，比如，从整体角度，或者从不同的组进行等。\n假如100个人有20个人发生了结局事件，那么真实概率就是20%，我们的模型会对每个人都计算出一个概率，如果大于某个值（比如说0.5），我们就认为这个人会发生结局事件，小于0.5就认为不会发生结局事件，那么这100个人每个人都会得到一个概率，把100个人的概率加起来再除以100，就得到了模型预测的平均概率，这个概率就是从整体角度评价的。\n假如把100个人按照预测概率大小排好序，然后分成10组，每组10人，在每个组内计算模型平均概率，再和真实概率（比如一组内10人有3人发生了结局事件，那这一组的真实概率就是30%）比较，然后把真实概率作为横坐标，预测概率作为纵坐标，就可以画出Calibration curve了，这就是从不同组的角度评价的（这是目前多数做法）。\n校准度还可以用Hosmer-Lemeshow 检验（H-L 检验）进行检验，若得到的P值小于0.05，那说明模型的预测值和真实值之间有差异，实际上Calibration curve就是Hosmer-Lemeshow检验的结果可视化，但是也仅仅是能说明有差异而已，并不能量化差异的大小。"
  },
  {
    "objectID": "clinmodel-evalution.html#多个模型之间的比较",
    "href": "clinmodel-evalution.html#多个模型之间的比较",
    "title": "16  临床预测模型的评价方法",
    "section": "16.4 多个模型之间的比较",
    "text": "16.4 多个模型之间的比较\n模型之间的区分度可以用AUC或者C-Statistic进行比较，校准度可以用AIC（Akaike index criterion）、BIC（Bayesian index criterion）、R^2等指标进行比较。\n除此之外，还可以用净重新分类指数和综合判别改善指数来比较模型的优劣。\n\n16.4.1 NRI\nNRI，net reclassification index，净重新分类指数，是用来比较模型准确度的，这个概念有点难理解，但是非常重要，在临床研究中非常常见，是评价模型的一大利器！\n我们可以这样简单的理解，假如有一群受试者，有些是患病的，有些是没病的，你建立了一个模型1，通过计算把受试者分为了有病组和没病组（当然有些是分错的），然后你又建立了模型2，也是把这群受试者分成了有病组和没病组，相当于是重新分类了！2个模型计算的结果是不一样的，有的可能重新分类分对了，有的可能分错了，然后，净重分类 = 重新分对的 - 重新分错的，净重分类指数只要再除以总人数就可以了，这样得到的就是绝对净重分类指数。\n绝对净重分类指数反应的是模型对整体的分类能力有无改变，但是，对于有病组和没病组，都是有可能出现重新分类分对了/重新分类分错了/重新分类和之前一样，这3种情况的。所以，我们完全可以把有病组和没病组分开计算净重分类指数，然后再相加，这样得到的就是相加净重分类指数（additive NRI）。\n2种净重分类指数，在临床使用时建议都计算一下，我们用上面那篇文献中的一个例子说明。\n两种模型（旧模型为RCRI，新模型为CCTA + RCRI）评价患者术后发生心梗的风险，结果如下表所示，其中绿色表示正确重新分类，粉色表示错误重新分类，灰色表示没有重新分类。\n\n根据这个表计算出来的相加NRI为11%，但是绝对NRI为-8%。绝对NRI为-8%说明新模型比旧模型差，难道我们就要继续采用旧模型了吗？但是我们的结局事件是心梗，是会危及患者生命的，这时候我们就需要适当提高假阳性，因为即使漏诊1例对漏诊的患者来说也是生命攸关的大事，所以这种情况下新模型还是值得采用的。\n所以到底怎么取舍，不能只看数字，一定要结合实际情况，写文章时遇到了不好的结果也要好好讨论！\n\n\n16.4.2 IDI\nNRI、AUC、C-Statistic都是要先设置一个阈值，然后在这个阈值的前提下进行各种比较，它们只是考虑了单独一个点的情况，不能反应模型的整体情况。因此，又来了新的指标-IDI，Integrated Discrimination Improvement，综合判别改善指数。\nIDI的计算很简单，它反映了两个模型在预测概率上的差别，是基于模型对每个个体的预测概率计算所得。它的计算方法为：\n\nPnew,events：患者组，新模型对每个人预测概率的平均值；\nPold,events ：患者组，旧模型对每个人预测概率的平均值；\n对于患者来说，那肯定是预测概率越高越准确，所以2者相减，值越大，表明新模型的预测能力越好；\nPnew,non-events：非患者组，新模型对每个人预测概率的平均值；\nPold,non-events：非患者组，旧模型对每个人预测概率的平均值；\n对于非患者来说，那肯定是预测概率越小越准确，因此，2者相减，值越小，表明新模型的预测能力越好；\n两部分再相减，就是值越大，表明模型预测能力越好，如果IDI为负值，那说明还不如原来的模型好。"
  },
  {
    "objectID": "clinmodel-evalution.html#决策曲线-dca",
    "href": "clinmodel-evalution.html#决策曲线-dca",
    "title": "16  临床预测模型的评价方法",
    "section": "16.5 决策曲线 DCA",
    "text": "16.5 决策曲线 DCA\n临床都会碰到假阳性和假阴性问题，也就是误诊和漏诊问题，对于不同的疾病，可能我们需要的阈值不同，有时需要更高的假阳性，有时需要更高的假阴性，因此这就是一个实际临床效用问题，到底怎么办，才能使患者获益最大？\n决策曲线分析（Decision Curve Analysis，DCA）就是为了解决这一问题。\n当患者有症状但尚不能诊断为某种疾病时，临床医生必须决定是(1)经验性治疗，(2)不治疗，或(3)在选择选项1和2之前进行进一步的检查。是否治疗取决于临床医生的经验、疾病治疗的有效性和并发症，以及患者接受治疗风险和负担的意愿。决策曲线分析是一种通过考虑患者风险和获益的可能范围来评估临床决策是否可行的方法。\n\nDCA中的一个关键概念是“概率阈值”，即患者选择接受治疗的概率水平。结合上面这幅图，横坐标就是概率阈值，当概率阈值到达某个水平时，我们就采取治疗措施（简单理解）。此时对于一个病人来说，治疗有可能会利大于弊，也有可能会弊大于利，纵坐标就是利减去弊之后的净获益。\n概率阈值也是概率，取值自然是0-1之间的，所以对于每一个概率，我们都可以计算出一个净获益（计算方法上图中给出了），然后把所有的点连起来，就是决策曲线了（上图中3条曲线，代表3种不同方法的决策曲线）。\n上图中还有2条特殊的线，一条水平的蓝色线，表示所有人都不接受治疗时，此时不管概率阈值是多少，净获益肯定都是0。一条黄色的线，表示所有人都接受治疗时，随着概率阈值的改变，其净获益的改变。这两条线代表了2种极端的情况。\n在给定的概率阈值下，肯定是净获益越大越好，所以一般来说，肯定是曲线距离两条特殊的线越远越好。"
  },
  {
    "objectID": "clinmodel-evalution.html#临床影响曲线-cic",
    "href": "clinmodel-evalution.html#临床影响曲线-cic",
    "title": "16  临床预测模型的评价方法",
    "section": "16.6 临床影响曲线 CIC",
    "text": "16.6 临床影响曲线 CIC\n临床影响曲线，Clinical Impact Curve，属于DCA的变种，是Kerr等人对DCA的进一步发展，结合下面这张图来看：\n\n横坐标还是概率阈值，纵坐标是人数。紫色线表示在不同的概率阈值下，被模型判定为高风险的人数；红色线条表示在不同的概率阈值下，被模型判定为高风险且真的发生结局事件的人数。在最下面还增加了一个损失：获益比，表示在不同的概率阈值下，损失和获益的比例。\n了解这些基础内容后，我们就可以通过各种各样的工具实现它，后面我们将会详细介绍如果通过R语言实现以上内容。"
  },
  {
    "objectID": "roc-binominal.html#方法1proc",
    "href": "roc-binominal.html#方法1proc",
    "title": "17  二分类资料ROC曲线绘制",
    "section": "17.1 方法1：pROC",
    "text": "17.1 方法1：pROC\n使用pROC包，不过使用这个包需要注意，一定要指定direction，否则可能会得出错误的结果。\n这个R包计算AUC是基于中位数的，哪一组的中位数大就计算哪一组的AUC，在计算时千万要注意！\n关于这个R包的详细使用，请参考文章：用pROC实现ROC曲线分析\n使用pROC包的aSAH数据，其中outcome列是结果变量，1代表Good，2代表Poor。\n\nlibrary(pROC)\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\n\ndata(aSAH)\ndim(aSAH)\n## [1] 113   7\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n计算AUC及可信区间：\n\nres &lt;- roc(aSAH$outcome,aSAH$s100b,ci=T,auc=T)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nres\n## \n## Call:\n## roc.default(response = aSAH$outcome, predictor = aSAH$s100b,     auc = T, ci = T)\n## \n## Data: aSAH$s100b in 72 controls (aSAH$outcome Good) &lt; 41 cases (aSAH$outcome Poor).\n## Area under the curve: 0.7314\n## 95% CI: 0.6301-0.8326 (DeLong)\n\n\nplot(res,legacy.axes = TRUE)\n\n\n\n\n可以显示最佳截点，比如AUC最大的点：\n\nplot(res,\n     legacy.axes = TRUE,\n     thresholds=\"best\", # AUC最大的点\n     print.thres=\"best\") \n\n\n\n\n可以显示AUC的可信区间：\n\nrocobj &lt;- plot.roc(aSAH$outcome, aSAH$s100b,\n                   main=\"Confidence intervals\", \n                   percent=TRUE,ci=TRUE, \n                   print.auc=TRUE\n                   ) \n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\nciobj &lt;- ci.se(rocobj,\n               specificities=seq(0, 100, 5)\n               )\n\nplot(ciobj, type=\"shape\", col=\"#1c61b6AA\")\nplot(ci(rocobj, of=\"thresholds\", thresholds=\"best\")) \n\n\n\n\n多条ROC曲线画在一起：\n\nrocobj1 &lt;- plot.roc(aSAH$outcome, aSAH$s100,percent=TRUE, col=\"#1c61b6\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nrocobj2 &lt;- lines.roc(aSAH$outcome, aSAH$ndka, percent=TRUE, col=\"#008600\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\nlegend(\"bottomright\", legend=c(\"S100B\", \"NDKA\"), col=c(\"#1c61b6\", \"#008600\"), lwd=2)\n\n\n\n\n两条ROC曲线的比较，可以添加P值：\n\nrocobj1 &lt;- plot.roc(aSAH$outcome, aSAH$s100,percent=TRUE, col=\"#1c61b6\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nrocobj2 &lt;- lines.roc(aSAH$outcome, aSAH$ndka, percent=TRUE, col=\"#008600\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\nlegend(\"bottomright\", legend=c(\"S100B\", \"NDKA\"), col=c(\"#1c61b6\", \"#008600\"), lwd=2)\n\ntestobj &lt;- roc.test(rocobj1, rocobj2)\n\ntext(50, 50, labels=paste(\"p-value =\", format.pval(testobj$p.value)), adj=c(0, .5))"
  },
  {
    "objectID": "roc-binominal.html#方法2rocr",
    "href": "roc-binominal.html#方法2rocr",
    "title": "17  二分类资料ROC曲线绘制",
    "section": "17.2 方法2：ROCR",
    "text": "17.2 方法2：ROCR\n使用ROCR，如果你只是为了画一条ROC曲线，这是我最推荐的方法了，美观又简单！\n\nlibrary(ROCR)\n\n使用非常简单，3句代码，其中第2句是关键，可以更改各种参数，然后就可以画出各种不同的图形：\n\npred &lt;- prediction(aSAH$s100b,aSAH$outcome)\nperf &lt;- performance(pred, \"tpr\",\"fpr\")\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4)\n\nplot(perf,lwd=2,col=\"tomato\")\nabline(0,1,lty=2)\nlegend(\"bottomright\", legend=\"AUC of s100b: 0.7314\", col=\"tomato\", lwd=2,bty = \"n\")\n\n\n\n\n添加箱线图：\n\nperf &lt;- performance(pred, \"tpr\", \"fpr\")\nperf\n## A performance instance\n##   'False positive rate' vs. 'True positive rate' (alpha: 'Cutoff')\n##   with 51 data points\n\nplot(perf,\n     avg=\"threshold\",\n     spread.estimate=\"boxplot\")\n\n\n\n\n还可以绘制PR曲线，召回率recall为横坐标，精确率precision 为纵坐标：\n\nperf &lt;- performance(pred, \"prec\", \"rec\")\nplot(perf,\n     avg= \"threshold\",\n     colorize=TRUE,\n     lwd= 3,\n     main= \"Precision-Recall plot\")\nplot(perf,\n     lty=3,\n     col=\"grey78\",\n     add=TRUE)\n\n\n\n\n还可以把特异度为横坐标，灵敏度为纵坐标：\n\nperf &lt;- performance(pred, \"sens\", \"spec\")\nplot(perf,\n     avg= \"threshold\",\n     colorize=TRUE,\n     lwd= 3,\n     main=\"Sensitivity/Specificity plots\")\nplot(perf,\n     lty=3,\n     col=\"grey78\",\n     add=TRUE)\n\n\n\n\n这个包还可以计算非常多其他的指标，各种图都能画，大家可以自己探索。"
  },
  {
    "objectID": "roc-binominal.html#方法3tidymodels",
    "href": "roc-binominal.html#方法3tidymodels",
    "title": "17  二分类资料ROC曲线绘制",
    "section": "17.3 方法3：tidymodels",
    "text": "17.3 方法3：tidymodels\n使用tidymodels。这个包很有来头，它是R中专门做机器学习的，可以到公众号：医学和生信笔记中查看更多关于它的教程，它也是目前R语言机器学习领域两大当红辣子鸡之一！另一个是mlr3。\n\nsuppressPackageStartupMessages(library(tidymodels))\n\n它很优雅，如果你要计算AUC，那么就是roc_auc()函数：\n\naSAH %&gt;% roc_auc(outcome, s100b,event_level=\"second\")\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc binary         0.731\n\n如果你是要画ROC曲线，那么就是roc_curve()函数：\n\naSAH %&gt;% roc_curve(outcome, s100b,event_level=\"second\") %&gt;% \n  ggplot(aes(x = 1 - specificity, y = sensitivity)) +\n  geom_path(size=1.2,color=\"firebrick\") +\n  geom_abline(lty = 3) +\n  coord_equal() +\n  theme_bw()\n\n\n\n\n还有太多方法可以画ROC了，不过pROC和ROCR基本上技能解决99%的问题了。\n最后，给大家看看cran中比较常见的画ROC曲线的包，大家有兴趣可以自己探索：\n\nlibrary(pkgsearch) \n\nrocPkg &lt;-  pkg_search(query=\"ROC\",size=200)\n\nrocPkgShort &lt;- rocPkg %&gt;% \n               filter(maintainer_name != \"ORPHANED\") %&gt;%\n               select(score, package, downloads_last_month) %&gt;%\n               arrange(desc(downloads_last_month))\nhead(rocPkgShort,20)\n## # A data frame: 20 × 3\n##      score package         downloads_last_month\n##  *   &lt;dbl&gt; &lt;chr&gt;                          &lt;int&gt;\n##  1 12333.  pROC                          208519\n##  2  4487.  caTools                       105907\n##  3   990.  ROCR                           53779\n##  4   404.  riskRegression                 12677\n##  5  2637.  PRROC                           9912\n##  6  1833.  plotROC                         3783\n##  7  2303.  cvAUC                           3657\n##  8   346.  mlr3viz                         3110\n##  9   385.  PresenceAbsence                 2653\n## 10  1881.  survivalROC                     2612\n## 11  1801.  precrec                         2547\n## 12  1838.  timeROC                         2371\n## 13   117.  RcmdrPlugin.EZR                 2196\n## 14   460.  ROCit                           1956\n## 15   180.  WVPlots                         1940\n## 16   213.  logcondens                      1800\n## 17   152.  PredictABEL                     1208\n## 18   151.  MLeval                           960\n## 19    51.7 wrProteo                         877\n## 20  1446.  plotwidgets                      637\n\npROC高居榜首，遥遥领先！"
  },
  {
    "objectID": "roc-survive.html#加载r包和数据",
    "href": "roc-survive.html#加载r包和数据",
    "title": "18  生存资料ROC曲线绘制",
    "section": "18.1 加载R包和数据",
    "text": "18.1 加载R包和数据\n\nrm(list = ls())\nlibrary(timeROC)\nlibrary(survival)\n\nload(file = \"./datasets/timeROC.RData\")"
  },
  {
    "objectID": "roc-survive.html#多个时间点roc",
    "href": "roc-survive.html#多个时间点roc",
    "title": "18  生存资料ROC曲线绘制",
    "section": "18.2 多个时间点ROC",
    "text": "18.2 多个时间点ROC\n首先看一下数据结构，对于多个时间点的ROC，需要3列数据：time, event, marker(比如你计算得到的risk score)。\n看一下画图所需的数据长什么样子，event这一列，0代表living，1代表dead，futime这一列单位是年，也可以改成其他的。\n\nstr(df)\n## 'data.frame':    297 obs. of  3 variables:\n##  $ event    : num  0 0 1 0 0 1 0 0 0 0 ...\n##  $ riskScore: num  -0.249 -0.511 -0.211 -0.427 0.279 ...\n##  $ futime   : num  3.03 1.16 1.82 1.52 1.34 ...\n\n\n# 构建timeroc\n\nROC &lt;- timeROC(T=df$futime,   \n               delta=df$event,   \n               marker=df$riskScore,   \n               cause=1,                #阳性结局指标数值\n               weighting=\"marginal\",   #计算方法，默认为marginal\n               times=c(1, 2, 3),       #时间点，选取1年，3年和5年的生存率\n               iid=TRUE)\n\nROC   #查看模型变量信息\n## Time-dependent-Roc curve estimated using IPCW  (n=297, without competing risks). \n##     Cases Survivors Censored AUC (%)   se\n## t=1    57       203       37   71.02 3.68\n## t=2    66       106      125   69.23 3.94\n## t=3    68        74      155   65.53 4.85\n## \n## Method used for estimating IPCW:marginal \n## \n## Total computation time : 0.08  secs.\n\n画图很简单：\n\nplot(ROC, \n     time=1, col=\"red\", lwd=2, title = \"\")   #time是时间点，col是线条颜色\nplot(ROC,\n     time=2, col=\"blue\", add=TRUE, lwd=2)    #add指是否添加在上一张图中\nplot(ROC,\n     time=3, col=\"orange\", add=TRUE, lwd=2)\n\n#添加标签信息\nlegend(\"bottomright\",\n       c(paste0(\"AUC at 1 year: \",round(ROC[[\"AUC\"]][1],2)), \n         paste0(\"AUC at 2 year: \",round(ROC[[\"AUC\"]][2],2)), \n         paste0(\"AUC at 3 year: \",round(ROC[[\"AUC\"]][3],2))),\n       col=c(\"red\", \"blue\", \"orange\"),\n       lty=1, lwd=2,bty = \"n\")"
  },
  {
    "objectID": "roc-survive.html#多指标roc",
    "href": "roc-survive.html#多指标roc",
    "title": "18  生存资料ROC曲线绘制",
    "section": "18.3 多指标ROC",
    "text": "18.3 多指标ROC\n首先也是看一下所需要的数据结构，其中futime和event是必须的，另外的几列是你想要用来画ROC曲线图的指标，可以自己添加，在这里我使用了riskScore, gender, TNM分期。 在gender这一列，1是female，2是male，t,n,m这3列，数字代表不同的分期\n\nstr(df2)\n## 'data.frame':    297 obs. of  8 variables:\n##  $ event    : num  0 0 1 0 0 1 0 0 0 0 ...\n##  $ age      : int  59 63 65 73 59 66 56 42 61 48 ...\n##  $ riskScore: num  -0.249 -0.511 -0.211 -0.427 0.279 ...\n##  $ futime   : num  3.03 1.16 1.82 1.52 1.34 ...\n##  $ gender   : num  2 2 2 1 2 2 1 2 2 2 ...\n##  $ t        : num  4 4 4 3 3 3 5 3 NA 4 ...\n##  $ n        : num  1 5 1 1 1 1 3 1 NA 1 ...\n##  $ m        : num  1 1 1 1 1 3 1 1 3 3 ...\n\n多指标的ROC曲线非常简单，就是构建多个ROC，依次添加即可：\n\n# riskScore的ROC曲线\nROC.risk &lt;- timeROC(T=df2$futime,\n                    delta=df2$event,   \n                    marker=df2$riskScore,   \n                    cause=1,                \n                    weighting=\"marginal\",   \n                    times=3,   \n                    iid=TRUE)\n\n\n# gender的ROC曲线\nROC.gender &lt;- timeROC(T=df2$futime,   \n                      delta=df2$event,   \n                      marker=df2$gender,   \n                      cause=1,   \n                      weighting=\"marginal\",   \n                      times=3,   \n                      iid=TRUE)\n\n\n# age的ROC曲线\nROC.age &lt;- timeROC(T=df2$futime,   \n                   delta=df2$event,   \n                   marker=df2$age,   \n                   cause=1,   \n                   weighting=\"marginal\",   \n                   times=3,   \n                   iid=TRUE)\n\n\n# T分期的ROC曲线\nROC.T &lt;- timeROC(T=df2$futime,\n                 delta=df2$event,  \n                 marker=df2$t,   \n                 cause=1, \n                 weighting=\"marginal\", \n                 times=3, \n                 iid=TRUE)\n\n\n# N分期的ROC曲线\nROC.N &lt;- timeROC(T=df2$futime,   \n                 delta=df2$event,   \n                 marker=df2$n,   \n                 cause=1,   \n                 weighting=\"marginal\",   \n                 times=3,   \n                 iid=TRUE)\n\n\n# M分期的ROC曲线\nROC.M &lt;- timeROC(T=df2$futime,   \n                 delta=df2$event,   \n                 marker=df2$m,   \n                 cause=1,   \n                 weighting=\"marginal\",   \n                 times=3,   \n                 iid=TRUE)\n\n把每个曲线拼在一起即可，添加一个图例：\n\nplot(ROC.risk, time = 3, col=\"#E41A1C\", lwd=2, title = \"\")\nplot(ROC.gender, time = 3, col=\"#A65628\", lwd=2, add = T)\nplot(ROC.age, time = 3, col=\"#4DAF4A\", lwd=2, add = T)\nplot(ROC.T, time = 3, col=\"#377EB8\", lwd=2, add = T)\nplot(ROC.N, time = 3, col=\"#984EA3\", lwd=2, add = T)\nplot(ROC.M, time = 3, col=\"#FFFF33\", lwd=2, add = T)\nlegend(\"bottomright\",\n       c(paste0(\"Risk score: \",round(ROC.risk[[\"AUC\"]][2],2)), \n         paste0(\"gender: \",round(ROC.gender[[\"AUC\"]][2],2)), \n         paste0(\"age: \",round(ROC.age[[\"AUC\"]][2],2)),\n         paste0(\"T: \",round(ROC.T[[\"AUC\"]][2],2)),\n         paste0(\"N: \",round(ROC.N[[\"AUC\"]][2],2)),\n         paste0(\"M: \",round(ROC.M[[\"AUC\"]][2],2))\n         ),\n       col=c(\"#E41A1C\", \"#A65628\", \"#4DAF4A\",\"#377EB8\",\"#984EA3\",\"#FFFF33\"),\n       lty=1, lwd=2,bty = \"n\")"
  },
  {
    "objectID": "roc-bestcut.html#proc",
    "href": "roc-bestcut.html#proc",
    "title": "19  ROC曲线的最佳截点",
    "section": "19.1 pROC",
    "text": "19.1 pROC\n只能用于二分类数据，不能用于生存数据。\n使用pROC包需要注意，一定要指定direction，否则可能会得出错误的结果。\n这个R包计算AUC是基于中位数的，哪一组的中位数大就计算哪一组的AUC，在计算时千万要注意！\n关于这个包的详细介绍，请参考文章：用pROC实现ROC曲线分析\n使用pROC包的aSAH数据，其中outcome列是结果变量，1代表Good，2代表Poor。\n\nlibrary(pROC)\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\n\ndata(aSAH)\ndim(aSAH)\n## [1] 113   7\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n计算AUC及可信区间：\n\nres &lt;- pROC::roc(aSAH$outcome,aSAH$s100b,ci=T,auc=T)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nres\n## \n## Call:\n## roc.default(response = aSAH$outcome, predictor = aSAH$s100b,     auc = T, ci = T)\n## \n## Data: aSAH$s100b in 72 controls (aSAH$outcome Good) &lt; 41 cases (aSAH$outcome Poor).\n## Area under the curve: 0.7314\n## 95% CI: 0.6301-0.8326 (DeLong)\n\n显示最佳截点，比如AUC最大的点：\n\nplot(res,\n     legacy.axes = TRUE,\n     thresholds=\"best\", # AUC最大的点\n     print.thres=\"best\") \n\n\n\n\n最佳截点是0.205，特异度是0.806，敏感度是0.634。"
  },
  {
    "objectID": "roc-bestcut.html#cutoff",
    "href": "roc-bestcut.html#cutoff",
    "title": "19  ROC曲线的最佳截点",
    "section": "19.2 cutoff",
    "text": "19.2 cutoff\ncutoff包中的roc函数也可以用于确定二分类数据ROC曲线的最佳截点，这个R包还可以用于连续性变量最佳截点的计算，之前专门介绍过：最佳截断值确定之cutoff\n\nlibrary(cutoff)\n## \n## Attaching package: 'cutoff'\n## The following object is masked from 'package:pROC':\n## \n##     roc\n\ncutoff::roc(aSAH$s100b, aSAH$outcome)\n##                      type  auc cutoff sensitivity specificity\n## 1 positive classification 0.73   0.22   0.6341463   0.8055556\n\n该包给出的最佳截点是0.22，敏感度是0.6341463，特异度是0.8055556。和pROC的结果有一点点差别，问题不大。"
  },
  {
    "objectID": "roc-bestcut.html#optimalcutpoints",
    "href": "roc-bestcut.html#optimalcutpoints",
    "title": "19  ROC曲线的最佳截点",
    "section": "19.3 optimalcutpoints",
    "text": "19.3 optimalcutpoints\nOptimalCutpoints包也是用于二分类数据ROC曲线的最佳截点，不能用于生存数据。\n该包中最重要的函数是optimal.cutpoints()、control.cutpoints()、summary.optimal.cutpoints()和plot.optimal.cutpoints()函数。optimal.cutpoints()函数根据所选的准则计算最佳切点以及其准确性度量。可以选择多个准则来选择最佳切点。control.cutpoints()函数用于设置每种方法特定的几个参数，例如成本值或诊断准确性度量的最小值。summary.optimal.cutpoints()和plot.optimal.cutpoints()函数分别生成数值和图形输出。数值输出包括有关最佳切点的信息，包括选择最佳值的方法，以及最佳切点的数量（在某些情况下可能有多个值）和最佳切点及其准确性度量的估计值。\n使用起来也是非常简单，没有难度：\n\nlibrary(OptimalCutpoints)\n\noc_youden &lt;- optimal.cutpoints(X=\"s100b\",\n                               status = \"outcome\",\n                               tag.healthy=\"Good\",\n                               method=\"Youden\",#支持的标准超多\n                               data=aSAH\n                               )\nsummary(oc_youden)\n## \n## Call:\n## optimal.cutpoints.default(X = \"s100b\", status = \"outcome\", tag.healthy = \"Good\", \n##     methods = \"Youden\", data = aSAH)\n## \n## Area under the ROC curve (AUC):  0.731 (0.63, 0.833) \n## \n## CRITERION: Youden\n## Number of optimal cutoffs: 1\n## \n##                     Estimate\n## cutoff             0.2200000\n## Se                 0.6341463\n## Sp                 0.8055556\n## PPV                0.6500000\n## NPV                0.7945205\n## DLR.Positive       3.2613240\n## DLR.Negative       0.4541632\n## FP                14.0000000\n## FN                15.0000000\n## Optimal criterion  0.4397019\n\n给出的结果也是和pROC以及cutoff包是一致的。\n画出来的图也是和pROC一样的：\n\nplot(oc_youden)\n\n\n\n## Press return for next page....\n\n\n\n\n除此之外，这个包还可以指定协变量，还可以使用最大化敏感度/特异度等指标确定最佳截点，大家可以去查看帮助文档。"
  },
  {
    "objectID": "roc-bestcut.html#cutpointr",
    "href": "roc-bestcut.html#cutpointr",
    "title": "19  ROC曲线的最佳截点",
    "section": "19.4 cutpointr",
    "text": "19.4 cutpointr\ncutpointr是一个用于整洁计算“最佳”切点的R包。它支持多种计算切点的方法，并包括几个可以通过选择切点来最大化或最小化的度量标准。cutpointr可以自动通过自助法计算最佳切点的变异性，并返回各种性能指标的袋外估计值。\n支持支二分类数据不支持生存数据。\n\ninstall.packages(\"cutpointr\")\n\n这个包也支持多种标准，使用时也要注意方向。\n\nlibrary(cutpointr)\n## \n## Attaching package: 'cutpointr'\n## The following object is masked from 'package:cutoff':\n## \n##     roc\n## The following objects are masked from 'package:pROC':\n## \n##     auc, roc\n\ncp &lt;- cutpointr(data=aSAH, \n                x=s100b, \n                class=outcome, \n                method = maximize_metric, # 最大化指标\n                metric = youden) # 选择指标\n## Assuming the positive class is Poor\n## Assuming the positive class has higher x values\nsummary(cp)\n## Method: maximize_metric \n## Predictor: s100b \n## Outcome: outcome \n## Direction: &gt;= \n## \n##     AUC   n n_pos n_neg\n##  0.7314 113    41    72\n## \n##  optimal_cutpoint youden    acc sensitivity specificity tp fn fp tn\n##              0.22 0.4397 0.7434      0.6341      0.8056 26 15 14 58\n## \n## Predictor summary: \n##     Data Min.    5% 1st Qu. Median      Mean 3rd Qu.   95% Max.        SD NAs\n##  Overall 0.03 0.046    0.09   0.14 0.2469912    0.33 0.722 2.07 0.2721603   0\n##     Good 0.04 0.040    0.08   0.11 0.1615278    0.17 0.470 0.50 0.1308548   0\n##     Poor 0.03 0.070    0.12   0.30 0.3970732    0.56 0.860 2.07 0.3751949   0\n\n画图：\n\nplot(cp)\n\n\n\n\n除此之外，还有很多其他函数没介绍，大家可以参考github：https://github.com/Thie1e/cutpointr"
  },
  {
    "objectID": "roc-bestcut.html#survavalroc",
    "href": "roc-bestcut.html#survavalroc",
    "title": "19  ROC曲线的最佳截点",
    "section": "19.5 survavalROC",
    "text": "19.5 survavalROC\n能做time-dependent-ROC分析的R包不多，找了好久也没发现一个R包可以完成time-dependent-ROC的所有分析，timeROC是比较全能的了，但是不能计算最佳截点，survavalROC可以计算最佳截点，但是又不能同时计算多个时间点的ROC曲线。\n所以最佳截点我们可以通过survivalROC包实现。\n\nload(file = \"./datasets/timeROC.RData\")\nlibrary(survivalROC)\n\n# 1年的最佳截点\nroc1 &lt;- survivalROC(Stime = df$futime,\n                   status = df$event,\n                   marker = df$riskScore,\n                   method = \"KM\",\n                   predict.time = 1 # 时间选1年\n                   )\n\nroc1$cut.values[which.max(roc1$TP - roc1$FP)] # 最佳截点的值，基于约登指数计算出来\n## [1] -0.07986499\n\n最佳截点是-0.07986499，就是这么简单，下面就是画图：\n\nplot(roc1$FP, roc1$TP, type=\"l\", xlim=c(0,1), ylim=c(0,1),   \n  xlab=paste( \"FP\", \"\\n\", \"AUC = \",round(roc1$AUC,3)), \n  ylab=\"TP\")\n  abline(0,1)\n\n\n\n\n除了以上介绍的R包，还有ThresholdROC也是同样的用法，这里就不介绍了，大家感兴趣的自己学习一下。\n公众号后台回复ROC即可获取ROC曲线合集，回复最佳截点即可获取最佳截断值相关推文合集。"
  },
  {
    "objectID": "roc-smooth.html#二分类资料的平滑roc曲线",
    "href": "roc-smooth.html#二分类资料的平滑roc曲线",
    "title": "20  平滑ROC曲线",
    "section": "20.1 二分类资料的平滑ROC曲线",
    "text": "20.1 二分类资料的平滑ROC曲线\n直接通过pROC即可实现，这个包详细使用请参考：用pROC实现ROC曲线分析\npROC包中提供了一个aSAH数据集，这是一个动脉瘤性蛛网膜下腔出血的数据集，一共113行，7列。其中：\n\ngos6：格拉斯哥量表评分\noutcome：结果变量\ngender：性别\nage：年龄\nwfns：世界神经外科医师联合会公认的神经学量表评分\ns100b：生物标志物\nndka：生物标志物\n\n\nlibrary(pROC)\ndata(\"aSAH\")\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n可以直接在roc函数中指定，也可以使用smooth函数：\n\nroc(aSAH$outcome, aSAH$wfns, smooth = T)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n## \n## Call:\n## roc.default(response = aSAH$outcome, predictor = aSAH$wfns, smooth = T)\n## \n## Data: aSAH$wfns in 72 controls (aSAH$outcome Good) &lt; 41 cases (aSAH$outcome Poor).\n## Smoothing: binormal \n## Area under the curve: 0.8454\n# 或者\nrr &lt;- roc(aSAH$outcome, aSAH$wfns)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nrrs &lt;- smooth(rr)\n\n画图：\n\nplot(rrs,auc.polygon=T,auc.polygon.col=\"steelblue\",\n     print.auc=T,\n     print.auc.x=1,print.auc.y=0.9,print.auc.col=\"firebrick\",\n     print.auc.cex=2)"
  },
  {
    "objectID": "roc-smooth.html#生存资料的平滑roc曲线",
    "href": "roc-smooth.html#生存资料的平滑roc曲线",
    "title": "20  平滑ROC曲线",
    "section": "20.2 生存资料的平滑ROC曲线",
    "text": "20.2 生存资料的平滑ROC曲线\n不考虑时间因素的ROC曲线可以使用pROC包中的smooth参数实现平滑版的曲线。time-dependent ROC目前还没发现比较好的方法可以直接实现，只能使用ggplot2曲线救国了。\n\nrm(list = ls())\nlibrary(timeROC)\nlibrary(survival)\n\nload(file = \"./datasets/timeROC.RData\")\n\n\n20.2.1 方法1：ggplot2\n首先看一下数据结构，对于多个时间点的ROC，需要3列数据：time, event, marker(比如你计算得到的risk score)\n首先还是借助timeROC包构建多个时间点的ROC对象：\n\nROC &lt;- timeROC(T = df$futime,   \n               delta = df$event,   \n               marker = df$riskScore,   \n               cause = 1,                \n               weighting = \"marginal\",   \n               times = c(1, 2, 3),       \n               iid = TRUE)\n\nROC   #查看模型变量信息\n## Time-dependent-Roc curve estimated using IPCW  (n=297, without competing risks). \n##     Cases Survivors Censored AUC (%)   se\n## t=1    57       203       37   71.02 3.68\n## t=2    66       106      125   69.23 3.94\n## t=3    68        74      155   65.53 4.85\n## \n## Method used for estimating IPCW:marginal \n## \n## Total computation time : 0.08  secs.\n\n默认的画出来也还可以，但是就是觉得可以更好看一点。\n\nplot(ROC, \n     time=1, col=\"red\", lwd=2, title = \"\")   #time是时间点，col是线条颜色\nplot(ROC,\n     time=2, col=\"blue\", add=TRUE, lwd=2)    #add指是否添加在上一张图中\nplot(ROC,\n     time=3, col=\"orange\", add=TRUE, lwd=2)\n\n#添加标签信息\nlegend(\"bottomright\",\n       c(paste0(\"AUC at 1 year: \",round(ROC[[\"AUC\"]][1],2)), \n         paste0(\"AUC at 2 year: \",round(ROC[[\"AUC\"]][2],2)), \n         paste0(\"AUC at 3 year: \",round(ROC[[\"AUC\"]][3],2))),\n       col=c(\"red\", \"blue\", \"orange\"),\n       lty=1, lwd=2,bty = \"n\")   \n\n\n\n\n下面使用ggplot2画图。\n首先是提取数据，这个提取数据比起lasso的真的是很简单了，不过现在提取lasso的数据也很简单。\n\ndf_plot &lt;- data.frame(tpr = as.numeric(ROC$TP),\n                 fpr = as.numeric(ROC$FP),\n                 year = rep(c(\"1-year\",\"2-year\",\"3-year\"),each = nrow(ROC$TP)))\n\nhead(df_plot)\n##          tpr         fpr   year\n## 1 0.00000000 0.000000000 1-year\n## 2 0.00000000 0.004926108 1-year\n## 3 0.01809868 0.004926108 1-year\n## 4 0.03681243 0.004926108 1-year\n## 5 0.03681243 0.009852217 1-year\n## 6 0.05425138 0.009852217 1-year\n\n下面是画图代码，平滑曲线，说简单确实简单，如果对ggplot2不熟悉，确实也很难想到：\n\nlibrary(ggplot2)\n\np &lt;- ggplot(df_plot, aes(fpr, tpr, color = year)) +\n  geom_smooth(se=FALSE, linewidth=1.2)+ # 这就是平滑曲线的关键\n  geom_abline(slope = 1, intercept = 0, color = \"grey10\",linetype = 2) +\n  scale_color_manual(values = c(\"#E41A1C\",\"#377EB8\",\"#4DAF4A\"),\n                     name = NULL, \n                     labels = c(paste0(\"AUC at 1 year: \",round(ROC[[\"AUC\"]][1],2)), \n                                paste0(\"AUC at 2 year: \",round(ROC[[\"AUC\"]][2],2)), \n                                paste0(\"AUC at 3 year: \",round(ROC[[\"AUC\"]][3],2)))\n                     ) + \n  coord_fixed(ratio = 1) +\n  labs(x = \"1 - Specificity\", y = \"Sensitivity\") +\n  theme_minimal(base_size = 14, base_family = \"sans\") +\n  theme(legend.position = c(0.7,0.15), \n        panel.border = element_rect(fill = NA),\n        axis.text = element_text(color = \"black\"))\n\np\n## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n20.2.2 方法2：risksetROC\n使用risksetROC包实现。\n使用survival包中的pbc数据集。\n\nlibrary(risksetROC)\n## Loading required package: MASS\nlibrary(survival)\ndata(pbc)\nstr(pbc)\n## 'data.frame':    418 obs. of  20 variables:\n##  $ id      : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ fudays  : int  400 4500 1012 1925 1504 2503 1832 2466 2400 51 ...\n##  $ status  : int  2 0 2 2 1 2 0 2 2 2 ...\n##  $ drug    : int  1 1 1 1 2 2 2 2 1 2 ...\n##  $ age     : int  21464 20617 25594 19994 13918 24201 20284 19379 15526 25772 ...\n##  $ sex     : int  1 1 0 1 1 1 1 1 1 1 ...\n##  $ ascites : int  1 0 0 0 0 0 0 0 0 1 ...\n##  $ hepatom : int  1 1 0 1 1 1 1 0 0 0 ...\n##  $ spiders : int  1 1 0 1 1 0 0 0 1 1 ...\n##  $ edema   : num  1 0 0.5 0.5 0 0 0 0 0 1 ...\n##  $ bili    : num  14.5 1.1 1.4 1.8 3.4 0.8 1 0.3 3.2 12.6 ...\n##  $ chol    : int  261 302 176 244 279 248 322 280 562 200 ...\n##  $ albumin : num  2.6 4.14 3.48 2.54 3.53 3.98 4.09 4 3.08 2.74 ...\n##  $ copper  : int  156 54 210 64 143 50 52 52 79 140 ...\n##  $ alkphos : num  1718 7395 516 6122 671 ...\n##  $ sgot    : num  137.9 113.5 96.1 60.6 113.2 ...\n##  $ trig    : int  172 88 55 92 72 63 213 189 88 143 ...\n##  $ platelet: int  190 221 151 183 136 NA 204 373 251 302 ...\n##  $ protime : num  12.2 10.6 12 10.3 10.9 11 9.7 11 11 11.5 ...\n##  $ stage   : int  4 3 4 4 3 3 3 3 2 4 ...\n\n做一些准备工作。只使用前312行数据，生存状态用1表示终点事件，0表示删失，然后建立cox模型，计算出线性预测值作为marker：\n\npbc1 &lt;- pbc[1:312,]\nsurvival.status &lt;- ifelse(pbc1$status==2,1,0)\nsurvival.time &lt;- pbc1$fudays\n\npbc1$status1 &lt;- survival.status\n\nfit &lt;- coxph(Surv(survival.time, status1) ~ log(bili)+\n               log(protime)+edema+albumin+age,\n             data = pbc1\n             )\neta &lt;- fit$linear.predictors\n\n使用方法和timeROC以及survivalROC基本上是一样的：\n\nnobs &lt;- length(survival.time[survival.status==1])\nspan &lt;- 1.0*(nobs^(-0.2))\n\n# 3种方法都试一下，然后画在一起\nROC.CC90 &lt;- risksetROC(Stime = survival.time, status = survival.status,\n                       marker = eta, predict.time = 90, method = \"Cox\",\n                       main=\"time-denpendent ROC with riksetROC\",\n                       lty=2, lwd=2,col=\"red\"\n                       )\nROC.SS90 &lt;- risksetROC(Stime = survival.time, status = survival.status,\n                       marker = eta, predict.time = 90, method = \"Schoenfeld\",\n                       plot = F, span = span\n                       )\nROC.LL90 &lt;- risksetROC(Stime = survival.time, status = survival.status,\n                       marker = eta, predict.time = 90, method = \"LocalCox\",\n                       plot = F, span = span\n                       )\n\nlines(ROC.SS90$FP, ROC.SS90$TP, lty=3, lwd=2, col=\"darkblue\")\nlines(ROC.LL90$FP, ROC.LL90$TP, lty=4, lwd=2, col=\"green\")\nlegend(0.6,0.25, lty = c(2,3,4),col = c(\"red\",\"darkblue\",\"green\"),\n       legend = c(\"Cox\",\"Schoenfeld\",\"LocalCox\"), bty = \"n\")\n\n\n\n\n完美的平滑曲线，不用自己实现。"
  },
  {
    "objectID": "roc-compare.html#二分类资料的roc比较",
    "href": "roc-compare.html#二分类资料的roc比较",
    "title": "21  ROC曲线的显著性检验",
    "section": "21.1 二分类资料的ROC比较",
    "text": "21.1 二分类资料的ROC比较\n可以通过pROC包实现的，使用其中roc.test()函数可实现两个ROC的Delong检验。\n使用pROC包的aSAH数据，其中outcome列是结果变量，1代表Good，2代表Poor。\n\nlibrary(pROC)\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\n\ndata(aSAH)\ndim(aSAH)\n## [1] 113   7\n\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n构建两个ROC对象，然后直接比较即可：\n\nroc1 &lt;- roc(aSAH$outcome,aSAH$s100b)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nroc2 &lt;- roc(aSAH$outcome,aSAH$ndka)\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\nres &lt;- roc.test(roc1,roc2)\nres\n## \n##  DeLong's test for two correlated ROC curves\n## \n## data:  roc1 and roc2\n## Z = 1.3908, p-value = 0.1643\n## alternative hypothesis: true difference in AUC is not equal to 0\n## 95 percent confidence interval:\n##  -0.04887061  0.28769174\n## sample estimates:\n## AUC of roc1 AUC of roc2 \n##   0.7313686   0.6119580\n\n这个函数里面有个method参数：delong/bootstrap/venkatraman，默认是delong，delong和bootstrap用于比较AUC，如果只是ROC曲线的比较，需要用venkatraman。关于这几种方法的具体原理，大家可以去翻相关的论文~\nroc.test只能用于两个ROC的比较，如果是多个比较，可以使用MedCalc软件，这个是和SPSS类似的软件，只要点点点即可。\n当然也是可以直接画在图里的：\n\nrocobj1 &lt;- plot.roc(aSAH$outcome, aSAH$s100,percent=TRUE, col=\"#1c61b6\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\nrocobj2 &lt;- lines.roc(aSAH$outcome, aSAH$ndka, percent=TRUE, col=\"#008600\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\nlegend(\"bottomright\", legend=c(\"S100B\", \"NDKA\"), col=c(\"#1c61b6\", \"#008600\"), lwd=2)\n\ntestobj &lt;- roc.test(rocobj1, rocobj2)\n\ntext(50, 50, labels=paste(\"p-value =\", format.pval(testobj$p.value)), adj=c(0, .5))\n\n\n\n\n当然你也可以用其他非参数检验的方法进行比较，比如mann whitney u检验。"
  },
  {
    "objectID": "roc-compare.html#生存资料roc的比较",
    "href": "roc-compare.html#生存资料roc的比较",
    "title": "21  ROC曲线的显著性检验",
    "section": "21.2 生存资料ROC的比较",
    "text": "21.2 生存资料ROC的比较\n使用timeROC包实现。\n还是用之前推文中用过的例子，获取数据请翻看之前的推文~\n\nrm(list = ls())\nlibrary(timeROC)\nlibrary(survival)\n\nload(file = \"./datasets/timeROC.RData\")\n\n使用其中的df2这个数据：\n\nstr(df2)\n## 'data.frame':    297 obs. of  8 variables:\n##  $ event    : num  0 0 1 0 0 1 0 0 0 0 ...\n##  $ age      : int  59 63 65 73 59 66 56 42 61 48 ...\n##  $ riskScore: num  -0.249 -0.511 -0.211 -0.427 0.279 ...\n##  $ futime   : num  3.03 1.16 1.82 1.52 1.34 ...\n##  $ gender   : num  2 2 2 1 2 2 1 2 2 2 ...\n##  $ t        : num  4 4 4 3 3 3 5 3 NA 4 ...\n##  $ n        : num  1 5 1 1 1 1 3 1 NA 1 ...\n##  $ m        : num  1 1 1 1 1 3 1 1 3 3 ...\n\n构建几个timeROC:\n\n# riskScore的ROC曲线\nROC.risk &lt;- timeROC(T=df2$futime,\n                    delta=df2$event,   \n                    marker=df2$riskScore,   \n                    cause=1,                \n                    weighting=\"marginal\",   \n                    times=3,  # c(1,2) \n                    iid=TRUE)\n\n\n# age的ROC曲线\nROC.age &lt;- timeROC(T=df2$futime,   \n                   delta=df2$event,   \n                   marker=df2$age,   \n                   cause=1,   \n                   weighting=\"marginal\",   \n                   times=3,   # c(1,2)\n                   iid=TRUE)\n\n比较就用compare()函数即可：\n\ncompare(ROC.risk, ROC.age)\n## $p_values_AUC\n##       t=0       t=3 \n##        NA 0.4544231\n\n同时使用多个时间点也是可以的：\n\n# riskScore的ROC曲线\nROC.risk &lt;- timeROC(T=df2$futime,\n                    delta=df2$event,   \n                    marker=df2$riskScore,   \n                    cause=1,                \n                    weighting=\"marginal\",   \n                    times=c(1,2),\n                    iid=TRUE)\n\n\n# age的ROC曲线\nROC.age &lt;- timeROC(T=df2$futime,   \n                   delta=df2$event,   \n                   marker=df2$age,   \n                   cause=1,   \n                   weighting=\"marginal\",   \n                   times=c(1,2),\n                   iid=TRUE)\n\ncompare(ROC.risk, ROC.age)\n## $p_values_AUC\n##        t=1        t=2 \n## 0.09758546 0.27995259\n\ncompare(ROC.risk, ROC.age, adjusted = T) # 计算调整p值\n## $p_values_AUC\n##                     t=1       t=2\n## Non-adjusted 0.09758546 0.2799526\n## Adjusted     0.14983636 0.3984702\n## \n## $Cor\n##           [,1]      [,2]\n## [1,] 1.0000000 0.7750774\n## [2,] 0.7750774 1.0000000\n\n画图就不演示了，可以参考前面的内容。"
  },
  {
    "objectID": "roc-attention.html#准备数据",
    "href": "roc-attention.html#准备数据",
    "title": "22  R语言计算AUC(ROC)注意事项",
    "section": "22.1 准备数据",
    "text": "22.1 准备数据\n假如我们要用ca125的值预测患者是不是癌症，虚构一个数据：\n\nset.seed(20220840)\nca125_1 &lt;- c(rnorm(10,80,20),rnorm(20,50,10))\nca125_2 &lt;- c(rnorm(10,20,20),rnorm(20,70,10))\nclass=c(rep(1:0,c(10,20)))\n\ntumor &lt;- c(rep(c(\"癌症\",\"非癌症\"),c(10,20)))\n\ndf &lt;- data.frame(`class`=class,`ca125_1`=ca125_1,`ca125_2`=ca125_2,\n                 `tumor`=tumor\n                 )\npsych::headTail(df)\n##     class ca125_1 ca125_2  tumor\n## 1       1   51.88    5.96   癌症\n## 2       1   82.46   10.59   癌症\n## 3       1  113.67  -19.22   癌症\n## 4       1   63.49    6.08   癌症\n## ...   ...     ...     ...   &lt;NA&gt;\n## 27      0   53.33   74.34 非癌症\n## 28      0   59.92   51.11 非癌症\n## 29      0   46.46   64.21 非癌症\n## 30      0   30.03    66.5 非癌症"
  },
  {
    "objectID": "roc-attention.html#rocr",
    "href": "roc-attention.html#rocr",
    "title": "22  R语言计算AUC(ROC)注意事项",
    "section": "22.2 ROCR",
    "text": "22.2 ROCR\nROCR的使用非常简单，两步完成，需要提供用来预测结果的指标以及真实结果。\n关于这个包计算的阳性结果的AUC还是阴性结果的AUC，它的规则是这样的：\nROCR默认计算顺序靠后的类别的AUC。如果提供给labels的值是有序因子型变量，排在前面的默认是阴性结果（negtive），排在后面的默认是阳性结果（positive），默认计算阳性结果（排序靠后）的AUC。如果是无序因子、数值、字符、逻辑型变量，会按照R语言的默认排序，比如按照数字大小、首字母顺序等，也是计算排序靠后的类别的AUC。\n比如我们这个df数据集，其中的tumor这一列是字符型，默认的顺序是：癌症，非癌症，是按照首字母顺序排列的，因此在计算AUC时，默认是计算的非癌症的AUC。\n\nlibrary(ROCR)\n\npred &lt;- prediction(predictions = ca125_1, # 预测指标\n                   labels = tumor # 真实结果\n                   )\n\nperformance(pred, \"auc\")@y.values[[1]]\n## [1] 0.075\n\n但是大多数时候我们需要的都是阳性结果（比如这里我们想计算癌症的AUC，而不是非癌症）的AUC，所以我建议大家在使用R包计算AUC或者画ROC曲线时，手动指定顺序！\n\npred &lt;- prediction(predictions = ca125_1, # 预测指标\n                   labels = tumor # 真实结果\n                   ,label.ordering = c(\"非癌症\",\"癌症\") # 此时就是计算癌症的AUC\n                   )\n\nperformance(pred, \"auc\")@y.values[[1]]\n## [1] 0.925"
  },
  {
    "objectID": "roc-attention.html#proc",
    "href": "roc-attention.html#proc",
    "title": "22  R语言计算AUC(ROC)注意事项",
    "section": "22.3 pROC",
    "text": "22.3 pROC\n\nlibrary(pROC)\n\npROC包计算AUC也需要：用来预测结果的指标以及真实结果。\n这个包计算pROC略有不同，它是根据中位数来的，谁的中位数大，就计算谁的AUC，比如我们的这个例子，计算下中位数看看：\n\n# 把ca125_1按照tumor的两个类别进行分组，然后分别计算中位数\ntapply(ca125_1, tumor, median)\n##     癌症   非癌症 \n## 81.34426 49.99926\n\n结果是癌症组的中位数＞非癌症组的中位数，所以是计算癌症的AUC。\n计算AUC：\n\nroc(response=tumor, predictor=ca125_1)\n## Setting levels: control = 癌症, case = 非癌症\n## Setting direction: controls &gt; cases\n## \n## Call:\n## roc.default(response = tumor, predictor = ca125_1)\n## \n## Data: ca125_1 in 10 controls (tumor 癌症) &gt; 20 cases (tumor 非癌症).\n## Area under the curve: 0.925\n\n再来看看ca125_2这一列指标：\n\n# 把ca125_2按照tumor的两个类别进行分组，然后分别计算中位数\ntapply(ca125_2, tumor, median)\n##     癌症   非癌症 \n## 13.52771 69.69272\n\n结果是癌症组的中位数＜非癌症组，所以是计算非癌症的AUC。\n\nroc(response=tumor, predictor=ca125_2)\n## Setting levels: control = 癌症, case = 非癌症\n## Setting direction: controls &lt; cases\n## \n## Call:\n## roc.default(response = tumor, predictor = ca125_2)\n## \n## Data: ca125_2 in 10 controls (tumor 癌症) &lt; 20 cases (tumor 非癌症).\n## Area under the curve: 0.9\n\n如果想要手动指定，需要设置levels和direction：\n\n# 此时计算的就是癌症的AUC\nroc(response=tumor, predictor=ca125_2,\n    levels=c(\"非癌症\", \"癌症\"), # 这个顺序随便设定，重要的是direction\n    direction = \"&lt;\" # 手动设定非癌症 &lt; 癌症\n    )\n## \n## Call:\n## roc.default(response = tumor, predictor = ca125_2, levels = c(\"非癌症\",     \"癌症\"), direction = \"&lt;\")\n## \n## Data: ca125_2 in 20 controls (tumor 非癌症) &lt; 10 cases (tumor 癌症).\n## Area under the curve: 0.1"
  },
  {
    "objectID": "roc-attention.html#yardstick",
    "href": "roc-attention.html#yardstick",
    "title": "22  R语言计算AUC(ROC)注意事项",
    "section": "22.4 yardstick",
    "text": "22.4 yardstick\n最后给大家演示下yardstick包的做法，其中truth必须提供因子型，使用event_level指定到底是计算谁的AUC：\n\ndf$tumor &lt;- factor(df$tumor,levels = c(\"癌症\",\"非癌症\"))\n\nlibrary(yardstick)\nroc_auc(data = df, \n        tumor,\n        ca125_1,\n        event_level=\"first\")\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc binary         0.925\n\n画ROC曲线：\n\nyardstick::roc_curve(df, truth=tumor,\n                   ca125_1,\n                   event_level=\"first\") |&gt; \n  ggplot2::autoplot()\n\n\n\n\nR语言中的ROC曲线R包都有这样的潜规则，大家在使用的时候一定要注意哦~\n示例数据还提供了用数值表示的结果变量class，感兴趣的可以试试看，是不是和我说的一样！"
  },
  {
    "objectID": "roc-many.html#准备数据",
    "href": "roc-many.html#准备数据",
    "title": "23  多指标联合诊断的ROC曲线",
    "section": "23.1 准备数据",
    "text": "23.1 准备数据\n\nlibrary(pROC)\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\n\ndata(aSAH)\nstr(aSAH)\n## 'data.frame':    113 obs. of  7 variables:\n##  $ gos6   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 5 5 5 5 1 1 4 1 5 4 ...\n##  $ outcome: Factor w/ 2 levels \"Good\",\"Poor\": 1 1 1 1 2 2 1 2 1 1 ...\n##  $ gender : Factor w/ 2 levels \"Male\",\"Female\": 2 2 2 2 2 1 1 1 2 2 ...\n##  $ age    : int  42 37 42 27 42 48 57 41 49 75 ...\n##  $ wfns   : Ord.factor w/ 5 levels \"1\"&lt;\"2\"&lt;\"3\"&lt;\"4\"&lt;..: 1 1 1 1 3 2 5 4 1 2 ...\n##  $ s100b  : num  0.13 0.14 0.1 0.04 0.13 0.1 0.47 0.16 0.18 0.1 ...\n##  $ ndka   : num  3.01 8.54 8.09 10.42 17.4 ...\n\n其中outcome是结果变量，是二分类的，其余列是预测变量。"
  },
  {
    "objectID": "roc-many.html#多指标联合诊断的roc",
    "href": "roc-many.html#多指标联合诊断的roc",
    "title": "23  多指标联合诊断的ROC曲线",
    "section": "23.2 多指标联合诊断的ROC",
    "text": "23.2 多指标联合诊断的ROC\n假如现在我想使用s100b/ndka/age这3个变量来预测结果，该如何画出这3个变量联合诊断的ROC曲线呢？\n首先，使用这3个变量建立逻辑回归：\n\nf &lt;- glm(outcome ~ s100b + ndka + age, data = aSAH, family = binomial())\n\n然后，计算逻辑回归给出的概率：\n\n# 等价于直接使用 f$fitted\npred &lt;- predict(f, newdata = aSAH, type = \"response\")\n\naSAH$pred &lt;- pred\n\n用这个pred就可以画ROC曲线了：\n\nlibrary(yardstick)\nlibrary(ggplot2)\n\nroc_curve(data=aSAH, outcome, pred,event_level = \"second\") |&gt; \n  autoplot()"
  },
  {
    "objectID": "roc-many.html#测试集怎么办",
    "href": "roc-many.html#测试集怎么办",
    "title": "23  多指标联合诊断的ROC曲线",
    "section": "23.3 测试集怎么办？",
    "text": "23.3 测试集怎么办？\n很简单，只要把predict中的数据集换成测试集即可：\n\n# 换成测试集即可\npred &lt;- predict(f, newdata = 你的测试集, type = \"response\")\n\n剩下的就都一样了！"
  },
  {
    "objectID": "roc-bootstrap.html#fbroc",
    "href": "roc-bootstrap.html#fbroc",
    "title": "24  bootstrap ROC/AUC",
    "section": "24.1 fbroc",
    "text": "24.1 fbroc\n先介绍一个最简单的，用fbroc这个包实现，因为你在必应或者谷歌搜索bootstrap ROC in R，前几个结果中就是这个包。\n\nlibrary(fbroc)\n## Loading required package: ggplot2\n\n这个包在使用时需要把结果变量变为逻辑型：\n\noutcome1 &lt;- ifelse(aSAH$outcome == \"Good\",FALSE,TRUE)\n\n然后1行代码即可实现，默认是1000次bootstrap：\n\nset.seed(123)\nresult.boot &lt;- boot.roc(aSAH$s100b, outcome1)\nresult.boot\n## \n## Bootstraped uncached ROC Curve with 41 positive and 72 negative samples. \n##  \n## The AUC is 0.73.\n##  \n## 1000 bootstrap samples will be calculated. \n## The results use up 0 MB of memory.\n\n获取1000次bootstrap AUC的可信区间，还同时给出了标准误：\n\nset.seed(123)\nperf(result.boot, \"auc\", conf.level = 0.95)\n## \n## \n##                 Bootstrapped ROC performance metric\n## \n## Metric: AUC\n## Bootstrap replicates: 1000\n## Observed: 0.731\n## Std. Error: 0.052\n## 95% confidence interval:\n## 0.625 0.824\n\n把这1000条ROC曲线画在一起，就得到bootstrap ROC了：\n\nplot(result.boot)\n\n\n\n\n这个是我目前找到的最简单的方法。"
  },
  {
    "objectID": "roc-bootstrap.html#tidyverse",
    "href": "roc-bootstrap.html#tidyverse",
    "title": "24  bootstrap ROC/AUC",
    "section": "24.2 tidyverse",
    "text": "24.2 tidyverse\n后面的方法就是根据开头说的思路，一步一步的实现了。\n先说个tidy的方法，借助tidyverse和tidymodels实现。\n\nlibrary(yardstick)\nlibrary(rsample)\nlibrary(tidyverse)\n\n先说下如何在tidymodels中绘制ROC曲线，详情可参考：tidymodels-yardstick：衡量模型性能\n在tidymodels中画一条ROC曲线非常简单，首先是计算画图需要的数据：\n\nroc_data &lt;- roc_curve(aSAH, outcome, s100b,event_level = \"second\")\nroc_data\n## # A tibble: 52 × 3\n##    .threshold specificity sensitivity\n##         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n##  1    -Inf         0            1    \n##  2       0.03      0            1    \n##  3       0.04      0            0.976\n##  4       0.05      0.0694       0.976\n##  5       0.06      0.111        0.976\n##  6       0.07      0.139        0.976\n##  7       0.08      0.222        0.902\n##  8       0.09      0.306        0.878\n##  9       0.1       0.389        0.829\n## 10       0.11      0.486        0.780\n## # ℹ 42 more rows\n\n然后是画图：\n\nautoplot(roc_data)\n\n\n\n\n接下来只要使用bootstrap生成1000个自助集就可以很方便的绘制1000条ROC曲线了。\n生成1000个自助集：\n\nset.seed(123)\nasb &lt;- bootstraps(aSAH, times = 1000)\nasb\n## # Bootstrap sampling \n## # A tibble: 1,000 × 2\n##    splits           id           \n##    &lt;list&gt;           &lt;chr&gt;        \n##  1 &lt;split [113/44]&gt; Bootstrap0001\n##  2 &lt;split [113/43]&gt; Bootstrap0002\n##  3 &lt;split [113/47]&gt; Bootstrap0003\n##  4 &lt;split [113/41]&gt; Bootstrap0004\n##  5 &lt;split [113/37]&gt; Bootstrap0005\n##  6 &lt;split [113/37]&gt; Bootstrap0006\n##  7 &lt;split [113/39]&gt; Bootstrap0007\n##  8 &lt;split [113/38]&gt; Bootstrap0008\n##  9 &lt;split [113/33]&gt; Bootstrap0009\n## 10 &lt;split [113/42]&gt; Bootstrap0010\n## # ℹ 990 more rows\n\n定义一个函数，获取自助集：这是tidymodels中的常见操作，可参考：tidymodels数据划分\n\nff &lt;- function(split){analysis(split)}\n\n下面就是提取1000个自助集的数据，对每个自助集进行1次ROC分析，以获取画图数据：\n\nplot_data &lt;- asb %&gt;% \n  mutate(boot_data = map(splits, ff)) %&gt;% \n  unnest(boot_data) %&gt;% \n  group_by(id) %&gt;% \n  roc_curve(outcome, s100b,event_level = \"second\") \n\ndim(plot_data)\n## [1] 40007     4\nhead(plot_data)\n## # A tibble: 6 × 4\n## # Groups:   id [1]\n##   id            .threshold specificity sensitivity\n##   &lt;chr&gt;              &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n## 1 Bootstrap0001    -Inf         0            1    \n## 2 Bootstrap0001       0.04      0            1    \n## 3 Bootstrap0001       0.05      0.0779       1    \n## 4 Bootstrap0001       0.06      0.143        1    \n## 5 Bootstrap0001       0.07      0.195        1    \n## 6 Bootstrap0001       0.08      0.312        0.944\n\n最后把1000条ROC曲线画在一起即可：也就是大家需要的bootstrap ROC：\n\nggplot()+\n  # 自助集的ROC曲线，共1000条\n  geom_path(data = plot_data,\n            mapping=aes(1-specificity, sensitivity,group=id),color = \"grey\")+\n  # 原始数据的ROC曲线\n  geom_path(data = roc_data, mapping = aes(1-specificity, sensitivity),\n            color=\"blue\", linewidth=1.5)+\n  theme_bw()\n\n\n\n\n由于我们已经进行了1000次ROC分析，那自然就可以获得1000个AUC，所以根据这1000个AUC，就可以计算均值、标准差、标准误、可信区间。\n先获取1000个AUC：\n\nboot_auc &lt;- asb %&gt;% \n  mutate(boot_data = map(splits, ff)) %&gt;% \n  unnest(boot_data) %&gt;% \n  group_by(id) %&gt;% \n  roc_auc(outcome, s100b,event_level = \"second\") \n#boot_auc\ndim(boot_auc)\n## [1] 1000    4\nhead(boot_auc)\n## # A tibble: 6 × 4\n##   id            .metric .estimator .estimate\n##   &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 Bootstrap0001 roc_auc binary         0.799\n## 2 Bootstrap0002 roc_auc binary         0.721\n## 3 Bootstrap0003 roc_auc binary         0.774\n## 4 Bootstrap0004 roc_auc binary         0.707\n## 5 Bootstrap0005 roc_auc binary         0.743\n## 6 Bootstrap0006 roc_auc binary         0.701\n\n这1000个AUC基本接近正态分布：\n\nggplot(boot_auc, aes(x=.estimate))+\n  geom_density()\n\n\n\n\n计算置信区间，公式如下（数学知识和统计知识，网络搜索或者看课本都可以）：\n\n可信区间下限 = 均值 - z * 标准误\n可信区间上限 = 均值 + z * 标准误\n\n先计算标准误：\n\nsample_mean &lt;- mean(boot_auc$.estimate)\nsample_mean\n## [1] 0.7315554\nsample_size &lt;- nrow(boot_auc)\nstandard_d &lt;- sd(boot_auc$.estimate)\nse &lt;- standard_d/sqrt(sample_size)\nse\n## [1] 0.001544964\n\n计算置信区间：\n\nconf_low &lt;- sample_mean - 1.96 * se\nconf_low\n## [1] 0.7285273\n\nconf_high &lt;- sample_mean + 1.96 * se\nconf_high\n## [1] 0.7345836"
  },
  {
    "objectID": "roc-bootstrap.html#base-r",
    "href": "roc-bootstrap.html#base-r",
    "title": "24  bootstrap ROC/AUC",
    "section": "24.3 base R",
    "text": "24.3 base R\n和tidy的方法没有本质区别，只是实现方式使用base R语法而已。这让我想起了某个外国网友对R的评论：目前很多人不是纠结于用R还是用Python，而是纠结于用base R还是tidy R。base R和tidy R真是太割裂了。\n先进行1次bootstrap（获取样本编号）看看效果：\n\nset.seed(123)\nbootset &lt;- sample(nrow(aSAH), size = nrow(aSAH), replace = T)\nbootset\n##   [1]  31  79  51  14  67  42  50  43 101  14  25  90  91  69  91  57  92   9\n##  [19]  93  99  72  26   7  42   9  83  36  78  81  43 103  76  15  32 106 109\n##  [37]   7   9  41  74  23  27  60  53   7  53  27  96  38  89  34  93  69  72\n##  [55]  76  63  13  82  97  91  25  38  21  79  41  47  90  60  95  16  94   6\n##  [73] 107  72  86  86  39  31 112  81  50 113  34   4  13  69  25  52  22  89\n##  [91]  32 110  25  87  35  40 112  30  12  31 110  30  64  99  14  93  96  71\n## [109]  67  23  79  85  37\n\n然后定义一个函数，获取每次的自助集：\n\nget_bootset &lt;- function(data){\n  boot_index &lt;- sample(nrow(data), size = nrow(data), replace = T)\n  bootset &lt;- data[boot_index,]\n  return(bootset)\n}\n\n#set.seed(123)\n#get_bootset(aSAH)\n\n使用bootstrap获取1000个自助集，通过for循环实现：\n\n# 每次结果都不一样\nbootsets &lt;- list()\nfor(i in 1:1000){\n  bootsets[[i]] &lt;- get_bootset(aSAH)\n}\nlength(bootsets)\n## [1] 1000\n\n对每一个自助集进行1次ROC分析，通过for循环实现：\n\nlibrary(pROC)\nrocs &lt;- list()\n\nfor(i in 1:1000){\n  rocs[[i]] &lt;- pROC::roc(bootsets[[i]][,\"outcome\"], bootsets[[i]][,\"s100b\"],\n                   quiet=T)\n}\n\n画1000条ROC曲线，还是通过for循环实现：\n\n# 提供一个画布\nplot(roc(aSAH$outcome, aSAH$s100b),col=\"blue\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\n# 画1000条ROC曲线\nfor(i in 1:1000){\n  lines.roc(rocs[[i]],col=\"grey\")\n}\n\n# 画完1000条把原来的挡住了，重新画一条\nlines.roc(roc(aSAH$outcome, aSAH$s100b),col=\"blue\")\n## Setting levels: control = Good, case = Poor\n## Setting direction: controls &lt; cases\n\n\n\n\n然后是计算1000个AUC的置信区间，和tidy的方法一样的。\n计算1000个AUC：\n\naucs &lt;- list()\n\nfor(i in 1:1000){\n  aucs[[i]] &lt;- auc(pROC::roc(bootsets[[i]][,\"outcome\"],bootsets[[i]][,\"s100b\"],\n                   quiet=T))\n}\naucs &lt;- unlist(aucs)\n\n计算可信区间：\n\nsample_mean &lt;- mean(aucs)\nsample_mean\n## [1] 0.7312995\nsample_size &lt;- length(aucs)\nstandard_d &lt;- sd(aucs)\nse &lt;- standard_d/sqrt(sample_size)\nse\n## [1] 0.001569356\n\n95%的可信区间，参考课本或者这个知乎的解释\n\nconf_low &lt;- sample_mean - 1.96 * se\nconf_low\n## [1] 0.7282235\n\nconf_high &lt;- sample_mean + 1.96 * se\nconf_high\n## [1] 0.7343754\n\n这种方法由于我没有在每次重抽样时设定种子数，导致结果是不可重复的哈，每次都不太一样~"
  },
  {
    "objectID": "roc-bootstrap.html#boot",
    "href": "roc-bootstrap.html#boot",
    "title": "24  bootstrap ROC/AUC",
    "section": "24.4 boot",
    "text": "24.4 boot\nboot是专门做重抽样的经典R包，在《R语言实战》一书中有详细介绍。\n通过这个包也可以计算bootstrap AUC的置信区间，但是这种方法只能计算指标，不能画ROC曲线。\n\nlibrary(boot)\nlibrary(pROC)\n\n定义一个函数，提取AUC：\n\n# boot的使用方式很奇怪\nget_auc &lt;- function(data, ind, outcome, predictor){\n  d = data[ind,] #这句必须加\n  au &lt;- as.numeric(auc(pROC::roc(d[,outcome], d[,predictor],quiet=T)))\n  au\n}\n\nget_auc(aSAH, outcome=\"outcome\",predictor=\"s100b\")\n## [1] 0.7313686\n\n提供给boot使用即可：\n\nset.seed(123)\nba &lt;- boot(aSAH, get_auc, R = 1000,\n           outcome=\"outcome\",predictor=\"s100b\")\nba\n## \n## ORDINARY NONPARAMETRIC BOOTSTRAP\n## \n## \n## Call:\n## boot(data = aSAH, statistic = get_auc, R = 1000, outcome = \"outcome\", \n##     predictor = \"s100b\")\n## \n## \n## Bootstrap Statistics :\n##      original       bias    std. error\n## t1* 0.7313686 0.0001084232  0.05365581\n\n结果给出了原始的AUC，以及1000次bootstrap得到的AUC的标准误。\n可以对这个结果画个图看看这1000个AUC的分布：\n\nplot(ba)\n\n\n\n\n获取这1000个AUC的置信区间，默认会给出95%的置信区间，并包含4种计算方法的结果：\n\nboot.ci(ba)\n## Warning in boot.ci(ba): bootstrap variances needed for studentized intervals\n## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n## Based on 1000 bootstrap replicates\n## \n## CALL : \n## boot.ci(boot.out = ba)\n## \n## Intervals : \n## Level      Normal              Basic         \n## 95%   ( 0.6261,  0.8364 )   ( 0.6314,  0.8479 )  \n## \n## Level     Percentile            BCa          \n## 95%   ( 0.6148,  0.8313 )   ( 0.6048,  0.8228 )  \n## Calculations and Intervals on Original Scale\n\n4种计算方法的置信区间都有了。\nOVER！"
  },
  {
    "objectID": "多分类数据的ROC曲线.html#yardstick",
    "href": "多分类数据的ROC曲线.html#yardstick",
    "title": "25  多分类数据的ROC曲线",
    "section": "25.1 yardstick",
    "text": "25.1 yardstick\nyardstick作为tidymodels的核心包之一，其使用语法和tidy系列完全一样，并且支持所有的tidy特性，学习成本非常低。而且背靠Rstudio这颗大树，稳定性和可靠性也有保障。\n接下来我们会演示衡量具有3个或更多类别的分类数据的方法。下面使用的示例数据包含了4个类别：\n\nlibrary(tidymodels)\n\ndata(hpc_cv)\ntibble(hpc_cv)\n## # A tibble: 3,467 × 7\n##    obs   pred     VF      F       M          L Resample\n##    &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;   \n##  1 VF    VF    0.914 0.0779 0.00848 0.0000199  Fold01  \n##  2 VF    VF    0.938 0.0571 0.00482 0.0000101  Fold01  \n##  3 VF    VF    0.947 0.0495 0.00316 0.00000500 Fold01  \n##  4 VF    VF    0.929 0.0653 0.00579 0.0000156  Fold01  \n##  5 VF    VF    0.942 0.0543 0.00381 0.00000729 Fold01  \n##  6 VF    VF    0.951 0.0462 0.00272 0.00000384 Fold01  \n##  7 VF    VF    0.914 0.0782 0.00767 0.0000354  Fold01  \n##  8 VF    VF    0.918 0.0744 0.00726 0.0000157  Fold01  \n##  9 VF    VF    0.843 0.128  0.0296  0.000192   Fold01  \n## 10 VF    VF    0.920 0.0728 0.00703 0.0000147  Fold01  \n## # ℹ 3,457 more rows\n\n此数据的真实结果（obs）和预测结果（pred）都是因子型，并且数据中包含每个类别的预测概率（VF、F、M、L）。\n计算多分类性能指标的函数与计算二分类性能指标的函数用法基本相同：\n\naccuracy(hpc_cv, obs, pred)\n## # A tibble: 1 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy multiclass     0.709\n\nmcc(hpc_cv, obs, pred)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 mcc     multiclass     0.515\n\n有一些指标是用来处理仅具有两个类别的数据的，但是可以通过一些方法将其扩展到具有多个类别的数据。比如宏平均（macro-averaging）、宏加权平均（macro-weighted averaging）和微平均（micro-averaging）：\n\n宏平均使用标准的二分类方法计算一组一对多的指标，并对这些指标取平均值。\n宏加权平均执行相同的操作，但平均值会根据每个类别中的样本数加权。\n微平均计算每个类别的贡献并汇总，然后从汇总中计算单个指标。\n\n我们可以基于上面的解释自己计算，也可以使用yardstick中的函数可以帮助我们实现，只需指定estimator参数即可：\n\nsensitivity(hpc_cv, obs, pred, estimator = \"macro\")\n## # A tibble: 1 × 3\n##   .metric     .estimator .estimate\n##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n## 1 sensitivity macro          0.560\n\nsensitivity(hpc_cv, obs, pred, estimator = \"macro_weighted\")\n## # A tibble: 1 × 3\n##   .metric     .estimator     .estimate\n##   &lt;chr&gt;       &lt;chr&gt;              &lt;dbl&gt;\n## 1 sensitivity macro_weighted     0.709\n\nsensitivity(hpc_cv, obs, pred, estimator = \"micro\")\n## # A tibble: 1 × 3\n##   .metric     .estimator .estimate\n##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;\n## 1 sensitivity micro          0.709\n\n也有一些指标使用预测概率衡量多分类数据，比如ROC曲线，此时必须将所有预测概率列都提供给函数：\n\nroc_auc(hpc_cv, obs, VF, F, M, L) # 默认hand_till法\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc hand_till      0.829\n\n除此之外，ROC曲线还可以使用宏加权平均衡量多分类数据：\n\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro_weighted\")\n## # A tibble: 1 × 3\n##   .metric .estimator     .estimate\n##   &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;\n## 1 roc_auc macro_weighted     0.868\n\n或者使用宏平均法：\n\nroc_auc(hpc_cv, obs, VF, F, M, L, estimator = \"macro\")\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc macro          0.869\n\n画图也是非常简单，先使用roc_curve计算画图所需数据（注意此函数不能更改计算方法），然后使用ggplot2画图即可：\n\nroc_curve(hpc_cv, obs, VF, F, M, L) %&gt;% \n  autoplot()\n\n\n\n\n结果会同时展示4个类别的ROC曲线，当然也可以自己提取数据画。数就是图，图就是数，只要把数据提取出来，就一定能画出来图。提取数据才是最关键的一步，因为画图无非就是ggplot2而已，随便买本书（我只推荐两本：《ggplot2数据分析与图形艺术》和《R数据可视化手册》）认真看看，ggplot2就学得差不多了。提取数据多数都是列表(list)格式，在tidy的世界中更常见的是tibble和列表列，这就需要你认真学习R语言的基础知识。\n\nplot_data &lt;- roc_curve(hpc_cv, obs, VF, F, M, L)\nclass(plot_data)\n## [1] \"roc_df\"     \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nglimpse(plot_data)\n## Rows: 13,876\n## Columns: 4\n## $ .level      &lt;chr&gt; \"VF\", \"VF\", \"VF\", \"VF\", \"VF\", \"VF\", \"VF\", \"VF\", \"VF\", \"VF\"…\n## $ .threshold  &lt;dbl&gt; -Inf, 4.118856e-18, 1.151238e-16, 1.447982e-16, 1.857902e-…\n## $ specificity &lt;dbl&gt; 0.0000000000, 0.0000000000, 0.0005889282, 0.0011778563, 0.…\n## $ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 0.9…\n\n这个结果一看就知道.level是4个类别，.threshold是预测变量的阈值，剩下两列是敏感性和特异性，也就是画图需要的数据。\n可以只画其中一个类别，我们以VF这个类别为例：\n\nplot_data %&gt;% \n  filter(.level == \"VF\") %&gt;% \n  ggplot(aes(1-specificity, sensitivity))+\n  geom_line()+\n  geom_abline(linetype = 2)+\n  theme_bw()\n\n\n\n\n而且有了数据，你很容易可以实现bootstrap ROC/AUC，无非就是重复多次而已。可参考：bootstrap ROC/AUC\n所有类别都画就是加个分面就行了：\n\nplot_data %&gt;% \n  #filter(.level == \"VF\") %&gt;% \n  ggplot(aes(1-specificity, sensitivity))+\n  geom_line()+\n  geom_abline(linetype = 2)+\n  facet_wrap(vars(.level))+\n  theme_bw()\n\n\n\n\n一模一样。\n所有这些性能指标都可以使用dplyr进行分组计算：\n\n# 每一次重抽样的1-vs-所有ROC曲线\nhpc_cv %&gt;% \n  group_by(Resample) %&gt;% \n  roc_curve(obs, VF, F, M, L) %&gt;% \n  autoplot()\n\n\n\n\n以上就是yardstick的简单介绍。后台回复tidymodels即可获取相关推文合集."
  },
  {
    "objectID": "多分类数据的ROC曲线.html#multiroc",
    "href": "多分类数据的ROC曲线.html#multiroc",
    "title": "25  多分类数据的ROC曲线",
    "section": "25.2 multiROC",
    "text": "25.2 multiROC\n也可以实现多分类的ROC曲线，但是很久不更新了，上次更新是2018.6.26。\ngithub地址：https://github.com/WandeRum/multiROC\n\nlibrary(multiROC)\n\n还是用上面的数据作为演示。使用起来很费事，首先得准备正确的格式。需要一个data.frame，包含真实标签和预测数值（可以是概率也可以不是，但肯定不能是预测类别），真实标签必须每个类别都是单独的1列，且列名必须是xxx_true，且必须用1表示阳性，0表示阴性，预测数值的列名必须是xxx_pred。\n首先是真实类别进行独热编码，然后改列名：\n\nhpc_cv1 &lt;- hpc_cv %&gt;% \n  model.matrix(~obs-1, .) %&gt;% \n  bind_cols(hpc_cv %&gt;% select(VF,F,M,L))\n\n# 预测结果必须添加一个  _pred_xx 后缀，不然就报错，太zz了。。。\ncolnames(hpc_cv1) &lt;- c(paste0(c(\"VF\",\"F\",\"M\",\"L\"),\"_true\"),\n                       paste0(c(\"VF\",\"F\",\"M\",\"L\"),\"_pred_rf\")) # _rf是随便加的\n\nhead(hpc_cv1)\n##   VF_true F_true M_true L_true VF_pred_rf  F_pred_rf   M_pred_rf    L_pred_rf\n## 1       1      0      0      0  0.9136340 0.07786694 0.008479147 1.991225e-05\n## 2       1      0      0      0  0.9380672 0.05710623 0.004816447 1.011557e-05\n## 3       1      0      0      0  0.9473710 0.04946767 0.003156287 4.999849e-06\n## 4       1      0      0      0  0.9289077 0.06528949 0.005787179 1.564496e-05\n## 5       1      0      0      0  0.9418764 0.05430830 0.003808013 7.294581e-06\n## 6       1      0      0      0  0.9510978 0.04618223 0.002716177 3.841455e-06\n\n计算AUC，会同时使用宏平均和微平均计算：\n\nroc_res &lt;- multi_roc(hpc_cv1)\n## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n## collapsing to unique 'x' values\n\n## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n## collapsing to unique 'x' values\n\n## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n## collapsing to unique 'x' values\n\n## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):\n## collapsing to unique 'x' values\nroc_res$AUC\n## $rf\n## $rf$VF\n## [1] 0.9145978\n## \n## $rf$F\n## [1] 0.7912642\n## \n## $rf$M\n## [1] 0.8389398\n## \n## $rf$L\n## [1] 0.9322527\n## \n## $rf$macro\n## [1] 0.8692609\n## \n## $rf$micro\n## [1] 0.9028392\n\n计算画图数据：\n\nplot_roc_df &lt;- plot_roc_data(roc_res)\ndim(plot_roc_df)\n## [1] 38140     5\nhead(plot_roc_df)\n##   Specificity  Sensitivity Group       AUC Method\n## 1           1 0.0000000000    VF 0.9145978     rf\n## 2           1 0.0005652911    VF 0.9145978     rf\n## 3           1 0.0011305822    VF 0.9145978     rf\n## 4           1 0.0016958734    VF 0.9145978     rf\n## 5           1 0.0022611645    VF 0.9145978     rf\n## 6           1 0.0028264556    VF 0.9145978     rf\n\n然后使用ggplot2画图即可：\n\nggplot(plot_roc_df, aes(x = 1-Specificity, y=Sensitivity)) +\n  geom_path(aes(color = Group), linewidth=1.5) +\n  geom_segment(aes(x = 0, y = 0, xend = 1, yend = 1), \n               colour='grey', linetype = 'dotdash') +\n  theme_bw() + \n  theme(plot.title = element_text(hjust = 0.5), \n        legend.justification=c(1, 0), legend.position=c(.95, .05),\n        legend.title=element_blank(), \n        legend.background = element_rect(fill=NULL, linewidth=0.5, \n                                         linetype=\"solid\",colour =\"black\")\n        )\n\n\n\n\n这个图是全都画在一起的，你也可以使用分面的形式。\n画图是很简单的，但是说实话这个准备数据的过程不太方便~\nOVER！\n后台回复ROC获取超全合集，回复最佳截点即可获取ROC曲线的最佳截点合集。"
  },
  {
    "objectID": "cindex.html#logistic回归的c-statistic",
    "href": "cindex.html#logistic回归的c-statistic",
    "title": "26  C-index的计算",
    "section": "26.1 logistic回归的C-statistic",
    "text": "26.1 logistic回归的C-statistic\n今天学习C-index的4种计算方法，在二分类变量中，C-statistic就是AUC，二者在数值上是一样的。\n使用lowbirth数据集，这个数据集是关于低出生体重儿是否会死亡的数据集，其中dead这一列是结果变量，0代表死亡，1代表存活，其余列都是预测变量。数据的预处理和之前一样。\n\nrm(list = ls())\nlowbirth &lt;- read.csv(\"./datasets/lowbirth.csv\")\n\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n\ntmp &lt;- lowbirth %&gt;% \n  mutate(across(where(is.character),as.factor),\n         vent = factor(vent),\n         black = ifelse(race == \"black\",1,0),\n         white = ifelse(race == \"white\",1,0),\n         other = ifelse(race %in% c(\"native American\",\"oriental\"),1,0)\n         ) %&gt;% \n  select(- race)\n\nglimpse(tmp)\n## Rows: 565\n## Columns: 12\n## $ birth    &lt;dbl&gt; 81.514, 81.552, 81.558, 81.593, 81.610, 81.624, 81.626, 81.68…\n## $ lowph    &lt;dbl&gt; 7.250000, 7.059998, 7.250000, 6.969997, 7.320000, 7.160000, 7…\n## $ pltct    &lt;int&gt; 244, 114, 182, 54, 282, 153, 229, 182, 361, 378, 255, 186, 26…\n## $ bwt      &lt;int&gt; 1370, 620, 1480, 925, 1255, 1350, 1310, 1110, 1180, 970, 770,…\n## $ delivery &lt;fct&gt; abdominal, vaginal, vaginal, abdominal, vaginal, abdominal, v…\n## $ apg1     &lt;int&gt; 7, 1, 8, 5, 9, 4, 6, 6, 6, 2, 4, 8, 1, 8, 5, 9, 9, 9, 6, 2, 1…\n## $ vent     &lt;fct&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1…\n## $ sex      &lt;fct&gt; female, female, male, female, female, female, male, male, mal…\n## $ dead     &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n## $ black    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0…\n## $ white    &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1…\n## $ other    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\n26.1.1 方法1：rms\n使用rms包构建模型，模型结果中Rank Discrim.下面的C 就是C-Statistic，本模型中C-Statistic = 0.879。\n\nlibrary(rms)\n## Loading required package: Hmisc\n## \n## Attaching package: 'Hmisc'\n## The following objects are masked from 'package:dplyr':\n## \n##     src, summarize\n## The following objects are masked from 'package:base':\n## \n##     format.pval, units\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\ndd &lt;- datadist(tmp)\noptions(datadist=\"dd\")\n\nfit2 &lt;- lrm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp,x=T,y=T)\n\nfit2\n## Logistic Regression Model\n## \n## lrm(formula = dead ~ birth + lowph + pltct + bwt + vent + black + \n##     white, data = tmp, x = T, y = T)\n## \n##                        Model Likelihood      Discrimination    Rank Discrim.    \n##                              Ratio Test             Indexes          Indexes    \n## Obs           565    LR chi2     167.56      R2       0.432    C       0.879    \n##  0            471    d.f.             7      R2(7,565)0.247    Dxy     0.759    \n##  1             94    Pr(&gt; chi2) &lt;0.0001    R2(7,235.1)0.495    gamma   0.759    \n## max |deriv| 1e-06                            Brier    0.095    tau-a   0.211    \n## \n##           Coef    S.E.    Wald Z Pr(&gt;|Z|)\n## Intercept 38.3815 11.0303  3.48  0.0005  \n## birth     -0.1201  0.0914 -1.31  0.1890  \n## lowph     -4.1451  1.1881 -3.49  0.0005  \n## pltct     -0.0017  0.0019 -0.91  0.3644  \n## bwt       -0.0031  0.0006 -5.14  &lt;0.0001 \n## vent=1     2.7526  0.7436  3.70  0.0002  \n## black      1.1974  0.8448  1.42  0.1564  \n## white      0.8597  0.8655  0.99  0.3206\n\n\n\n26.1.2 方法2：ROCR\nROCR包计算AUC，logistic回归的AUC就是C-statistic。这种方法和SPSS得到的一样。\n\nlibrary(ROCR)\n\ntmp$predvalue&lt;-predict(fit2)\n\n# 取出C-Statistics，和上面结果一样\npred &lt;- prediction(tmp$predvalue, tmp$dead)\n\nauc &lt;- round(performance(pred, \"auc\")@y.values[[1]],digits = 4)\n\n这个包也是用来画ROC曲线常用的包，可以根据上面的结果直接画出ROC曲线：\n\nperf &lt;- performance(pred,\"tpr\",\"fpr\")\nplot(perf,col=\"tomato\",lwd=2)\nabline(0,1,lty=2, col=\"grey\")\n\n\n\n\n\n\n26.1.3 方法3：pROC\npROC包计算AUC，这个包也是画ROC曲线常用的R包，但是这个包在使用时需要注意，这部分内容会在后面详细介绍。\n\nlibrary(pROC)\n## Type 'citation(\"pROC\")' for a citation.\n## \n## Attaching package: 'pROC'\n## The following objects are masked from 'package:stats':\n## \n##     cov, smooth, var\n\n# 计算AUC，也就是C-statistic\nroc(tmp$dead, tmp$predvalue, legacy.axes = T, print.auc = T, print.auc.y = 45)\n## Setting levels: control = 0, case = 1\n## Setting direction: controls &lt; cases\n## \n## Call:\n## roc.default(response = tmp$dead, predictor = tmp$predvalue, legacy.axes = T,     print.auc = T, print.auc.y = 45)\n## \n## Data: tmp$predvalue in 471 controls (tmp$dead 0) &lt; 94 cases (tmp$dead 1).\n## Area under the curve: 0.8794\n\n也是可以直接画法ROC曲线的：\n\nroc.plot &lt;- roc(tmp$dead, tmp$predvalue)\n## Setting levels: control = 0, case = 1\n## Setting direction: controls &lt; cases\n\nplot(roc.plot,legacy.axes=T)\n\n\n\n\n\n\n26.1.4 方法4：Hmisc\n使用Hmisc包。结果中的C就是C-Statistic。\n\nlibrary(Hmisc)\nsomers2(tmp$predvalue, tmp$dead)\n##           C         Dxy           n     Missing \n##   0.8793875   0.7587749 565.0000000   0.0000000"
  },
  {
    "objectID": "cindex.html#cox回归的c-statistic",
    "href": "cindex.html#cox回归的c-statistic",
    "title": "26  C-index的计算",
    "section": "26.2 cox回归的C-statistic",
    "text": "26.2 cox回归的C-statistic\ncox回归的C-statistic可以用survival包计算，需要注意，生存分析的C-statistic和AUC是不一样的。\n使用survival包自带的lung数据集进行演示。\n\nlibrary(survival)\nlibrary(dplyr)\n\ndf1 &lt;- lung %&gt;% \n  mutate(status=ifelse(status == 1,1,0))\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\nR语言自带的coxph函数即可给出C-index，非常简单：\n\ncox_fit1 &lt;- coxph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n              data = lung,x = T, y = T)\n\nsummary(cox_fit1)\n## Call:\n## coxph(formula = Surv(time, status) ~ age + sex + ph.ecog + ph.karno + \n##     pat.karno, data = lung, x = T, y = T)\n## \n##   n= 223, number of events= 160 \n##    (5 observations deleted due to missingness)\n## \n##                coef exp(coef)  se(coef)      z Pr(&gt;|z|)   \n## age        0.011383  1.011448  0.009510  1.197  0.23134   \n## sex       -0.561464  0.570373  0.170689 -3.289  0.00100 **\n## ph.ecog    0.565533  1.760386  0.186716  3.029  0.00245 **\n## ph.karno   0.015853  1.015979  0.009853  1.609  0.10762   \n## pat.karno -0.010111  0.989940  0.006881 -1.470  0.14169   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##           exp(coef) exp(-coef) lower .95 upper .95\n## age          1.0114     0.9887    0.9928     1.030\n## sex          0.5704     1.7532    0.4082     0.797\n## ph.ecog      1.7604     0.5681    1.2209     2.538\n## ph.karno     1.0160     0.9843    0.9965     1.036\n## pat.karno    0.9899     1.0102    0.9767     1.003\n## \n## Concordance= 0.647  (se = 0.025 )\n## Likelihood ratio test= 32.9  on 5 df,   p=4e-06\n## Wald test            = 33  on 5 df,   p=4e-06\n## Score (logrank) test = 33.79  on 5 df,   p=3e-06\n\nConcordance就是C-statistic，本次示例中为0.647。\n以上就是C-statistic的计算。\n获取lowbirth数据请在公众号：医学和生信笔记 后台回复20220520"
  },
  {
    "objectID": "cindex-compare.html#二分类资料c-index的比较",
    "href": "cindex-compare.html#二分类资料c-index的比较",
    "title": "27  C-index的比较",
    "section": "27.1 二分类资料C-index的比较",
    "text": "27.1 二分类资料C-index的比较\n二分类资料的AUC和C-index是一样的，所以可以参考Chapter 21关于ROC曲线的显著性检验。"
  },
  {
    "objectID": "cindex-compare.html#生存资料c-index的比较",
    "href": "cindex-compare.html#生存资料c-index的比较",
    "title": "27  C-index的比较",
    "section": "27.2 生存资料C-index的比较",
    "text": "27.2 生存资料C-index的比较\n可以使用compareC包，专门用来比较生存资料的C-index。\n\nrm(list = ls())\nlibrary(compareC)\n\n还是用之前推文的数据，获取数据可以查看历史推文。\n\nload(file = \"./datasets/timeROC.RData\")\nstr(df2)\n## 'data.frame':    297 obs. of  8 variables:\n##  $ event    : num  0 0 1 0 0 1 0 0 0 0 ...\n##  $ age      : int  59 63 65 73 59 66 56 42 61 48 ...\n##  $ riskScore: num  -0.249 -0.511 -0.211 -0.427 0.279 ...\n##  $ futime   : num  3.03 1.16 1.82 1.52 1.34 ...\n##  $ gender   : num  2 2 2 1 2 2 1 2 2 2 ...\n##  $ t        : num  4 4 4 3 3 3 5 3 NA 4 ...\n##  $ n        : num  1 5 1 1 1 1 3 1 NA 1 ...\n##  $ m        : num  1 1 1 1 1 3 1 1 3 3 ...\n\n只要提供4个参数：time，status，第一个指标，第二个指标，即可。\n\ncompareC(df2$futime,\n         df2$event,\n         df2$riskScore,\n         df2$age\n         )\n## $est.c\n##       Cxy       Cxz \n## 0.3383690 0.3894508 \n## \n## $est.diff_c\n## [1] -0.05108181\n## \n## $est.vardiff_c\n## [1] 0.002124315\n## \n## $est.varCxy\n## [1] 0.001046384\n## \n## $est.varCxz\n## [1] 0.00112578\n## \n## $est.cov\n## [1] 2.392402e-05\n## \n## $zscore\n## [1] -1.108299\n## \n## $pval\n## [1] 0.2677329\n\n第1行给出了2个C指数； 第2行是2个C指数相减的差值； 第3行是2个C指数方差相减的差值； 第4、5行是方差； 第6行是协方差； 第7、8行是z值和p值。"
  },
  {
    "objectID": "cindex-compare.html#两个cox模型的比较",
    "href": "cindex-compare.html#两个cox模型的比较",
    "title": "27  C-index的比较",
    "section": "27.3 两个cox模型的比较",
    "text": "27.3 两个cox模型的比较\n下面再多说一点两个cox模型的比较，也是很简单的，方差分析即可，使用anova()函数。\n我们用lung数据集进行演示。\n\nlibrary(survival)\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n\ndf1 &lt;- lung %&gt;% \n  mutate(status=ifelse(status == 1,1,0))\n\n建立两个cox模型：\n\ncox_fit1 &lt;- coxph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n                  data = lung,x = T, y = T)\n\ncox_fit2 &lt;- coxph(Surv(time, status) ~ ph.ecog + ph.karno + pat.karno,\n                  data = lung,x = T, y = T)\n\n直接使用anova()即可：\n\nanova(cox_fit1,cox_fit2)\n## Analysis of Deviance Table\n##  Cox model: response is  Surv(time, status)\n##  Model 1: ~ age + sex + ph.ecog + ph.karno + pat.karno\n##  Model 2: ~ ph.ecog + ph.karno + pat.karno\n##    loglik  Chisq Df Pr(&gt;|Chi|)   \n## 1 -706.48                        \n## 2 -712.83 12.703  2   0.001745 **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nrms包也带了一个lrtest()函数，也是直接构建两个cph模型即可进行比较，简单演示一下。\n\nsuppressMessages(library(rms))\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\ndd &lt;- datadist(lung)\noptions(datadist=\"dd\")\n\ncox_fit1 &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n                data = lung,x = T, y = T)\n\ncox_fit2 &lt;- cph(Surv(time, status) ~ ph.ecog + ph.karno + pat.karno,\n                data = lung,x = T, y = T)\n\nlrtest(cox_fit1,cox_fit2)\n## \n## Model 1: Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno\n## Model 2: Surv(time, status) ~ ph.ecog + ph.karno + pat.karno\n## \n##   L.R. Chisq         d.f.            P \n## 12.702485990  2.000000000  0.001744577\n\n可以看到P值是一样的~"
  },
  {
    "objectID": "conf_matrix-3d.html#d混淆矩阵可视化",
    "href": "conf_matrix-3d.html#d混淆矩阵可视化",
    "title": "28  三维混淆矩阵",
    "section": "28.1 2d混淆矩阵可视化",
    "text": "28.1 2d混淆矩阵可视化\n如果要可视化混淆矩阵，简单的2维非常简单，比如用ggplot2即可：\n\nggplot(df1, aes(pred, outcome))+\n  geom_tile(aes(fill=ca125))+\n  geom_label(data = df1 %&gt;% count(pred, outcome), aes(pred,outcome,label=n),size=12)+\n  theme_minimal()+\n  theme(legend.position = \"none\")\n\n\n\n\n想要修改更多细节，大家自己动手即可，非常简单！"
  },
  {
    "objectID": "conf_matrix-3d.html#d版混淆矩阵可视化",
    "href": "conf_matrix-3d.html#d版混淆矩阵可视化",
    "title": "28  三维混淆矩阵",
    "section": "28.2 3d版混淆矩阵可视化",
    "text": "28.2 3d版混淆矩阵可视化\n也是很简单，使用barplot3d这个包即可。这个包需要本地安装。\n注意这个包画图的顺序：从左到右，从前到后！\n\nlibrary(barplot3d)\n\n# 把混淆矩阵结果放到1个向量里\ninputdata &lt;- c(15,5,1,9)\n\n# 定义4个颜色\nmycolors &lt;- c(\"#00468BB2\", \"#ED0000B2\", \"#42B540B2\", \"#0099B4B2\")\n\nbarplot3d(rows = 2,cols = 2, z = inputdata, # 一共4个数，2行，2列\n          scalexy=8, # 让柱子胖一点\n          topcolors=mycolors, # 柱子顶部的颜色\n          sidecolors=mycolors, # 柱子的填充色\n          xlabels = c(\"pred.No\",\"pred.Yes\"), \n          ylabels = c(\"No\",\"Yes\"),\n          xsub = \"Prediction\",\n          ysub = \"Outcome\",\n          zsub = \"Count\"\n          )\n\n出图如下：\n\n这个图是3d的，画出来之后会单独弹出一个窗口，你可以随意旋转，找到自己喜欢的角度保存即可！"
  },
  {
    "objectID": "nri.html#logistic的nri",
    "href": "nri.html#logistic的nri",
    "title": "29  NRI净重新分类指数",
    "section": "29.1 logistic的NRI",
    "text": "29.1 logistic的NRI\n\n29.1.1 nricens包\n\n#install.packages(\"nricens\") # 安装R包\nlibrary(nricens)\n## Loading required package: survival\n\n使用survival包中的pbc数据集用于演示，这是一份关于原发性硬化性胆管炎的数据，其实是一份用于生存分析的数据，是有时间变量的，但是这里我们用于演示logistic回归，只要不使用time这一列就可以了。\n\nlibrary(survival)\n\n# 只使用部分数据\ndat = pbc[1:312,] \ndat = dat[ dat$time &gt; 2000 | (dat$time &lt; 2000 & dat$status == 2), ]\n\nstr(dat) # 数据长这样\n## 'data.frame':    232 obs. of  20 variables:\n##  $ id      : int  1 2 3 4 6 8 9 10 11 12 ...\n##  $ time    : int  400 4500 1012 1925 2503 2466 2400 51 3762 304 ...\n##  $ status  : int  2 0 2 2 2 2 2 2 2 2 ...\n##  $ trt     : int  1 1 1 1 2 2 1 2 2 2 ...\n##  $ age     : num  58.8 56.4 70.1 54.7 66.3 ...\n##  $ sex     : Factor w/ 2 levels \"m\",\"f\": 2 2 1 2 2 2 2 2 2 2 ...\n##  $ ascites : int  1 0 0 0 0 0 0 1 0 0 ...\n##  $ hepato  : int  1 1 0 1 1 0 0 0 1 0 ...\n##  $ spiders : int  1 1 0 1 0 0 1 1 1 1 ...\n##  $ edema   : num  1 0 0.5 0.5 0 0 0 1 0 0 ...\n##  $ bili    : num  14.5 1.1 1.4 1.8 0.8 0.3 3.2 12.6 1.4 3.6 ...\n##  $ chol    : int  261 302 176 244 248 280 562 200 259 236 ...\n##  $ albumin : num  2.6 4.14 3.48 2.54 3.98 4 3.08 2.74 4.16 3.52 ...\n##  $ copper  : int  156 54 210 64 50 52 79 140 46 94 ...\n##  $ alk.phos: num  1718 7395 516 6122 944 ...\n##  $ ast     : num  137.9 113.5 96.1 60.6 93 ...\n##  $ trig    : int  172 88 55 92 63 189 88 143 79 95 ...\n##  $ platelet: int  190 221 151 183 NA 373 251 302 258 71 ...\n##  $ protime : num  12.2 10.6 12 10.3 11 11 11 11.5 12 13.6 ...\n##  $ stage   : int  4 3 4 4 3 3 2 4 4 4 ...\ndim(dat) # 232 20\n## [1] 232  20\n\n然后就是准备计算NRI所需要的各个参数。\n\n# 定义结局事件，0是存活，1是死亡\nevent = ifelse(dat$time &lt; 2000 & dat$status == 2, 1, 0)\n\n# 两个只由预测变量组成的矩阵\nz.std = as.matrix(subset(dat, select = c(age, bili, albumin)))\nz.new = as.matrix(subset(dat, select = c(age, bili, albumin, protime)))\n\n# 建立2个模型\nmstd = glm(event ~ age + bili + albumin, family = binomial(), data = dat, x=TRUE)\nmnew = glm(event ~ age + bili + albumin + protime, family = binomial(), data = dat, x=TRUE)\n\n# 取出模型预测概率\np.std = mstd$fitted.values\np.new = mnew$fitted.values\n\n然后就是计算NRI，对于二分类变量，使用nribin()函数，这个函数提供了3种参数使用组合，任选一种都可以计算出来（结果一样），以下3组参数任选1组即可。 mdl.std, mdl.new 或者 event, z.std, z.new 或者 event, p.std, p.new。\n\n# 这3种方法算出来都是一样的结果\n\n# 两个模型\nnribin(mdl.std = mstd, mdl.new = mnew, \n       cut = c(0.3,0.7), \n       niter = 500, \n       updown = 'category')\n\n# 结果变量 + 两个只有预测变量的矩阵\nnribin(event = event, z.std = z.std, z.new = z.new, \n       cut = c(0.3,0.7), \n       niter = 500, \n       updown = 'category')\n\n## 结果变量 + 两个模型得到的预测概率\nnribin(event = event, p.std = p.std, p.new = p.new, \n       cut = c(0.3,0.7), \n       niter = 500, \n       updown = 'category')\n\n其中，cut是判断风险高低的阈值，我们使用了0.3,0.7，代表0-0.3是低风险，0.3-0.7是中风险，0.7-1是高风险，这个阈值是自己设置的，大家根据经验或者文献设置即可。\nniter是使用bootstrap法进行重抽样的次数，默认是1000，大家可以自己设置。\nupdown参数，当设置为category时，表示低、中、高风险这种方式；当设置为diff时，此时cut的取值只能设置1个，比如设置0.2，即表示当新模型预测的风险和旧模型相差20%时，认为是重新分类。\n上面的代码运行后结果是这样的：\n\nUP and DOWN calculation:\n  #of total, case, and control subjects at t0:  232 88 144\n\n  Reclassification Table for all subjects:\n        New\nStandard &lt; 0.3 &lt; 0.7 &gt;= 0.7\n  &lt; 0.3    135     4      0\n  &lt; 0.7      1    31      4\n  &gt;= 0.7     0     2     55\n\n  Reclassification Table for case:\n        New\nStandard &lt; 0.3 &lt; 0.7 &gt;= 0.7\n  &lt; 0.3     14     0      0\n  &lt; 0.7      0    18      3\n  &gt;= 0.7     0     1     52\n\n  Reclassification Table for control:\n        New\nStandard &lt; 0.3 &lt; 0.7 &gt;= 0.7\n  &lt; 0.3    121     4      0\n  &lt; 0.7      1    13      1\n  &gt;= 0.7     0     1      3\n\nNRI estimation:\nPoint estimates:\n                  Estimate\nNRI            0.001893939\nNRI+           0.022727273\nNRI-          -0.020833333\nPr(Up|Case)    0.034090909\nPr(Down|Case)  0.011363636\nPr(Down|Ctrl)  0.013888889\nPr(Up|Ctrl)    0.034722222\n\nNow in bootstrap..\n\nPoint & Interval estimates:\n                  Estimate   Std.Error        Lower       Upper\nNRI            0.001893939 0.027816095 -0.053995513 0.055354449\nNRI+           0.022727273 0.021564394 -0.019801980 0.065789474\nNRI-          -0.020833333 0.017312438 -0.058823529 0.007518797\nPr(Up|Case)    0.034090909 0.019007629  0.000000000 0.072164948\nPr(Down|Case)  0.011363636 0.010924271  0.000000000 0.039603960\nPr(Down|Ctrl)  0.013888889 0.009334685  0.000000000 0.035211268\nPr(Up|Ctrl)    0.034722222 0.014716046  0.006993007 0.066176471\n\n首先是3个混淆矩阵，第一个是全体的，第2个是case（结局为1）组的，第3个是control（结局为2）组的，有了这3个矩阵，我们可以自己计算净重分类指数。\n看case组：\n净重分类指数 = ((0+3)-(0+1)) / 88 ≈ 0.022727273\n再看control组：\n净重分类指数 = ((1+1)-(4+1)) / 144 ≈ -0.020833333\n相加净重分类指数 = case组净重分类指数 + control组净重分类指数 = 2/88 - 3/144 ≈ 0.000315657\n再往下是不做bootstrap时得到的估计值，其中NRI就是绝对净重分类指数，NRI+是case组的净重分类指数，NRI-是control组的净重分类指数（和我们计算的一样哦），最后是做了500次bootstrap后得到的估计值，并且有标准误和可信区间。\n最后还会得到一张图：\n\n这张图中的虚线对应的坐标，就是我们在cut中设置的阈值，这张图对应的是上面结果中的第一个混淆矩阵，反应的是总体的情况，case是结果为1的组，也就是发生结局的组，control是结果为0的组，也就是未发生结局的组。\nP值没有直接给出，但是可以自己计算。\n\n# 计算P值\nz &lt;- abs(0.001893939/0.027816095)\np &lt;- (1 - pnorm(z))*2\np\n## [1] 0.9457157\n\n\n\n29.1.2 PredictABEL包\n\n#install.packages(\"PredictABEL\") #安装R包\nlibrary(PredictABEL)  \n\n# 取出模型预测概率，这个包只能用预测概率计算\np.std = mstd$fitted.values\np.new = mnew$fitted.values \n\n然后就是计算NRI：\n\ndat$event &lt;- event\n\nreclassification(data = dat,\n                 cOutcome = 21, # 结果变量在哪一列\n                 predrisk1 = p.std,\n                 predrisk2 = p.new,\n                 cutoff = c(0,0.3,0.7,1)\n                 )\n##  _________________________________________\n##  \n##      Reclassification table    \n##  _________________________________________\n## \n##  Outcome: absent \n##   \n##              Updated Model\n## Initial Model [0,0.3) [0.3,0.7) [0.7,1]  % reclassified\n##     [0,0.3)       121         4       0               3\n##     [0.3,0.7)       1        13       1              13\n##     [0.7,1]         0         1       3              25\n## \n##  \n##  Outcome: present \n##   \n##              Updated Model\n## Initial Model [0,0.3) [0.3,0.7) [0.7,1]  % reclassified\n##     [0,0.3)        14         0       0               0\n##     [0.3,0.7)       0        18       3              14\n##     [0.7,1]         0         1      52               2\n## \n##  \n##  Combined Data \n##   \n##              Updated Model\n## Initial Model [0,0.3) [0.3,0.7) [0.7,1]  % reclassified\n##     [0,0.3)       135         4       0               3\n##     [0.3,0.7)       1        31       4              14\n##     [0.7,1]         0         2      55               4\n##  _________________________________________\n## \n##  NRI(Categorical) [95% CI]: 0.0019 [ -0.0551 - 0.0589 ] ; p-value: 0.94806 \n##  NRI(Continuous) [95% CI]: 0.0391 [ -0.2238 - 0.3021 ] ; p-value: 0.77048 \n##  IDI [95% CI]: 0.0044 [ -0.0037 - 0.0126 ] ; p-value: 0.28396\n\n结果得到的是相加净重分类指数，还给出了IDI和P值。两个包算是各有优劣吧，大家可以自由选择。"
  },
  {
    "objectID": "nri.html#生存资料的nri",
    "href": "nri.html#生存资料的nri",
    "title": "29  NRI净重新分类指数",
    "section": "29.2 生存资料的NRI",
    "text": "29.2 生存资料的NRI\n还是使用survival包中的pbc数据集用于演示，这次要构建cox回归模型，因此我们要使用time这一列了。\n\n29.2.1 nricens包\n\nlibrary(nricens)\nlibrary(survival)\n\ndat &lt;- pbc[1:312,]\ndat$status &lt;- ifelse(dat$status==2, 1, 0) # 0表示活着，1表示死亡\n\n然后准备所需参数：\n\n# 两个只由预测变量组成的矩阵\nz.std = as.matrix(subset(dat, select = c(age, bili, albumin)))\nz.new = as.matrix(subset(dat, select = c(age, bili, albumin, protime)))\n\n# 建立2个cox模型\nmstd &lt;- coxph(Surv(time,status) ~ age + bili + albumin, data = dat, x=TRUE)\nmnew &lt;- coxph(Surv(time,status) ~ age + bili + albumin + protime, data = dat, x=TRUE)\n\n# 计算在2000天的模型预测概率，这一步不要也行，看你使用哪些参数\np.std &lt;- get.risk.coxph(mstd, t0=2000)\np.new &lt;- get.risk.coxph(mnew, t0=2000)\n\n计算NRI：\n\nnricens(mdl.std= mstd, mdl.new = mnew, \n        t0 = 2000, \n        cut = c(0.3, 0.7),\n        niter = 1000, \n        updown = 'category')\n\nUP and DOWN calculation:\n  #of total, case, and control subjects at t0:  312 88 144\n\n  Reclassification Table for all subjects:\n        New\nStandard &lt; 0.3 &lt; 0.7 &gt;= 0.7\n  &lt; 0.3    202     7      0\n  &lt; 0.7     13    53      6\n  &gt;= 0.7     0     0     31\n\n  Reclassification Table for case:\n        New\nStandard &lt; 0.3 &lt; 0.7 &gt;= 0.7\n  &lt; 0.3     19     3      0\n  &lt; 0.7      3    32      4\n  &gt;= 0.7     0     0     27\n\n  Reclassification Table for control:\n        New\nStandard &lt; 0.3 &lt; 0.7 &gt;= 0.7\n  &lt; 0.3    126     3      0\n  &lt; 0.7      5     7      2\n  &gt;= 0.7     0     0      1\n\nNRI estimation by KM estimator:\n\nPoint estimates:\n                Estimate\nNRI           0.05377635\nNRI+          0.03748660\nNRI-          0.01628974\nPr(Up|Case)   0.07708938\nPr(Down|Case) 0.03960278\nPr(Down|Ctrl) 0.04256352\nPr(Up|Ctrl)   0.02627378\n\nNow in bootstrap..\n\nPoint & Interval estimates:\n                Estimate        Lower      Upper\nNRI           0.05377635 -0.082230381 0.16058172\nNRI+          0.03748660 -0.084245197 0.13231776\nNRI-          0.01628974 -0.030861213 0.06753616\nPr(Up|Case)   0.07708938  0.000000000 0.19102291\nPr(Down|Case) 0.03960278  0.000000000 0.15236016\nPr(Down|Ctrl) 0.04256352  0.004671535 0.09863170\nPr(Up|Ctrl)   0.02627378  0.006400463 0.05998424\n\n\n结果的解读和logistic的一模一样。\n\n\n29.2.2 survNRI包\n\n# 安装R包\ndevtools::install_github(\"mdbrown/survNRI\")\n\n加载R包并使用，还是用上面的pbc数据集。\n\nlibrary(survNRI)\n## Loading required package: MASS\nlibrary(survival)\n\n# 使用部分数据\ndat &lt;- pbc[1:312,]\ndat$status &lt;- ifelse(dat$status==2, 1, 0) # 0表示活着，1表示死亡\n\nres &lt;- survNRI(time  = \"time\", event = \"status\", \n        model1 = c(\"age\", \"bili\", \"albumin\"), # 模型1的自变量\n        model2 = c(\"age\", \"bili\", \"albumin\", \"protime\"), # 模型2的自变量\n        data = dat, \n        predict.time = 2000, # 预测的时间点\n        method = \"all\", \n        bootMethod = \"normal\",  \n        bootstraps = 500, \n        alpha = .05)\n\n查看结果，$estimates给出了不同组的NRI以及总的NRI，包括了使用不同方法（KM/IPW/SmoothIPW/SEM/Combined）得到的结果；$CI给出了可信区间。\n\nprint.survNRI(res)\n## \n##  Net Reclassification Improvement at time t = 2000\n##   with 95% bootstrap confidence intervals based on normal approximation.\n## \n##  method     |  event NRI              non-event NRI             NRI \n## -------------------------------------------------------------------------------\n##   KM        |  0.204 (-0.024,0.433)   0.319 (0.122,0.720)   0.523 (-0.046,0.888)   \n##   IPW       |  0.224 (-0.006,0.454)   0.327 (0.129,0.750)   0.552 (-0.042,0.921)   \n##   SmoothIPW |  0.196 (-0.031,0.424)   0.314 (0.118,0.707)   0.511 (-0.050,0.875)   \n##   SEM       |  0.075 (-0.117,0.266)   0.263 (0.080,0.521)   0.338 (-0.044,0.646)   \n##   Combined  |  0.196 (-0.032,0.424)   0.314 (0.118,0.707)   0.511 (-0.050,0.875)   \n## -------------------------------------------------------------------------------\n\nOK，这就是NRI的计算，除此之外，随机森林、决策树、lasso回归、SVM等，这些模型，都是可以计算的NRI的，大家可以关注公众号查看相关内容。"
  },
  {
    "objectID": "idi.html#logistic的idi",
    "href": "idi.html#logistic的idi",
    "title": "30  IDI综合判别改善指数",
    "section": "30.1 logistic的IDI",
    "text": "30.1 logistic的IDI\n二分类变量的NRI和IDI计算使用PredictABEL包。\n使用survival包中的pbc数据集用于演示，这是一份关于原发性硬化性胆管炎的数据，其实是一份用于生存分析的数据，是有时间变量的，但是这里我们用于演示logistic回归，只要不使用time这一列就可以了。\n\nlibrary(survival)\n\n# 只使用部分数据\ndat = pbc[1:312,] \ndat = dat[ dat$time &gt; 2000 | (dat$time &lt; 2000 & dat$status == 2), ]\n\nstr(dat) # 数据长这样\n## 'data.frame':    232 obs. of  20 variables:\n##  $ id      : int  1 2 3 4 6 8 9 10 11 12 ...\n##  $ time    : int  400 4500 1012 1925 2503 2466 2400 51 3762 304 ...\n##  $ status  : int  2 0 2 2 2 2 2 2 2 2 ...\n##  $ trt     : int  1 1 1 1 2 2 1 2 2 2 ...\n##  $ age     : num  58.8 56.4 70.1 54.7 66.3 ...\n##  $ sex     : Factor w/ 2 levels \"m\",\"f\": 2 2 1 2 2 2 2 2 2 2 ...\n##  $ ascites : int  1 0 0 0 0 0 0 1 0 0 ...\n##  $ hepato  : int  1 1 0 1 1 0 0 0 1 0 ...\n##  $ spiders : int  1 1 0 1 0 0 1 1 1 1 ...\n##  $ edema   : num  1 0 0.5 0.5 0 0 0 1 0 0 ...\n##  $ bili    : num  14.5 1.1 1.4 1.8 0.8 0.3 3.2 12.6 1.4 3.6 ...\n##  $ chol    : int  261 302 176 244 248 280 562 200 259 236 ...\n##  $ albumin : num  2.6 4.14 3.48 2.54 3.98 4 3.08 2.74 4.16 3.52 ...\n##  $ copper  : int  156 54 210 64 50 52 79 140 46 94 ...\n##  $ alk.phos: num  1718 7395 516 6122 944 ...\n##  $ ast     : num  137.9 113.5 96.1 60.6 93 ...\n##  $ trig    : int  172 88 55 92 63 189 88 143 79 95 ...\n##  $ platelet: int  190 221 151 183 NA 373 251 302 258 71 ...\n##  $ protime : num  12.2 10.6 12 10.3 11 11 11 11.5 12 13.6 ...\n##  $ stage   : int  4 3 4 4 3 3 2 4 4 4 ...\ndim(dat) # 232 20\n## [1] 232  20\n\n然后就是准备计算IDI所需要的各个参数。\n\n# 定义结局事件，0是存活，1是死亡\nevent = ifelse(dat$time &lt; 2000 & dat$status == 2, 1, 0)\n\n# 建立2个模型\nmstd = glm(event ~ age + bili + albumin, family = binomial(), data = dat, x=TRUE)\nmnew = glm(event ~ age + bili + albumin + protime, family = binomial(), data = dat, x=TRUE)\n\n# 取出模型预测概率\np.std = mstd$fitted.values\np.new = mnew$fitted.values\n\n接下来就是使用PredictABEL计算IDI：\n\n#install.packages(\"PredictABEL\") #安装R包\nlibrary(PredictABEL)  \n\ndat$event &lt;- event\n\nreclassification(data = dat,\n                 cOutcome = 21, # 结果变量在哪一列\n                 predrisk1 = p.std,\n                 predrisk2 = p.new,\n                 cutoff = c(0,0.3,0.7,1)\n                 )\n##  _________________________________________\n##  \n##      Reclassification table    \n##  _________________________________________\n## \n##  Outcome: absent \n##   \n##              Updated Model\n## Initial Model [0,0.3) [0.3,0.7) [0.7,1]  % reclassified\n##     [0,0.3)       121         4       0               3\n##     [0.3,0.7)       1        13       1              13\n##     [0.7,1]         0         1       3              25\n## \n##  \n##  Outcome: present \n##   \n##              Updated Model\n## Initial Model [0,0.3) [0.3,0.7) [0.7,1]  % reclassified\n##     [0,0.3)        14         0       0               0\n##     [0.3,0.7)       0        18       3              14\n##     [0.7,1]         0         1      52               2\n## \n##  \n##  Combined Data \n##   \n##              Updated Model\n## Initial Model [0,0.3) [0.3,0.7) [0.7,1]  % reclassified\n##     [0,0.3)       135         4       0               3\n##     [0.3,0.7)       1        31       4              14\n##     [0.7,1]         0         2      55               4\n##  _________________________________________\n## \n##  NRI(Categorical) [95% CI]: 0.0019 [ -0.0551 - 0.0589 ] ; p-value: 0.94806 \n##  NRI(Continuous) [95% CI]: 0.0391 [ -0.2238 - 0.3021 ] ; p-value: 0.77048 \n##  IDI [95% CI]: 0.0044 [ -0.0037 - 0.0126 ] ; p-value: 0.28396\n\nIDI在最后一行，同时给出了95%的可信区间和P值；还给出了NRI和P值。重分类表中，absent是event=0的结果，present是event=1的结果。"
  },
  {
    "objectID": "idi.html#生存资料的idi",
    "href": "idi.html#生存资料的idi",
    "title": "30  IDI综合判别改善指数",
    "section": "30.2 生存资料的IDI",
    "text": "30.2 生存资料的IDI\n生存资料的IDI计算使用survIDINRI包计算。\n\n# 安装R包\ninstall.packages(\"survIDINRI\")\n\n加载R包并使用，还是用上面的pbc数据集。\n\n#library(survNRI)\nlibrary(survIDINRI)\n## Loading required package: survC1\n\n# 使用部分数据\ndat &lt;- pbc[1:312,]\ndat$status &lt;- ifelse(dat$status==2, 1, 0) # 0表示活着，1表示死亡\n\nstr(dat)\n## 'data.frame':    312 obs. of  20 variables:\n##  $ id      : int  1 2 3 4 5 6 7 8 9 10 ...\n##  $ time    : int  400 4500 1012 1925 1504 2503 1832 2466 2400 51 ...\n##  $ status  : num  1 0 1 1 0 1 0 1 1 1 ...\n##  $ trt     : int  1 1 1 1 2 2 2 2 1 2 ...\n##  $ age     : num  58.8 56.4 70.1 54.7 38.1 ...\n##  $ sex     : Factor w/ 2 levels \"m\",\"f\": 2 2 1 2 2 2 2 2 2 2 ...\n##  $ ascites : int  1 0 0 0 0 0 0 0 0 1 ...\n##  $ hepato  : int  1 1 0 1 1 1 1 0 0 0 ...\n##  $ spiders : int  1 1 0 1 1 0 0 0 1 1 ...\n##  $ edema   : num  1 0 0.5 0.5 0 0 0 0 0 1 ...\n##  $ bili    : num  14.5 1.1 1.4 1.8 3.4 0.8 1 0.3 3.2 12.6 ...\n##  $ chol    : int  261 302 176 244 279 248 322 280 562 200 ...\n##  $ albumin : num  2.6 4.14 3.48 2.54 3.53 3.98 4.09 4 3.08 2.74 ...\n##  $ copper  : int  156 54 210 64 143 50 52 52 79 140 ...\n##  $ alk.phos: num  1718 7395 516 6122 671 ...\n##  $ ast     : num  137.9 113.5 96.1 60.6 113.2 ...\n##  $ trig    : int  172 88 55 92 72 63 213 189 88 143 ...\n##  $ platelet: int  190 221 151 183 136 NA 204 373 251 302 ...\n##  $ protime : num  12.2 10.6 12 10.3 10.9 11 9.7 11 11 11.5 ...\n##  $ stage   : int  4 3 4 4 3 3 3 3 2 4 ...\n\n构建参数需要的值：\n\n# 两个只由预测变量组成的矩阵\nz.std = as.matrix(subset(dat, select = c(age, bili, albumin)))\nz.new = as.matrix(subset(dat, select = c(age, bili, albumin, protime)))\n\n然后使用IDI.INF()函数计算IDI：\n\nres &lt;- IDI.INF(indata = dat[,c(2,3)],\n               covs0 = z.std,\n               covs1 = z.new,\n               t0 = 2000, # 时间点\n               npert = 500 # 重抽样次数\n               )\n\nIDI.INF.OUT(res) # 提取结果\n##     Est.  Lower Upper p-value\n## M1 0.020 -0.002 0.056   0.076\n## M2 0.202 -0.028 0.393   0.076\n## M3 0.011 -0.001 0.035   0.068\n\n\nm1：IDI的值，可信区间，P值\nm2：NRI的值，可信区间，P值\nm3：Median improvement in risk score，可信区间，p值。\n\n以上就是IDI的计算方法。\n除此之外，随机森林、决策树、lasso回归等也是可以计算IDI的，大家可以关注公众号查看相关内容。"
  },
  {
    "objectID": "calibration-logistic.html#加载数据",
    "href": "calibration-logistic.html#加载数据",
    "title": "31  logistic回归校准曲线绘制",
    "section": "31.1 加载数据",
    "text": "31.1 加载数据\n使用lowbirth数据集，这个数据集是关于低出生体重儿是否会死亡的数据集，其中dead这一列是结果变量，0代表死亡，1代表存活，其余列都是预测变量。\n\n注意：需要把分类变量因子化，对于无序分类变量，需要设置哑变量！\n\n\nrm(list = ls())\nlowbirth &lt;- read.csv(\"./datasets/lowbirth.csv\")\n\n查看一下数据：\n\ndim(lowbirth) # 565行，10列\n## [1] 565  10\nstr(lowbirth) \n## 'data.frame':    565 obs. of  10 variables:\n##  $ birth   : num  81.5 81.6 81.6 81.6 81.6 ...\n##  $ lowph   : num  7.25 7.06 7.25 6.97 7.32 ...\n##  $ pltct   : int  244 114 182 54 282 153 229 182 361 378 ...\n##  $ race    : chr  \"white\" \"black\" \"black\" \"black\" ...\n##  $ bwt     : int  1370 620 1480 925 1255 1350 1310 1110 1180 970 ...\n##  $ delivery: chr  \"abdominal\" \"vaginal\" \"vaginal\" \"abdominal\" ...\n##  $ apg1    : int  7 1 8 5 9 4 6 6 6 2 ...\n##  $ vent    : int  0 1 0 1 0 0 1 0 0 1 ...\n##  $ sex     : chr  \"female\" \"female\" \"male\" \"female\" ...\n##  $ dead    : int  0 1 0 1 0 0 0 0 0 1 ...\n\n简单的把人种分为白色和黑色人种（无序分类变量需要设置哑变量），再去掉race这一列，然后其余分类变量因子化。\n\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n\ntmp &lt;- lowbirth %&gt;% \n  mutate(across(where(is.character),as.factor),\n         vent = factor(vent),\n         black = ifelse(race == \"black\",1,0),\n         white = ifelse(race == \"white\",1,0),\n         other = ifelse(race %in% c(\"native American\",\"oriental\"),1,0)\n         ) %&gt;% \n  select(- race)\n\nglimpse(tmp)\n## Rows: 565\n## Columns: 12\n## $ birth    &lt;dbl&gt; 81.514, 81.552, 81.558, 81.593, 81.610, 81.624, 81.626, 81.68…\n## $ lowph    &lt;dbl&gt; 7.250000, 7.059998, 7.250000, 6.969997, 7.320000, 7.160000, 7…\n## $ pltct    &lt;int&gt; 244, 114, 182, 54, 282, 153, 229, 182, 361, 378, 255, 186, 26…\n## $ bwt      &lt;int&gt; 1370, 620, 1480, 925, 1255, 1350, 1310, 1110, 1180, 970, 770,…\n## $ delivery &lt;fct&gt; abdominal, vaginal, vaginal, abdominal, vaginal, abdominal, v…\n## $ apg1     &lt;int&gt; 7, 1, 8, 5, 9, 4, 6, 6, 6, 2, 4, 8, 1, 8, 5, 9, 9, 9, 6, 2, 1…\n## $ vent     &lt;fct&gt; 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1…\n## $ sex      &lt;fct&gt; female, female, male, female, female, female, male, male, mal…\n## $ dead     &lt;int&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…\n## $ black    &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0…\n## $ white    &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1…\n## $ other    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "calibration-logistic.html#方法1rms",
    "href": "calibration-logistic.html#方法1rms",
    "title": "31  logistic回归校准曲线绘制",
    "section": "31.2 方法1：rms",
    "text": "31.2 方法1：rms\n使用rms包。\n\nlibrary(rms)\n## Loading required package: Hmisc\n## \n## Attaching package: 'Hmisc'\n## The following objects are masked from 'package:dplyr':\n## \n##     src, summarize\n## The following objects are masked from 'package:base':\n## \n##     format.pval, units\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\ndd &lt;- datadist(tmp)\noptions(datadist=\"dd\")\n\nfit2 &lt;- lrm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp,x=T,y=T)\n\ncal2 &lt;- calibrate(fit2, method='boot', B=500)\n\n接下来就是画图：\n\nplot(cal2,\n     xlim = c(0,1),\n     ylim = c(0,1),\n     xlab = \"Prediced Probability\",\n     ylab = \"Observed Probability\",\n     cex.lab=1.2, cex.axis=1, cex.main=1.2, cex.sub=0.8,\n     #subtitles = FALSE,\n     legend = FALSE\n     ) \n## \n## n=565   Mean absolute error=0.012   Mean squared error=0.00029\n## 0.9 Quantile of absolute error=0.032\nlines(cal2[,c(\"predy\",\"calibrated.corrected\")], \n      type = 'l', #连线的类型，可以是\"p\",\"b\",\"o\"\n      lwd = 3, #连线的粗细\n      pch = 16, #点的形状，可以是0-20\n      col = \"#2166AC\") #连线的颜色\nlines(cal2[,c(\"predy\",\"calibrated.orig\")],type=\"l\",pch=16,lwd=3,col=\"tomato\")\nabline(0,1,\n       lty = 2, #对角线为虚线\n       lwd = 2, #对角线的粗细\n       col = \"#224444\") #对角线的颜色\nlegend(0.6,0.2,\n       c(\"Apparent\",\"Bias-corrected\",\"Ideal\"), \n       lty = c(2,1,1), \n       lwd = c(2,3,3), \n       col = c(\"black\",\"#2166AC\",\"tomato\"), \n       bty = \"n\"\n)\n\n\n\n\n还可以计算hosmer-lemeshow的P值。\n\n# 进行hosmer-lemeshow 检验\nlibrary(ResourceSelection)\n## ResourceSelection 0.3-6   2023-06-27\n\nmodel_glm &lt;- glm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp, family = binomial)\n\n# hosmer-lemeshow 检验\np.hoslem &lt;- hoslem.test(model_glm$y, fitted(model_glm), g=10)$p.value\np.hoslem\n## [1] 0.2340365\n\n# 构建 calibration\nfit2 &lt;- lrm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp,x=T,y=T)\n\ncal2 &lt;- calibrate(fit2, method='boot', B=500)\n\n画图还是和上面一样，就是多了一个添加 hosmer-lemeshow P值的步骤。\n\nplot(cal2,\n     xlim = c(0,1),\n     ylim = c(0,1),\n     xlab = \"Prediced Probability\",\n     ylab = \"Observed Probability\",\n     cex.lab=1.2, cex.axis=1, cex.main=1.2, cex.sub=0.8,\n     #subtitles = FALSE,\n     legend = FALSE\n     ) \n## \n## n=565   Mean absolute error=0.012   Mean squared error=0.00028\n## 0.9 Quantile of absolute error=0.032\nlines(cal2[,c(\"predy\",\"calibrated.corrected\")], \n      type = 'l', #连线的类型，可以是\"p\",\"b\",\"o\"\n      lwd = 3, #连线的粗细\n      pch = 16, #点的形状，可以是0-20\n      col = \"#2166AC\") #连线的颜色\nlines(cal2[,c(\"predy\",\"calibrated.orig\")],type=\"l\",pch=16,lwd=3,col=\"tomato\")\nabline(0,1,\n       lty = 2, #对角线为虚线\n       lwd = 2, #对角线的粗细\n       col = \"#224444\")#对角线的颜色\nlegend(0.6,0.2,\n       c(\"Apparent\",\"Bias-corrected\",\"Ideal\"), \n       lty = c(2,1,1), \n       lwd = c(2,3,3), \n       col = c(\"black\",\"#2166AC\",\"tomato\"), \n       bty = \"n\"\n)\ntext(0,0,bquote(\"Hosmer-Lemeshow \"~italic(P)~\" = \"~.(round(p.hoslem,3))),adj = 0)"
  },
  {
    "objectID": "calibration-logistic.html#方法2riskregression",
    "href": "calibration-logistic.html#方法2riskregression",
    "title": "31  logistic回归校准曲线绘制",
    "section": "31.3 方法2：riskRegression",
    "text": "31.3 方法2：riskRegression\n使用riskRegression包。\n\nfit2 &lt;- glm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp, family = binomial)\n\n\nlibrary(riskRegression)\n## riskRegression version 2023.09.08\n\nfit22 &lt;- Score(list(\"fit\"=fit2),\n               formula = dead ~ 1,\n               data = tmp,\n               metrics = c(\"auc\",\"brier\"),\n               summary = c(\"risks\",\"IPA\",\"riskQuantile\",\"ibs\"),\n               plots = \"calibration\",\n               null.model = T,\n               conf.int = T,\n               B = 500,\n               M = 50\n               )\nplotCalibration(fit22,col=\"tomato\",\n                xlab = \"Predicted Risk\",\n                ylab = \"Observerd RISK\",\n                bars = F)\n\n\n\n\n非常神奇的是，还可以用ggplot2来画！\n\nplotdata &lt;- plotCalibration(fit22,plot = F,method = \"nne\"\n                            #bandwidth = 0.1\n                            )\n\nlibrary(ggplot2)\n\nggplot(plotdata$plotFrames$fit, aes(x=Pred,y=Obs))+\n  geom_line(color=\"tomato\",linewidth=1.5)+\n  scale_x_continuous(limits = c(0,1),name = \"Predicted Risk\")+\n  scale_y_continuous(limits = c(0,1),name = \"Observerd Risk\")+\n  geom_abline(slope = 1,intercept = 0,lty=2)+\n  geom_rug(color=\"grey\")+\n  theme_bw()"
  },
  {
    "objectID": "calibration-logistic.html#多个calibration画在一起",
    "href": "calibration-logistic.html#多个calibration画在一起",
    "title": "31  logistic回归校准曲线绘制",
    "section": "31.4 多个calibration画在一起",
    "text": "31.4 多个calibration画在一起\n\n31.4.1 方法1\n首先构建多个画图需要的结果：\n\nfit2 &lt;- lrm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp,x=T,y=T)\ncal2 &lt;- calibrate(fit2, method='boot', B=500)\n\nfit1 &lt;- lrm(dead ~ lowph + bwt + vent + black,\n            data = tmp,x=T,y=T)\ncal1 &lt;- calibrate(fit1, method='boot', B=500)\n\nfit3 &lt;- lrm(dead ~ birth + lowph + pltct + bwt + vent,\n            data = tmp,x=T,y=T)\ncal3 &lt;- calibrate(fit3, method='boot', B=500)\n\n然后使用基础函数画在一起即可：\n\nplot(1,\n     type = \"n\",\n     xlim = c(0,1),\n     ylim = c(0,1),\n     xlab = \"Nomogram-prediced Probability\",\n     ylab = \"Observed Probability\",\n     cex.lab=1.2, cex.axis=1, cex.main=1.2, cex.sub=0.8\n)\nlines(cal1[,c(\"predy\",\"calibrated.corrected\")], \n      type = 'l', #连线的类型，可以是\"p\",\"b\",\"o\"\n      lwd = 3, #连线的粗细\n      pch = 16, #点的形状，可以是0-20\n      col = \"#2166AC\") #连线的颜色\nlines(cal2[,c(\"predy\",\"calibrated.corrected\")],type=\"l\",pch=16,lwd=3,col=\"tomato\")\nlines(cal3[,c(\"predy\",\"calibrated.corrected\")],type=\"l\",pch=16,lwd=3,col=\"skyblue\")\nabline(0,1,\n       lty = 2, #对角线为虚线\n       lwd = 2, #对角线的粗细\n       col = \"#224444\")#对角线的颜色\nlegend(0.6,0.2,\n       c(\"fit1\",\"fit2\",\"fit3\"), \n       lty = c(1,1,1), \n       lwd = c(3,3,3), \n       col = c(\"#2166AC\",\"tomato\",\"skyblue\"), \n       bty = \"n\"\n)\n\n\n\n\n\n\n31.4.2 方法2\n\nlibrary(riskRegression)\n\nfit1 &lt;- glm(dead ~ lowph + bwt + vent + black,data = tmp,family=binomial)\nfit2 &lt;- glm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = tmp, family = binomial)\nfit3 &lt;- glm(dead ~ birth + lowph + pltct + bwt + vent,data=tmp,\n            family=binomial)\n\nfit_all &lt;- Score(list(\"Fit1\"=fit1,\n                      \"Fit2\"=fit2,\n                      \"Fit3\"=fit3\n                      ),\n               formula = dead ~ 1,\n               data = tmp,\n               metrics = c(\"auc\",\"brier\"),\n               summary = c(\"risks\",\"IPA\",\"riskQuantile\",\"ibs\"),\n               plots = \"calibration\",\n               null.model = T,\n               conf.int = T,\n               B = 500,\n               M = 50\n               )\n\ndata_all &lt;- plotCalibration(fit_all,plot = F)\n\n接下来数据稍作变换，就可以画图了！\n\nplot_df &lt;- bind_rows(data_all$plotFrames) %&gt;% \n  mutate(fits = rep(c(\"fit1\",\"fit2\",\"fit3\"),each=76))\n\nggplot(plot_df, aes(Pred,Obs))+\n  geom_line(aes(group=fits,color=fits),linewidth=1.2)+\n  scale_color_manual(values = c(\"#2166AC\",\"tomato\",\"skyblue\"),name=NULL)+\n  scale_x_continuous(limits = c(0,1),name = \"Predicted Risk\")+\n  scale_y_continuous(limits = c(0,1),name = \"Observerd Risk\")+\n  geom_abline(slope = 1,intercept = 0,lty=2)+\n  geom_rug(aes(color=fits))+\n  theme_bw()\n\n\n\n\n获取lowbirth数据请在公众号后台回复20220520。"
  },
  {
    "objectID": "calibration-logistic-test.html#准备数据",
    "href": "calibration-logistic-test.html#准备数据",
    "title": "32  logistic回归测试集校准曲线的绘制",
    "section": "32.1 准备数据",
    "text": "32.1 准备数据\n数据来自于这篇推文：logistic回归校准曲线的绘制，数据获取方法也在上面的推文中给出了。\n\nlowbirth &lt;- read.csv(\"./datasets/lowbirth.csv\")\nlowbirth$black = ifelse(lowbirth$race == \"black\",1,0)\nlowbirth$white = ifelse(lowbirth$race == \"white\",1,0)\nlowbirth$other = ifelse(lowbirth$race %in% c(\"native American\",\"oriental\"),1,0)\nlowbirth$delivery = factor(lowbirth$delivery)\nlowbirth$sex &lt;- factor(lowbirth$sex)\nlowbirth$race &lt;- NULL\nstr(lowbirth)\n## 'data.frame':    565 obs. of  12 variables:\n##  $ birth   : num  81.5 81.6 81.6 81.6 81.6 ...\n##  $ lowph   : num  7.25 7.06 7.25 6.97 7.32 ...\n##  $ pltct   : int  244 114 182 54 282 153 229 182 361 378 ...\n##  $ bwt     : int  1370 620 1480 925 1255 1350 1310 1110 1180 970 ...\n##  $ delivery: Factor w/ 2 levels \"abdominal\",\"vaginal\": 1 2 2 1 2 1 2 2 1 2 ...\n##  $ apg1    : int  7 1 8 5 9 4 6 6 6 2 ...\n##  $ vent    : int  0 1 0 1 0 0 1 0 0 1 ...\n##  $ sex     : Factor w/ 2 levels \"female\",\"male\": 1 1 2 1 1 1 2 2 2 1 ...\n##  $ dead    : int  0 1 0 1 0 0 0 0 0 1 ...\n##  $ black   : num  0 1 1 1 1 1 0 1 0 0 ...\n##  $ white   : num  1 0 0 0 0 0 1 0 1 1 ...\n##  $ other   : num  0 0 0 0 0 0 0 0 0 0 ..."
  },
  {
    "objectID": "calibration-logistic-test.html#数据分割",
    "href": "calibration-logistic-test.html#数据分割",
    "title": "32  logistic回归测试集校准曲线的绘制",
    "section": "32.2 数据分割",
    "text": "32.2 数据分割\n把数据随机划分为训练集、测试集，划分比例为7:3\n\nset.seed(123)\nind &lt;- sample(1:nrow(lowbirth),nrow(lowbirth)*0.7)\n\ntrain_df &lt;- lowbirth[ind,]\ntest_df &lt;- lowbirth[- ind, ]"
  },
  {
    "objectID": "calibration-logistic-test.html#训练集的校准曲线",
    "href": "calibration-logistic-test.html#训练集的校准曲线",
    "title": "32  logistic回归测试集校准曲线的绘制",
    "section": "32.3 训练集的校准曲线",
    "text": "32.3 训练集的校准曲线\n在之前的推文中这种二分类资料训练集的校准曲线给大家介绍了非常多的方法：\n\nlogistic回归校准曲线的绘制\n\n这里我们直接使用rms包实现，已在上面的推文中详细介绍过了，这里就不多解释了。\n\nlibrary(rms)\n## Loading required package: Hmisc\n## \n## Attaching package: 'Hmisc'\n## The following objects are masked from 'package:base':\n## \n##     format.pval, units\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\ndd &lt;- datadist(train_df)\noptions(datadist=\"dd\")\n\nfit1 &lt;- lrm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = train_df,x=T,y=T)\n\ncal1 &lt;- calibrate(fit1, method='boot', B=100)\n\nplot(cal1,\n     xlim = c(0,1),\n     ylim = c(0,1),\n     xlab = \"Prediced Probability\",\n     ylab = \"Observed Probability\"\n     ) \n\n\n\n## \n## n=395   Mean absolute error=0.01   Mean squared error=0.00016\n## 0.9 Quantile of absolute error=0.021"
  },
  {
    "objectID": "calibration-logistic-test.html#测试集校准曲线方法1",
    "href": "calibration-logistic-test.html#测试集校准曲线方法1",
    "title": "32  logistic回归测试集校准曲线的绘制",
    "section": "32.4 测试集校准曲线方法1",
    "text": "32.4 测试集校准曲线方法1\n测试集的校准曲线对于logistic回归很简单，任何可以计算概率的算法都可以轻松画出训练集、测试集的校准曲线，无非就是计算实际概率和预测概率而已。\n二分类资料测试集的校准曲线在之前的推文中也做过很多次介绍，比如：\n\ntidymodels不能画校准曲线？\nmlr3的校准曲线也是一样画！\ntidymodels支持校准曲线了\n\n上面是3种实现方法，其实本质是一样的，前2种是手动计算，最后一种省去了自己计算的步骤，直接给你图形，并且完美继承yardstick的用法。\n这里再给大家介绍3种方法，加上上面介绍的方法，logistic测试集的校准曲线一共给大家介绍了6种方法！\n这个方法是基于rms包的。\n\n# 首先获取测试集的预测结果\nphat &lt;- predict(fit1, test_df, type = 'fitted')\n\n# 直接使用val.prob即可实现\nval.prob(phat, test_df$dead,statloc = F,cex = 1)\n\n\n\n##         Dxy     C (ROC)          R2           D    D:Chi-sq         D:p \n##  0.74384874  0.87192437  0.42860880  0.28181902 48.90923309          NA \n##           U    U:Chi-sq         U:p           Q       Brier   Intercept \n## -0.01085663  0.15437207  0.92571762  0.29267565  0.08935692  0.12059928 \n##       Slope        Emax         E90        Eavg         S:z         S:p \n##  1.08566597  0.29112563  0.07879941  0.03183303 -0.54088957  0.58858370\n\n这个结果，你可以把它当做测试集的校准曲线用，但其实val.prob函数的真正作用是实现外部验证数据(external validation)的校准曲线，这一点在Harrell大神写的书：Regression Modeling Strategies 中写的很清楚，或者你可以看函数的帮助文档。\n所以这个方法没有给你重抽样的选择，因为作者认为外部验证是对模型最后的检验，不需要重抽样。\n你可能在文献看见过训练集和测试集的校准曲线都是上面那张图的样式，类似下面这张图展示的，训练集和测试集一样的图，实现方法也很简单。\n\n\n# 获取训练集的预测结果\nphat_train &lt;- predict(fit1, train_df, type = 'fitted')\n\n# 直接使用val.prob即可实现\nval.prob(phat_train, train_df$dead,cex = 1)\n\n\n\n##           Dxy       C (ROC)            R2             D      D:Chi-sq \n##  7.595104e-01  8.797552e-01  4.274140e-01  2.924665e-01  1.165243e+02 \n##           D:p             U      U:Chi-sq           U:p             Q \n##            NA -5.063291e-03 -8.526513e-14  1.000000e+00  2.975298e-01 \n##         Brier     Intercept         Slope          Emax           E90 \n##  9.802204e-02 -8.039794e-10  1.000000e+00  3.476792e-02  3.030223e-02 \n##          Eavg           S:z           S:p \n##  1.012113e-02 -1.990865e-01  8.421951e-01\n\n上面这张图可以用作训练集的校准曲线。"
  },
  {
    "objectID": "calibration-logistic-test.html#测试集校准曲线方法2",
    "href": "calibration-logistic-test.html#测试集校准曲线方法2",
    "title": "32  logistic回归测试集校准曲线的绘制",
    "section": "32.5 测试集校准曲线方法2",
    "text": "32.5 测试集校准曲线方法2\n如果你非要对测试集的校准曲线进行重抽样，其实也很简单(除了rms还有很多手段可实现)。\n这里还是用rms包实现。\n二分类资料的校准曲线就是计算下实际概率和预测概率就好了，基于这个原理，我们可以自己实现，方法如下：\n\n# 首先也是获取测试集的预测值\nphat &lt;- predict(fit1, test_df)\ntest_df$phat &lt;- phat # 添加到测试集中\n\n# 以预测值为自变量，结果变量为因变量，在测试集中建立逻辑回归\nfit2 &lt;- lrm(dead ~ phat, data = test_df,x=T,y=T)\n\n# 对这个逻辑回归画校准曲线即可\ncal2 &lt;- calibrate(fit2, method='boot', B=100)\n\nplot(cal2,\n     xlim = c(0,1),\n     ylim = c(0,1),\n     xlab = \"Prediced Probability\",\n     ylab = \"Observed Probability\"\n     ) \n\n\n\n## \n## n=170   Mean absolute error=0.031   Mean squared error=0.00183\n## 0.9 Quantile of absolute error=0.078\n\n这个图就是测试集的校准曲线，并且使用了bootstrap方法进行了重抽样。\n可以看到其实两张图是一样的，唯一不同是我们手动实现的方法多了重抽样100次的矫正曲线，其余就都是一样的了！"
  },
  {
    "objectID": "calibration-logistic-test.html#测试集校准曲线方法3",
    "href": "calibration-logistic-test.html#测试集校准曲线方法3",
    "title": "32  logistic回归测试集校准曲线的绘制",
    "section": "32.6 测试集校准曲线方法3",
    "text": "32.6 测试集校准曲线方法3\n使用riskRegression包。这是我推荐的方法，这个包真的好用！你可能还听说过另一个包：pec，riskRegression正是pec的升级版，目前大部分功能都已经转移到riskRegression了。\n\n# 训练集建立模型\nfit &lt;- glm(dead ~ birth + lowph + pltct + bwt + vent + black + white,\n            data = train_df, family = binomial)\n\nlibrary(riskRegression)\n## riskRegression version 2023.09.08\n\n# 测试集的表现\nscore &lt;- Score(list(\"fit\"=fit),\n               formula = dead ~ 1,\n               data = test_df, # 这里选择测试集即可\n               metrics = c(\"auc\",\"brier\"),\n               summary = c(\"risks\",\"IPA\",\"riskQuantile\",\"ibs\"),\n               plots = \"calibration\",\n               null.model = T,\n               conf.int = T,\n               B = 100,\n               M = 50\n               )\n\n# 画图即可\nplotCalibration(score,col=\"tomato\",\n                xlab = \"Predicted Risk\",\n                ylab = \"Observerd RISK\",\n                bars = F)\n\n\n\n\n并且这个方法是可以返回数据自己用ggplot2美化的，详情参考二分类资料校准曲线的绘制，这里就不多介绍了。\n\nlogistic的校准曲线真的很简单，Cox回归测试集的校准曲下次再介绍。"
  },
  {
    "objectID": "calibration-cox.html#加载r包和数据",
    "href": "calibration-cox.html#加载r包和数据",
    "title": "33  Cox回归校准曲线绘制",
    "section": "33.1 加载R包和数据",
    "text": "33.1 加载R包和数据\n\nlibrary(survival)\nlibrary(rms)\n## Loading required package: Hmisc\n## \n## Attaching package: 'Hmisc'\n## The following objects are masked from 'package:base':\n## \n##     format.pval, units\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\nlibrary(ggplot2)\n\n试试使用自带数据lung数据集进行演示。\n大多数情况下都是使用1代表死亡，0代表删失，这个数据集用2代表死亡。但有的R包会报错，需要注意！\n\nrm(list = ls())\n\ndim(lung)\n## [1] 228  10\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ...\n\n\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:Hmisc':\n## \n##     src, summarize\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\nlibrary(tidyr)\n\ndf1 &lt;- lung %&gt;% \n  mutate(status=ifelse(status == 2,1,0))\n\nstr(lung)\n## 'data.frame':    228 obs. of  10 variables:\n##  $ inst     : num  3 3 3 5 1 12 7 11 1 7 ...\n##  $ time     : num  306 455 1010 210 883 ...\n##  $ status   : num  2 2 1 2 2 1 2 2 2 2 ...\n##  $ age      : num  74 68 56 57 60 74 68 71 53 61 ...\n##  $ sex      : num  1 1 1 1 1 1 2 2 1 1 ...\n##  $ ph.ecog  : num  1 0 0 1 0 1 2 2 1 2 ...\n##  $ ph.karno : num  90 90 90 90 100 50 70 60 70 70 ...\n##  $ pat.karno: num  100 90 90 60 90 80 60 80 80 70 ...\n##  $ meal.cal : num  1175 1225 NA 1150 NA ...\n##  $ wt.loss  : num  NA 15 15 11 0 0 10 1 16 34 ..."
  },
  {
    "objectID": "calibration-cox.html#方法1rms",
    "href": "calibration-cox.html#方法1rms",
    "title": "33  Cox回归校准曲线绘制",
    "section": "33.2 方法1：rms",
    "text": "33.2 方法1：rms\n\ndd &lt;- datadist(df1)\noptions(datadist = \"dd\")\n\n构建cox比例风险模型：\n\n# 1年\ncoxfit1 &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n              data = df1, x=T,y=T,surv = T,\n              time.inc = 365 # 1 年\n              )\n\n# m=50表示每次计算50个样本，一般取4-6个点，u=365和上面的time.inc对应\ncal1 &lt;- calibrate(coxfit1, cmethod=\"KM\", method=\"boot\",u=365,m=50,B=500) \n## Using Cox survival estimates at 365 Days\n## Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts): one\n## interval had &lt; 2 observations\n\n## Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts): one\n## interval had &lt; 2 observations\n\n## Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts): one\n## interval had &lt; 2 observations\n\n## Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts): one\n## interval had &lt; 2 observations\n\n## Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts): one\n## interval had &lt; 2 observations\n\n## Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts): one\n## interval had &lt; 2 observations\n\n然后就是画图：\n\nplot(cal1,\n     lwd = 2, # 误差线粗细\n     lty = 1, # 误差线类型，可选0-6\n     errbar.col = c(\"#2166AC\"), # 误差线颜色\n     xlim = c(0,1),ylim= c(0,1),\n     xlab = \"Nomogram-prediced OS (%)\",ylab = \"Observed OS (%)\",\n     cex.lab=1.2, cex.axis=1, cex.main=1.2, cex.sub=0.6) # 字体大小\nlines(cal1[,c('mean.predicted',\"KM\")], \n      type = 'b', # 连线的类型，可以是\"p\",\"b\",\"o\"\n      lwd = 3, # 连线的粗细\n      pch = 16, # 点的形状，可以是0-20\n      col = \"tomato\") # 连线的颜色\nbox(lwd = 2) # 边框粗细\nabline(0,1,lty = 3, # 对角线为虚线\n       lwd = 2, # 对角线的粗细\n       col = \"grey70\" # 对角线的颜色\n       ) \n\n\n\n\n再介绍一下多个校正曲线图形画在一起的方法。\n\n# 2年\ncoxfit2 &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n              data = df1, x=T,y=T,surv = T,\n              time.inc = 730 # 2 年\n              )\n\ncal2 &lt;- calibrate(coxfit2, cmethod=\"KM\", method=\"boot\",u=730,m=50,B=500) \n## Using Cox survival estimates at 730 Days\n## Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts): one\n## interval had &lt; 2 observations\n\n## Warning in groupkm(cox, Surv(y[, 1], y[, 2]), u = u, cuts = orig.cuts): one\n## interval had &lt; 2 observations\n\n画图：\n\nplot(cal1,lwd = 2,lty = 1,errbar.col = c(\"#2166AC\"),\n     xlim = c(0,1),ylim= c(0,1),\n     xlab = \"Nomogram-prediced OS (%)\",ylab = \"Observed OS (%)\",\n     col = c(\"#2166AC\"),\n     cex.lab=1.2,cex.axis=1, cex.main=1.2, cex.sub=0.6)\nlines(cal1[,c('mean.predicted',\"KM\")],\n      type = 'b', lwd = 3, col = c(\"#2166AC\"), pch = 16)\n\nplot(cal2,lwd = 2,lty = 0,errbar.col = c(\"#B2182B\"),\n     xlim = c(0,1),ylim= c(0,1),col = c(\"#B2182B\"),add = T)\nlines(cal2[,c('mean.predicted',\"KM\")],\n      type = 'b', lwd = 3, col = c(\"#B2182B\"), pch = 16)\n\nabline(0,1, lwd = 2, lty = 3, col = c(\"#224444\"))\n\nlegend(\"bottomright\", #图例的位置\n       legend = c(\"5-year\",\"8-year\"), #图例文字\n       col =c(\"#2166AC\",\"#B2182B\"), #图例线的颜色，与文字对应\n       lwd = 2,#图例中线的粗细\n       cex = 1.2,#图例字体大小\n       bty = \"n\")#不显示图例边框"
  },
  {
    "objectID": "calibration-cox.html#方法2riskregression",
    "href": "calibration-cox.html#方法2riskregression",
    "title": "33  Cox回归校准曲线绘制",
    "section": "33.3 方法2：riskRegression",
    "text": "33.3 方法2：riskRegression\n不过这种方法是把多个模型放在一张图上，不是同一个模型对应多个时间点。\n这种方法不能有缺失值。\n\n# 删除缺失值\ndf2 &lt;- na.omit(df1)\n\nlibrary(survival)\n\n# 构建模型\ncox_fit1 &lt;- coxph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n              data = df2,x = T, y = T)\ncox_fit2 &lt;- coxph(Surv(time, status) ~ age + ph.ecog + ph.karno,\n              data = df2,x = T, y = T)\n\n\n# 画图\nlibrary(riskRegression)\n## riskRegression version 2023.09.08\nset.seed(1)\ncox_fit_s &lt;- Score(list(\"fit1\" = cox_fit1,\n                        \"fit2\" = cox_fit2),\n               formula = Surv(time, status) ~ 1,\n               data = df2,\n               #metrics = c(\"auc\",\"brier\"),\n               #summary = c(\"risks\",\"IPA\",\"riskQuantile\",\"ibs\"),\n               plots = \"calibration\",\n               #null.model = T,\n               conf.int = T,\n               B = 500,\n               M = 50,\n               times=c(700) # limit the time range\n               )\nplotCalibration(cox_fit_s,\n                xlab = \"Predicted Risk\",\n                ylab = \"Observerd RISK\")\n## The default method for estimating calibration curves based on censored data has changed for riskRegression version 2019-9-8 or higher\n## Set cens.method=\"jackknife\" to get the estimate using pseudo-values.\n## However, note that the option \"jackknife\" is sensitive to violations of the assumption that the censoring is independent of both the event times and the covariates.\n## Set cens.method=\"local\" to suppress this message.\n\n\n\n\n当然也是可以用ggplot2画图的。\n\n# 获取数据\ndata_all &lt;- plotCalibration(cox_fit_s,plot = F)\n## The default method for estimating calibration curves based on censored data has changed for riskRegression version 2019-9-8 or higher\n## Set cens.method=\"jackknife\" to get the estimate using pseudo-values.\n## However, note that the option \"jackknife\" is sensitive to violations of the assumption that the censoring is independent of both the event times and the covariates.\n## Set cens.method=\"local\" to suppress this message.\n\n\n# 数据转换\nplot_df &lt;- bind_rows(data_all$plotFrames) %&gt;% \n  mutate(fits = rep(c(\"fit1\",\"fit2\"),c(44,38)))\n\n# 画图\nggplot(plot_df, aes(Pred,Obs))+\n  geom_line(aes(group=fits,color=fits),size=1.2)+\n  scale_color_manual(values = c(\"#2166AC\",\"tomato\"),name=NULL)+\n  scale_x_continuous(limits = c(0,1),name = \"Predicted Risk\")+\n  scale_y_continuous(limits = c(0,1),name = \"Observerd Risk\")+\n  geom_abline(slope = 1,intercept = 0,lty=2)+\n  geom_rug(aes(color=fits))+\n  theme_bw()\n## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n## ℹ Please use `linewidth` instead."
  },
  {
    "objectID": "calibration-cox-test.html#加载数据",
    "href": "calibration-cox-test.html#加载数据",
    "title": "34  Cox回归测试集校准曲线绘制",
    "section": "34.1 加载数据",
    "text": "34.1 加载数据\n\nrm(list = ls())\nlibrary(survival)\n\nlung$status &lt;- ifelse(lung$status == 2,1,0)\nlung &lt;- na.omit(lung)\n\ndim(lung)\n## [1] 167  10\nstr(lung)\n## 'data.frame':    167 obs. of  10 variables:\n##  $ inst     : num  3 5 12 7 11 1 7 6 12 22 ...\n##  $ time     : num  455 210 1022 310 361 ...\n##  $ status   : num  1 1 0 1 1 1 1 1 1 1 ...\n##  $ age      : num  68 57 74 68 71 53 61 57 57 70 ...\n##  $ sex      : num  1 1 1 2 2 1 1 1 1 1 ...\n##  $ ph.ecog  : num  0 1 1 2 2 1 2 1 1 1 ...\n##  $ ph.karno : num  90 90 50 70 60 70 70 80 80 90 ...\n##  $ pat.karno: num  90 60 80 60 80 80 70 80 70 100 ...\n##  $ meal.cal : num  1225 1150 513 384 538 ...\n##  $ wt.loss  : num  15 11 0 10 1 16 34 27 60 -5 ...\n##  - attr(*, \"na.action\")= 'omit' Named int [1:61] 1 3 5 12 13 14 16 20 23 25 ...\n##   ..- attr(*, \"names\")= chr [1:61] \"1\" \"3\" \"5\" \"12\" ..."
  },
  {
    "objectID": "calibration-cox-test.html#数据分割",
    "href": "calibration-cox-test.html#数据分割",
    "title": "34  Cox回归测试集校准曲线绘制",
    "section": "34.2 数据分割",
    "text": "34.2 数据分割\n把数据随机划分为训练集、测试集，划分比例为7:3\n\nset.seed(123)\nind &lt;- sample(1:nrow(lung),nrow(lung)*0.7)\n\ntrain_df &lt;- lung[ind,]\ntest_df &lt;- lung[- ind, ]\n\ndim(train_df)\n## [1] 116  10\ndim(test_df)\n## [1] 51 10"
  },
  {
    "objectID": "calibration-cox-test.html#方法1riskregression",
    "href": "calibration-cox-test.html#方法1riskregression",
    "title": "34  Cox回归测试集校准曲线绘制",
    "section": "34.3 方法1：riskRegression",
    "text": "34.3 方法1：riskRegression\n使用riskRegression轻松实现，训练集的校准曲线就不画了，直接画测试集的校准曲线。\n\nlibrary(riskRegression)\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\n## riskRegression version 2023.09.08\n\n# 在训练集建立模型\ncoxph_fit &lt;- coxph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n                   data = train_df,\n                   x = T, y = T)\n\n# 画出测试集的校准曲线\nset.seed(1)\ncox_fit_t &lt;- Score(list(\"fit1\" = coxph_fit),\n               formula = Surv(time, status) ~ 1,\n               data = test_df, # 这里写测试集即可\n               plots = \"calibration\",\n               conf.int = T,\n               B = 500, # 重抽样500次\n               M = 50,\n               times=c(100) # 时间\n               )\n\n# 画图即可\nplotCalibration(cox_fit_t,\n                cens.method=\"local\",\n                xlab = \"Predicted Risk\",\n                ylab = \"Observerd RISK\")\n\n\n\n\n这个就是测试集的校准曲线了，非常简单，而且支持返回数据自己用ggplot2画，你想要的样子它都可以有！可参考推文：生存资料校准曲线的绘制"
  },
  {
    "objectID": "calibration-cox-test.html#方法2rms",
    "href": "calibration-cox-test.html#方法2rms",
    "title": "34  Cox回归测试集校准曲线绘制",
    "section": "34.4 方法2：rms",
    "text": "34.4 方法2：rms\n回归建模神包rms当然也是支持COX回归测试集校准曲线的，不过隐藏的比较深。\n在上次介绍logistic回归测试集的校准曲线时，我们介绍过val.prob函数，这里给大家介绍下val.surv函数。\n首先还是要使用使用cph函数建立cox回归模型，这里我们指定时间为100天：\n\nsuppressMessages(library(rms))\n\n# 一定要记得打包数据\ndd &lt;- datadist(train_df)\noptions(datadist = \"dd\")\nunits(train_df$time) &lt;- \"days\"\n\ncph_fit &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n               data = train_df,\n               x = T, y = T,\n               time.inc = 100\n               )\n\n然后计算模型在测试集中的各种指标：\n\nv &lt;- val.surv(fit= cph_fit, # 模型\n         newdata = test_df, # 测试集\n         u=100, # 时间\n         #evaluate = 10,\n         S=Surv(test_df$time,test_df$status) # 测试集的生存对象\n         )\nv\n## \n## Validation of Predicted Survival at Time= 100    n= 51 , events= 40 \n## \n## hare fit:\n## \n## dim A/D   loglik       AIC        penalty \n##                                 min    max \n##   1 Add   -284.61    573.15   11.98     Inf\n##   2 Add   -278.62    565.11    7.05   11.98\n##   3 Add   -275.81    563.41      NA      NA\n##   4 Add   -271.57    558.87    3.15    7.05\n##   5 Add   -270.00    559.65    0.00    3.15\n## \n## the present optimal number of dimensions is 4.\n## penalty(AIC) was the default: BIC=log(samplesize): log(51)=3.93\n## \n##   dim1           dim2           beta        SE         Wald\n## Constant                            -5.6          1   -5.65\n## Time   3.6e+02                      0.01     0.0052    1.95\n## Co-1  linear                      -0.091       0.52   -0.17\n## Time   3.6e+02 Co-1  linear       0.0094     0.0033    2.89\n## \n## Function used to transform predictions:\n## function (p)  log(-log(p))\n## \n## Mean absolute error in predicted probabilities: 0.0894 \n## 0.9 Quantile of absolute errors               : 0.1025\n\n之后通过调用plot实现测试集的校准曲线，可以看到这个校准曲线并不是通过分箱的方法获得的，这里使用了一种hare方法(通过polspline包实现，感兴趣的可以自己看看)，且在测试集中没得选，只能用这种方法(训练集的校准曲线可通过method参数选择)。\n但是这里也要注意，仔细阅读Regression Modeling Strategies 就会发现，val.surv计算的是外部验证集的各种指标，画出来的图也是外部验证集的，这一点在logistic回归测试集校准曲线的6种实现方法 中也说过。\n\nplot(v, \n     scat1d.opts=list(nhistSpike=200, side=3)\n     )\n\n\n\n\n这里由于数据集样本量太少，结果并不好看，大家可以使用自己的数据(样本量尽量大一点)尝试一下。\n2种方法，我还是比较推荐riskRegression，因为简单又强大。\n但是很明显这种简单又好用的方法反而是在文献中出现的比较少的！\n大家经常读文献就会发现这种COX回归测试集的校准曲线↓：\n\n目前好像并没有包可以直接实现，不过也不是什么难题，下次介绍。"
  },
  {
    "objectID": "calibration-cox-test.html#方法3",
    "href": "calibration-cox-test.html#方法3",
    "title": "34  Cox回归测试集校准曲线绘制",
    "section": "34.5 方法3",
    "text": "34.5 方法3\n首先把数据集划分为训练集、测试集，然后在训练集建立cox模型\n\nrm(list = ls())\nlibrary(survival)\n\nlung$status &lt;- ifelse(lung$status == 2,1,0)\nlung &lt;- na.omit(lung)\n\nset.seed(123)\nind &lt;- sample(1:nrow(lung),nrow(lung)*0.7)\n\ntrain_df &lt;- lung[ind,]\ntest_df &lt;- lung[- ind, ]\n\n# 在训练集建立cox模型\ncoxph_fit &lt;- coxph(Surv(time, status) ~ age + sex + ph.ecog,\n                   data = train_df,\n                   x = T, y = T)\n\n\n34.5.1 训练集的校准曲线\n对于生存分析来说，校准曲线的横纵坐标分别是模型预测的生存概率(predicted survival probability)和K-M法估计的生存概率(也可以称为Observed survival probability)，其中模型预测的生存概率是模型算出来的(生存分析一般是根据COX算出来的)，K-M法估计的生存概率则是根据乘积极限法(K-M法)计算的。\n模型计算的生存概率很简单可以得到，比如训练集100天的校准曲线，首先计算cox模型预测的100天的生存概率：\n\n# 模型预测的概率\nset.seed(123)\ntrain_p &lt;- c((summary(survfit(coxph_fit, newdata=train_df), times=100)$surv))\nhead(train_p)\n## [1] 0.6958190 0.9347173 0.8681793 0.7350013 0.8694693 0.8213824\n\n以上是通过模型计算出的生存概率，为了画出校准曲线，我们还需要真实的生存概率，怎么计算？通过K-M法。\n如何通过K-M法计算真实的生存概率？\n首先，分组。根据什么东西分组？就根据我们通过模型计算的概率分组。\n为了保证每组都有人，我们就分为4组(可以多试几次)：\n\n# 根据这几个点进行切分\ncuts &lt;- unique(quantile(c(0, 1, train_p), seq(0, 1, length = 5), na.rm = TRUE))\ncuts\n## [1] 0.0000000 0.8097154 0.8526484 0.8892975 1.0000000\n\n然后计算K-M法估计的生存概率：\n\nsuppressMessages(library(rms))\n\nkm_surv &lt;- groupkm(train_p,\n                   Srv = Surv(train_df$time,train_df$status),\n                   u = 100,\n                   cuts = cuts)\n\nkm_surv\n##              x  n events        KM    std.err\n## [1,] 0.7597781 28     24 0.7500000 0.10910895\n## [2,] 0.8291634 29     20 0.8620690 0.07427814\n## [3,] 0.8695150 30     23 0.8333333 0.08164966\n## [4,] 0.9131132 29     13 0.9310345 0.05053987\n\n其中x就是模型预测的概率，KM是K-M法估计的真实概率，所以就可以画图了：\n\nplot(km_surv[,1], km_surv[,4],xlim=c(0,1),ylim=c(0,1))\nlines(km_surv[,1], km_surv[,4])\n\n\n\n\n上面这个图比较简陋，下面进行美化。\n\nplot(km_surv[,1], km_surv[,4],\n     xlim=c(0,1),ylim=c(0,1),\n     xlab = 'Predicted 100-day Survival Probability',\n     ylab = 'Observed 100-day Survival Probability'\n     )\nlines(km_surv[,1], km_surv[,4])\n\n# 计算误差线范围\nerrl &lt;- ifelse(km_surv[,\"KM\"] == 0, 0,  \n               km_surv[,\"KM\"] * exp(1.959964 * (-km_surv[,\"std.err\"])))\nerrh &lt;- ifelse(km_surv[,\"KM\"] == 0, 0, \n               pmin(1, km_surv[,\"KM\"] * exp(1.959964 * km_surv[,\"std.err\"])))\n# 添加误差线\nerrbar(x = km_surv[,\"x\"],\n       y = km_surv[,\"KM\"],\n       yminus = errl,yplus = errh,\n       add = T,\n       pch=16,cex=1,\n       asp=1,xaxs='i',yaxs='i'\n       )\n# 添加对角线\nabline(a = 0,b = 1,col='grey')\n\n\n\n\n这个图就基本和上面一样了，和开头的文献里的图一模一样！\n\n\n34.5.2 测试集的校准曲线\n方法一模一样，很简单！\n测试集100天的校准曲线，首先也是计算概率：\n\nset.seed(123)\n\ntest_p &lt;- c((summary(survfit(coxph_fit, newdata=test_df), times=100)$surv))\n\n然后分组：\n\ncuts_t &lt;- unique(quantile(c(0, 1, test_p), seq(0, 1, length = 5), na.rm = TRUE))\ncuts_t\n## [1] 0.0000000 0.7887829 0.8588841 0.8829986 1.0000000\n\n然后计算K-M法估计的生存概率：\n\nsuppressMessages(library(rms))\n\nkm_surv_t &lt;- groupkm(test_p,\n                     Srv = Surv(test_df$time,test_df$status),\n                     u = 100,\n                     cuts = cuts_t)\n\nkm_surv_t\n##              x  n events        KM   std.err\n## [1,] 0.7379403 12     10 0.6666667 0.2041241\n## [2,] 0.8263202 13     12 1.0000000 0.0000000\n## [3,] 0.8693478 13     11 0.8461538 0.1182625\n## [4,] 0.9073257 13      7 1.0000000 0.0000000\n\n然后就可以画图了：\n\nplot(km_surv_t[,1], km_surv_t[,4],\n     xlim=c(0,1),ylim=c(0,1),\n     xlab = 'Predicted 100-day Survival Probability',\n     ylab = 'Observed 100-day Survival Probability'\n     )\nlines(km_surv_t[,1], km_surv_t[,4])\n\n# 计算误差线范围\nerrl &lt;- ifelse(km_surv_t[,\"KM\"] == 0, 0,  \n               km_surv_t[,\"KM\"] * exp(1.959964 * (-km_surv_t[,\"std.err\"])))\nerrh &lt;- ifelse(km_surv_t[,\"KM\"] == 0, 0, \n               pmin(1, km_surv_t[,\"KM\"] * exp(1.959964 * km_surv_t[,\"std.err\"])))\n# 添加误差线\nerrbar(x = km_surv_t[,\"x\"],\n       y = km_surv_t[,\"KM\"],\n       yminus = errl,yplus = errh,\n       add = T,\n       pch=16,cex=1,\n       asp=1,xaxs='i',yaxs='i'\n       )\n# 添加对角线\nabline(a = 0,b = 1,col='grey')\n\n\n\n\n这个就是测试集的校准曲线了！\n两张图拼在一起就是大家文献中常见的图了！后期我会把这些函数打包，然后变为长数据，支持使用ggplot2画图！"
  },
  {
    "objectID": "calibration-tidymodels-man.html#加载数据和r包",
    "href": "calibration-tidymodels-man.html#加载数据和r包",
    "title": "35  tidymodels手动绘制校准曲线",
    "section": "35.1 加载数据和R包",
    "text": "35.1 加载数据和R包\n没有安装的R包的自己安装下~\n\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\ntidymodels_prefer()\n\n由于要做演示用，肯定要一份比较好的数据才能说明问题，今天用的这份数据，结果变量是一个二分类的。\n一共有91976行，26列，其中play_type是结果变量，因子型，其余列都是预测变量。\n\nrm(list = ls())\nall_plays &lt;- read_rds(\"./datasets/all_plays.rds\")\nglimpse(all_plays)\n## Rows: 91,976\n## Columns: 26\n## $ game_id                    &lt;dbl&gt; 2017090700, 2017090700, 2017090700, 2017090…\n## $ posteam                    &lt;chr&gt; \"NE\", \"NE\", \"NE\", \"NE\", \"NE\", \"NE\", \"NE\", \"…\n## $ play_type                  &lt;fct&gt; pass, pass, run, run, pass, run, pass, pass…\n## $ yards_gained               &lt;dbl&gt; 0, 8, 8, 3, 19, 5, 16, 0, 2, 7, 0, 3, 10, 0…\n## $ ydstogo                    &lt;dbl&gt; 10, 10, 2, 10, 7, 10, 5, 2, 2, 10, 10, 10, …\n## $ down                       &lt;ord&gt; 1, 2, 3, 1, 2, 1, 2, 1, 2, 1, 1, 2, 3, 1, 2…\n## $ game_seconds_remaining     &lt;dbl&gt; 3595, 3589, 3554, 3532, 3506, 3482, 3455, 3…\n## $ yardline_100               &lt;dbl&gt; 73, 73, 65, 57, 54, 35, 30, 2, 2, 75, 32, 3…\n## $ qtr                        &lt;ord&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ posteam_score              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7…\n## $ defteam                    &lt;chr&gt; \"KC\", \"KC\", \"KC\", \"KC\", \"KC\", \"KC\", \"KC\", \"…\n## $ defteam_score              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0…\n## $ score_differential         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, -7, 7, 7, 7, 7, …\n## $ shotgun                    &lt;fct&gt; 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0…\n## $ no_huddle                  &lt;fct&gt; 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ posteam_timeouts_remaining &lt;fct&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n## $ defteam_timeouts_remaining &lt;fct&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n## $ wp                         &lt;dbl&gt; 0.5060180, 0.4840546, 0.5100098, 0.5529816,…\n## $ goal_to_go                 &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n## $ half_seconds_remaining     &lt;dbl&gt; 1795, 1789, 1754, 1732, 1706, 1682, 1655, 1…\n## $ total_runs                 &lt;dbl&gt; 0, 0, 0, 1, 2, 2, 3, 3, 3, 0, 4, 4, 4, 5, 5…\n## $ total_pass                 &lt;dbl&gt; 0, 1, 2, 2, 2, 3, 3, 4, 5, 0, 5, 6, 7, 7, 8…\n## $ previous_play              &lt;fct&gt; First play of Drive, pass, pass, run, run, …\n## $ in_red_zone                &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1…\n## $ in_fg_range                &lt;fct&gt; 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1…\n## $ two_min_drill              &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "calibration-tidymodels-man.html#数据划分",
    "href": "calibration-tidymodels-man.html#数据划分",
    "title": "35  tidymodels手动绘制校准曲线",
    "section": "35.2 数据划分",
    "text": "35.2 数据划分\n把75%的数据用于训练集，剩下的做测试集。\n\nset.seed(20220520)\n\n# 数据划分，根据play_type分层\nsplit_pbp &lt;- initial_split(all_plays, 0.75, strata = play_type)\n\ntrain_data &lt;- training(split_pbp) # 训练集\ntest_data &lt;- testing(split_pbp) # 测试集"
  },
  {
    "objectID": "calibration-tidymodels-man.html#数据预处理",
    "href": "calibration-tidymodels-man.html#数据预处理",
    "title": "35  tidymodels手动绘制校准曲线",
    "section": "35.3 数据预处理",
    "text": "35.3 数据预处理\n使用recipe包进行数据预处理，如果你认真学习过caret，那这个包你应该不陌生。\n\npbp_rec &lt;- recipe(play_type ~ ., data = train_data)  %&gt;%\n  step_rm(half_seconds_remaining,yards_gained, game_id) %&gt;% # 移除这3列\n  step_string2factor(posteam, defteam) %&gt;%  # 变为因子类型\n  #update_role(yards_gained, game_id, new_role = \"ID\") %&gt;% \n  # 去掉高度相关的变量\n  step_corr(all_numeric(), threshold = 0.7) %&gt;% \n  step_center(all_numeric()) %&gt;%  # 中心化\n  step_zv(all_predictors())  # 去掉零方差变量"
  },
  {
    "objectID": "calibration-tidymodels-man.html#建立模型",
    "href": "calibration-tidymodels-man.html#建立模型",
    "title": "35  tidymodels手动绘制校准曲线",
    "section": "35.4 建立模型",
    "text": "35.4 建立模型\n就以经常用的随机森林进行演示，这里就不演示调参了，因为也不一定比默认参数的结果好……\n选择随机森林，建立workflow：\n\nrf_spec &lt;- rand_forest(mode = \"classification\") %&gt;% \n  set_engine(\"ranger\",importance = \"permutation\")\nrf_wflow &lt;- workflow() %&gt;% \n  add_recipe(pbp_rec) %&gt;% \n  add_model(rf_spec)\n\n在训练集建模：\n\nfit_rf &lt;- rf_wflow %&gt;% \n  fit(train_data)"
  },
  {
    "objectID": "calibration-tidymodels-man.html#模型评价",
    "href": "calibration-tidymodels-man.html#模型评价",
    "title": "35  tidymodels手动绘制校准曲线",
    "section": "35.5 模型评价",
    "text": "35.5 模型评价\n应用于测试集：\n\npred_rf &lt;- test_data %&gt;% select(play_type) %&gt;% \n  bind_cols(predict(fit_rf, test_data, type = \"prob\")) %&gt;% \n  bind_cols(predict(fit_rf, test_data, type = \"class\"))\n\n#save(pred_rf, file = \"../000预测模型/pred_rf.rdata\")\n\n这个pred_rf就是接下来一系列操作的基础，非常重要！！\n\nhead(pred_rf)\n## # A tibble: 6 × 4\n##   play_type .pred_pass .pred_run .pred_class\n##   &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;      \n## 1 pass           0.312     0.688 run        \n## 2 pass           0.829     0.171 pass       \n## 3 pass           0.806     0.194 pass       \n## 4 pass           0.678     0.322 pass       \n## 5 run            0.184     0.816 run        \n## 6 run            0.544     0.456 pass\n\n查看模型表现：\n你知道的又或者不知道的指标基本上都有：\n\nmetricsets &lt;- metric_set(accuracy, mcc, f_meas, j_index)\n\npred_rf %&gt;% metricsets(truth = play_type, estimate = .pred_class)\n## # A tibble: 4 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.731\n## 2 mcc      binary         0.441\n## 3 f_meas   binary         0.774\n## 4 j_index  binary         0.439\n\n混淆矩阵：\n\npred_rf %&gt;% conf_mat(truth = play_type, estimate = .pred_class)\n##           Truth\n## Prediction  pass   run\n##       pass 10622  3226\n##       run   2962  6185\n\n混淆矩阵图形版：\n\npred_rf %&gt;% \n  conf_mat(play_type,.pred_class) %&gt;% \n  autoplot()\n\n\n\n\n大家最喜欢的AUC：\n\npred_rf %&gt;% roc_auc(truth = play_type, .pred_pass)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc binary         0.799\n\n可视化结果，首先是大家喜闻乐见的ROC曲线：\n\npred_rf %&gt;% roc_curve(truth = play_type, .pred_pass) %&gt;% \n  autoplot()\n\n\n\n\npr曲线：\n\npred_rf %&gt;% pr_curve(truth = play_type, .pred_pass) %&gt;% \n  autoplot()\n\n\n\n\ngain_curve：\n\npred_rf %&gt;% gain_curve(truth = play_type, .pred_pass) %&gt;% \n  autoplot()\n\n\n\n\nlift_curve：\n\npred_rf %&gt;% lift_curve(truth = play_type, .pred_pass) %&gt;% \n  autoplot()\n\n\n\n\n\n就是没有校准曲线！！"
  },
  {
    "objectID": "calibration-tidymodels-man.html#校准曲线",
    "href": "calibration-tidymodels-man.html#校准曲线",
    "title": "35  tidymodels手动绘制校准曲线",
    "section": "35.6 校准曲线",
    "text": "35.6 校准曲线\n下面给大家手动画一个校准曲线。\n两种画法，差别不大，主要是分组方法不一样，第2种分组方法是大家常见的哦~\n如果你还不懂为什么我说校准曲线是散点图，建议你先看看前面的基础知识，看了不吃亏。\n\ncalibration_df &lt;- pred_rf %&gt;% \n   mutate(pass = if_else(play_type == \"pass\", 1, 0),\n          pred_rnd = round(.pred_pass, 2)\n          ) %&gt;% \n  group_by(pred_rnd) %&gt;% \n  dplyr::summarize(mean_pred = mean(.pred_pass),\n            mean_obs = mean(pass),\n            n = n()\n            )\n\nggplot(calibration_df, aes(mean_pred, mean_obs))+ \n  geom_point(aes(size = n), alpha = 0.5)+\n  geom_abline(linetype = \"dashed\")+\n  theme_minimal()\n\n\n\n\n第2种方法：\n\ncali_df &lt;- pred_rf %&gt;% \n  arrange(.pred_pass) %&gt;% \n  mutate(pass = if_else(play_type == \"pass\", 1, 0),\n         group = c(rep(1:249,each=92), rep(250,87))\n         ) %&gt;% \n  group_by(group) %&gt;% \n  dplyr::summarise(mean_pred = mean(.pred_pass),\n            mean_obs = mean(pass)\n            )\n\n\ncali_plot &lt;- ggplot(cali_df, aes(mean_pred, mean_obs))+ \n  geom_point(alpha = 0.5)+\n  geom_abline(linetype = \"dashed\")+\n  theme_minimal()\n\ncali_plot\n\n\n\n\n两种方法差别不大，效果都是很好的，这就说明，好就是好，不管你用什么方法，都是好！如果你的数据很烂，那大概率你的结果也是很烂！不管用什么方法都是烂！\n最后，随机森林这种方法是可以计算变量重要性的，当然也是能把结果可视化的。\n顺手给大家演示下如何可视化随机森林结果的变量重要性：\n\nlibrary(vip)\n\nfit_rf %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip(num_features = 10)\n\n\n\n\n所以，校准曲线的画法，你学会了吗？"
  },
  {
    "objectID": "calibration-tidymodels.html#安装",
    "href": "calibration-tidymodels.html#安装",
    "title": "36  tidymodels已原生支持校准曲线",
    "section": "36.1 安装",
    "text": "36.1 安装\n截止到2023.3.30日，只能安装开发版才有这个功能，cran上的版本还没有这个功能。\n\nremotes::install_github(\"tidymodels/probably\")"
  },
  {
    "objectID": "calibration-tidymodels.html#使用",
    "href": "calibration-tidymodels.html#使用",
    "title": "36  tidymodels已原生支持校准曲线",
    "section": "36.2 使用",
    "text": "36.2 使用\n和yardstick的用法一脉相承，如果你还不知道yardstick以及tidymodels系列的使用规范，请参考这篇入门教程：tidymodels用于机器学习的一些使用细节\n我们使用这篇推文中的数据：tidymodels不能画校准曲线？。\n具体过程就不再演示了，直接使用pred_rf这个对象进行演示。\n\nrm(list = ls())\nsuppressMessages(library(tidymodels))\nsuppressMessages(library(probably))\n\nload(file = \"./datasets/pred_rf.rdata\")\n\n这个结果就是tidymodels中得到的标准结果，一切模型衡量指标都是通过这个结果算出来的，包括校准曲线。\n结果长这样：\n\nhead(pred_rf)\n## # A tibble: 6 × 4\n##   play_type .pred_pass .pred_run .pred_class\n##   &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;      \n## 1 pass           0.312     0.688 run        \n## 2 pass           0.829     0.171 pass       \n## 3 pass           0.806     0.194 pass       \n## 4 pass           0.678     0.322 pass       \n## 5 run            0.184     0.816 run        \n## 6 run            0.544     0.456 pass\n\n\n36.2.1 基于分箱的校准曲线\n校准曲线是先对概率进行分箱，然后计算平均值得到的，如果你还不明白，请参考这篇推文：一文搞懂临床预测模型的评价！\n基于分箱的原理画校准曲线，通过以下函数实现：\n\npred_rf %&gt;% \n  cal_plot_breaks(play_type, .pred_pass)\n\n\n\n\n有几个参数可以调节：\n\npred_rf %&gt;% \n  cal_plot_breaks(play_type, .pred_pass, \n                  num_breaks = 5 , # 选几个点\n                  include_rug=F, # 是否添加地毯线\n                  include_ribbon = T, # 是否添加可信区间\n                  conf_level = 0.95 # 可信区间范围\n                  )\n\n\n\n\n\n\n36.2.2 基于窗口滑动的校准曲线\n但是有时候样本量太少，不方便分箱，tidymodels提供了一种窗口滑动的方式。\n比如一共100个样本，第一次取第1个到第10个样本，然后计算得到一个预测值，第二次取第6个到第15个样本，计算得到一个预测值，第三次取第11个带第20个样本，以此类推，彼此之间是有重复的。\n可通过cal_plot_windowed函数实现这样的校准曲线：\n\npred_rf %&gt;% \n  cal_plot_windowed(play_type, .pred_pass)\n\n\n\n\n其中step_size参数控制每次增加多少：\n\n# 每次增加2%\npred_rf %&gt;% \n  cal_plot_windowed(play_type, .pred_pass, step_size = 0.02)\n\n\n\n\n所有这些图都是基于ggplot2的，可以无缝衔接ggplot2语法进行美化、修改。\n对于分类模型的校准曲线终于有了！大家有需要的可以用起来了。"
  },
  {
    "objectID": "calibration-mlr3.html#加载r包",
    "href": "calibration-mlr3.html#加载r包",
    "title": "37  mlr3绘制校准曲线",
    "section": "37.1 加载R包",
    "text": "37.1 加载R包\n首先还是加载数据和R包，和之前的数据一样的。\n\nrm(list = ls())\nlibrary(mlr3verse)\n## Loading required package: mlr3\nlibrary(mlr3pipelines)\nlibrary(mlr3filters)"
  },
  {
    "objectID": "calibration-mlr3.html#建立任务",
    "href": "calibration-mlr3.html#建立任务",
    "title": "37  mlr3绘制校准曲线",
    "section": "37.2 建立任务",
    "text": "37.2 建立任务\n然后是对数据进行划分训练集和测试集，对数据进行预处理，为了和之前的tidymodels进行比较，这里使用的数据和预处理步骤都是和之前一样的。\n\n# 读取数据\nall_plays &lt;- readRDS(\"./datasets/all_plays.rds\")\n\n# 建立任务\npbp_task &lt;- as_task_classif(all_plays, target=\"play_type\")\n\n# 数据划分\nsplit_task &lt;- partition(pbp_task, ratio=0.75)\n\ntask_train &lt;- pbp_task$clone()$filter(split_task$train)\ntask_test &lt;- pbp_task$clone()$filter(split_task$test)"
  },
  {
    "objectID": "calibration-mlr3.html#数据预处理",
    "href": "calibration-mlr3.html#数据预处理",
    "title": "37  mlr3绘制校准曲线",
    "section": "37.3 数据预处理",
    "text": "37.3 数据预处理\n建立任务后就是建立数据预处理步骤，这里采用和上篇推文tidymodels中一样的预处理步骤：\n\n# 数据预处理\npbp_prep &lt;- po(\"select\", # 去掉3列\n               selector = selector_invert(\n                 selector_name(c(\"half_seconds_remaining\",\"yards_gained\",\"game_id\")))\n               ) %&gt;&gt;%\n  po(\"colapply\", # 把这两列变成因子类型\n     affect_columns = selector_name(c(\"posteam\",\"defteam\")),\n     applicator = as.factor) %&gt;&gt;% \n  po(\"filter\", # 去除高度相关的列\n     filter = mlr3filters::flt(\"find_correlation\"), filter.cutoff=0.3) %&gt;&gt;%\n  po(\"scale\", scale = F) %&gt;&gt;% # 中心化\n  po(\"removeconstants\") # 去掉零方差变量"
  },
  {
    "objectID": "calibration-mlr3.html#建立模型",
    "href": "calibration-mlr3.html#建立模型",
    "title": "37  mlr3绘制校准曲线",
    "section": "37.4 建立模型",
    "text": "37.4 建立模型\n先选择随机森林模型。\n\nrf_glr &lt;- as_learner(pbp_prep %&gt;&gt;% lrn(\"classif.ranger\", predict_type=\"prob\")) \nrf_glr$id &lt;- \"randomForest\"\n\n很多人喜欢在训练集中使用10折交叉验证，但其实这对于提高模型表现没什么用尤其是临床预测模型这个领域因为你的模型表现好不好很大程度上取决于你的数据好不好！鸭子是不会变成天鹅的，这一点可以看附录。\n\n# 加速\nlibrary(future)\nplan(\"multisession\",workers=12)\n\n# 为了加快速度，用个3折交叉验证\nrr &lt;- resample(task = task_train,\n               learner = rf_glr,\n               resampling = rsmp(\"cv\",folds = 3),\n               store_models = T\n               )\n## INFO  [20:44:17.522] [mlr3] Applying learner 'randomForest' on task 'all_plays' (iter 1/3)\n## INFO  [20:44:17.868] [mlr3] Applying learner 'randomForest' on task 'all_plays' (iter 2/3)\n## INFO  [20:44:18.212] [mlr3] Applying learner 'randomForest' on task 'all_plays' (iter 3/3)"
  },
  {
    "objectID": "calibration-mlr3.html#评价模型",
    "href": "calibration-mlr3.html#评价模型",
    "title": "37  mlr3绘制校准曲线",
    "section": "37.5 评价模型",
    "text": "37.5 评价模型\n先看看在训练集中的表现。\n混淆矩阵：\n\nrr$prediction()$confusion\n##         truth\n## response  pass   run\n##     pass 32043 10055\n##     run   8708 18176\n\n混淆矩阵可视化：\n\nautoplot(rr$prediction())\n\n\n\n\n查看其他结果：\n\nrr$aggregate(msrs(c(\"classif.auc\",\"classif.acc\",\"classif.bbrier\")))\n##    classif.auc    classif.acc classif.bbrier \n##      0.7960892      0.7280015      0.1798211\n\n喜闻乐见ROC曲线：\n\nautoplot(rr,type = \"roc\")\n\n\n\n\n喜闻乐见的prc曲线：\n\nautoplot(rr, type = \"prc\")\n\n\n\n\n箱线图：\n\nautoplot(rr, measure = msr(\"classif.auc\"))\n\n\n\n\n以上所有介绍的图形和评价方法都在之前的推文详细介绍过了~不会的赶紧翻看。"
  },
  {
    "objectID": "calibration-mlr3.html#训练集的校准曲线",
    "href": "calibration-mlr3.html#训练集的校准曲线",
    "title": "37  mlr3绘制校准曲线",
    "section": "37.6 训练集的校准曲线",
    "text": "37.6 训练集的校准曲线\n先画训练集的校准曲线，毫无难度，看不懂的可以加群一起讨论~\n\nprediction &lt;- as.data.table(rr$prediction())\nhead(prediction)\n##    row_ids truth response prob.pass  prob.run\n## 1:       8  pass      run 0.2116344 0.7883656\n## 2:      14  pass     pass 0.8305389 0.1694611\n## 3:      29   run     pass 0.6949074 0.3050926\n## 4:      32   run      run 0.2503858 0.7496142\n## 5:      37   run     pass 0.6836322 0.3163678\n## 6:      42  pass     pass 0.5705558 0.4294442\n\n方法1：\n\nsuppressPackageStartupMessages(library(tidyverse))\nlibrary(ggsci)\n\ncalibration_df &lt;- prediction %&gt;% \n   mutate(pass = if_else(truth == \"pass\", 1, 0),\n          pred_rnd = round(prob.pass, 2)\n          ) %&gt;% \n  group_by(pred_rnd) %&gt;% \n  dplyr::summarize(mean_pred = mean(prob.pass),\n            mean_obs = mean(pass),\n            n = n()\n            )\n\nggplot(calibration_df, aes(mean_pred, mean_obs))+ \n  geom_point(aes(size = n), alpha = 0.5)+\n  scale_color_lancet()+\n  geom_abline(linetype = \"dashed\")+\n  labs(x=\"Predicted Probability\", y= \"Observed Probability\")+\n  theme_minimal()\n\n\n\n\n第2种方法，大家比较喜欢的折线图！\n\ncali_df &lt;- prediction %&gt;% \n  arrange(prob.pass) %&gt;% \n  mutate(pass = if_else(truth == \"pass\", 1, 0),\n         group = c(rep(1:100,each=680), rep(101,982))\n         ) %&gt;% \n  group_by(group) %&gt;% \n  dplyr::summarise(mean_pred = mean(prob.pass),\n            mean_obs = mean(pass)\n            )\n\nggplot(cali_df, aes(mean_pred, mean_obs))+ \n  geom_line(linewidth=1)+\n  labs(x=\"Predicted Probability\", y= \"Observed Probability\")+\n  theme_minimal()\n\n\n\n\n是不是和上一篇中的tidymodels画出来的一模一样？没错，就是一样的，就是这么简单，想怎么画就怎么画 ！"
  },
  {
    "objectID": "calibration-mlr3.html#测试集的校准曲线",
    "href": "calibration-mlr3.html#测试集的校准曲线",
    "title": "37  mlr3绘制校准曲线",
    "section": "37.7 测试集的校准曲线",
    "text": "37.7 测试集的校准曲线\n先把模型用在测试集上，得到预测结果，然后画图！\n\ncv_pred &lt;- rf_glr$train(task_train)$predict(task_test)\n\ncv_pred_df &lt;- as.data.table(cv_pred)\nhead(cv_pred_df)\n##    row_ids truth response prob.pass   prob.run\n## 1:       3   run     pass 0.7906472 0.20935277\n## 2:       4   run     pass 0.6911209 0.30887909\n## 3:       6   run      run 0.4308536 0.56914641\n## 4:       7  pass     pass 0.8317469 0.16825305\n## 5:      13   run     pass 0.9383892 0.06161080\n## 6:      16  pass     pass 0.9243608 0.07563917\n\n先画个喜闻乐见的校准曲线：\n\ncali_df &lt;- cv_pred_df %&gt;% \n  arrange(prob.pass) %&gt;% \n  mutate(pass = if_else(truth == \"pass\", 1, 0),\n         group = c(rep(1:100,each=229), rep(101,94))\n         ) %&gt;% \n  group_by(group) %&gt;% \n  dplyr::summarise(mean_pred = mean(prob.pass),\n            mean_obs = mean(pass)\n            )\n\nggplot(cali_df, aes(mean_pred, mean_obs))+ \n  geom_line(linewidth=1)+\n  labs(x=\"Predicted Probability\", y= \"Observed Probability\")+\n  theme_minimal()\n\n\n\n\n另一种颜值高点的校准曲线，给你点颜色瞧瞧！\n\ncalibration_df &lt;- cv_pred_df %&gt;% \n   mutate(pass = if_else(truth == \"pass\", 1, 0),\n          pred_rnd = round(prob.pass, 2)\n          ) %&gt;% \n  group_by(pred_rnd) %&gt;% \n  dplyr::summarize(mean_pred = mean(prob.pass),\n            mean_obs = mean(pass),\n            n = n()\n            ) %&gt;% \n  mutate(group = case_when(n &lt; 100 ~ \"&lt;100\",\n                           n &lt; 200 ~ \"&lt;200\",\n                           n &lt; 300 ~ \"&lt;300\",\n                           n &lt; 400 ~ \"&lt;400\",\n                           TRUE ~ \"≥400\"\n                           ))\n\nggplot(calibration_df, aes(mean_pred, mean_obs))+ \n  geom_point(aes(size = n, color = group))+\n  scale_color_d3()+\n  geom_abline(linetype = \"dashed\")+\n  labs(x=\"Predicted Probability\", y= \"Observed Probability\")+\n  theme_minimal()\n\n\n\n\n配色略诡异…\n校准曲线，你学会了吗？\n我知道并没有，比如，多条画一起怎么搞？生存资料的怎么搞？"
  },
  {
    "objectID": "calibration-qhscrnomo.html#安装",
    "href": "calibration-qhscrnomo.html#安装",
    "title": "38  竞争风险模型的校准曲线",
    "section": "38.1 安装",
    "text": "38.1 安装\n2选1：\n\ndevtools::install_github(\"ClevelandClinicQHS/QHScrnomo\")\ninstall.packages(\"QHScrnomo\")"
  },
  {
    "objectID": "calibration-qhscrnomo.html#准备数据",
    "href": "calibration-qhscrnomo.html#准备数据",
    "title": "38  竞争风险模型的校准曲线",
    "section": "38.2 准备数据",
    "text": "38.2 准备数据\n使用casebase中的bmtcrr数据，只使用其中的一部分，并且把字符型变成因子型。\n\nlibrary(QHScrnomo)\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\n\ndata(\"bmtcrr\",package = \"casebase\")\nbmtcrr[,c(1,2,3,6)] &lt;- lapply(bmtcrr[,c(1,2,3,6)],as.factor)\nstr(bmtcrr)\n## 'data.frame':    177 obs. of  7 variables:\n##  $ Sex   : Factor w/ 2 levels \"F\",\"M\": 2 1 2 1 1 2 2 1 2 1 ...\n##  $ D     : Factor w/ 2 levels \"ALL\",\"AML\": 1 2 1 1 1 1 1 1 1 1 ...\n##  $ Phase : Factor w/ 4 levels \"CR1\",\"CR2\",\"CR3\",..: 4 2 3 2 2 4 1 1 1 4 ...\n##  $ Age   : int  48 23 7 26 36 17 7 17 26 8 ...\n##  $ Status: int  2 1 0 2 2 2 0 2 0 1 ...\n##  $ Source: Factor w/ 2 levels \"BM+PB\",\"PB\": 1 1 1 1 1 1 1 1 1 1 ...\n##  $ ftime : num  0.67 9.5 131.77 24.03 1.47 ..."
  },
  {
    "objectID": "calibration-qhscrnomo.html#拟合竞争风险模型",
    "href": "calibration-qhscrnomo.html#拟合竞争风险模型",
    "title": "38  竞争风险模型的校准曲线",
    "section": "38.3 拟合竞争风险模型",
    "text": "38.3 拟合竞争风险模型\n先使用rms拟合cox回归模型，这几个变量只是我随便挑选的，可能并不是完全适合~\n\ndd &lt;- datadist(bmtcrr)\noptions(datadist = \"dd\")\n\nfit &lt;- cph(Surv(ftime,Status == 1) ~ Sex + rcs(Age,3)+D+Phase, data = bmtcrr,\n           x = TRUE, y= TRUE, surv=TRUE, time.inc = 24)\n\n拟合好之后再使用crr.fit变为竞争风险模型，其实是借助了cmprsk::crr：\n\ncrr &lt;- crr.fit(fit = fit, cencode = 0, failcode = 1)\nclass(crr)\n## [1] \"cmprsk\" \"crr\"\nsummary(crr)\n##              Effects              Response : Surv(ftime, Status == 1) \n## \n##  Factor              Low High Diff. Effect    S.E.    Lower 0.95 Upper 0.95\n##  Age                 20  40   20    -0.337350 0.23489 -0.79772    0.12303  \n##   Hazard Ratio       20  40   20     0.713660      NA  0.45035    1.13090  \n##  Sex - F:M            2   1   NA     0.022279 0.28692 -0.54007    0.58463  \n##   Hazard Ratio        2   1   NA     1.022500      NA  0.58271    1.79430  \n##  D - ALL:AML          2   1   NA     0.363100 0.29546 -0.21599    0.94219  \n##   Hazard Ratio        2   1   NA     1.437800      NA  0.80575    2.56560  \n##  Phase - CR1:Relapse  4   1   NA    -1.135800 0.37803 -1.87670   -0.39488  \n##   Hazard Ratio        4   1   NA     0.321160      NA  0.15309    0.67376  \n##  Phase - CR2:Relapse  4   2   NA    -1.034200 0.35885 -1.73750   -0.33084  \n##   Hazard Ratio        4   2   NA     0.355520      NA  0.17596    0.71832  \n##  Phase - CR3:Relapse  4   3   NA    -0.914910 0.58559 -2.06260    0.23282  \n##   Hazard Ratio        4   3   NA     0.400550      NA  0.12712    1.26220\n\n可以用方差分析看看各个系数的显著性：\n\nanova(crr)\n##                 Wald Statistics          Response: Surv(ftime, Status == 1) \n## \n##  Factor     Chi-Square d.f. P     \n##  Sex         0.01      1    0.9381\n##  Age         2.25      2    0.3238\n##   Nonlinear  0.04      1    0.8510\n##  D           1.51      1    0.2191\n##  Phase      14.70      3    0.0021\n##  TOTAL      19.86      7    0.0059"
  },
  {
    "objectID": "calibration-qhscrnomo.html#内部验证",
    "href": "calibration-qhscrnomo.html#内部验证",
    "title": "38  竞争风险模型的校准曲线",
    "section": "38.4 内部验证",
    "text": "38.4 内部验证\n建立好模型之后，可以用tenf.crr对验证集进行交叉验证，查看感兴趣时间点的预测结果(死亡概率)，就相当于内部验证。\n\n# 默认10折交叉验证\nset.seed(123)\nbmtcrr$preds.tenf &lt;- tenf.crr(crr, time = 36, trace = FALSE)#可以计算线性预测值，可查看帮助文档\nstr(bmtcrr$preds.tenf)\n##  num [1:177] 0.485 0.171 0.284 0.299 0.206 ...\n\n结果是第36个月时，各个病人的死亡风险，而且是考虑到了竞争风险事件的。"
  },
  {
    "objectID": "calibration-qhscrnomo.html#计算c-index",
    "href": "calibration-qhscrnomo.html#计算c-index",
    "title": "38  竞争风险模型的校准曲线",
    "section": "38.5 计算C-index",
    "text": "38.5 计算C-index\n基于上面计算出的概率，计算cindex：\n\ncindex(prob = bmtcrr$preds.tenf,\n       fstatus = bmtcrr$Status,\n       ftime = bmtcrr$ftime,\n       type = \"crr\",\n       failcode = 1, cencode = 0\n       )\n##            N            n       usable   concordant       cindex \n##  177.0000000  177.0000000 8249.0000000 5092.0000000    0.6172869\n\ncindex=0.617，说明模型一般。"
  },
  {
    "objectID": "calibration-qhscrnomo.html#校准曲线",
    "href": "calibration-qhscrnomo.html#校准曲线",
    "title": "38  竞争风险模型的校准曲线",
    "section": "38.6 校准曲线",
    "text": "38.6 校准曲线\n也是基于上面计算出的cindex。\n\ngroupci(x = bmtcrr$preds.tenf,\n        ftime = bmtcrr$ftime,\n        fstatus = bmtcrr$Status,\n        g = 5, # 分成几组\n        u = 36, # 时间点\n        failcode = 1,\n        xlab = \"Predicted 3-year mortality\",\n        ylab = \"Actual 3-year mortality\"\n        )\n\n\n\n##              x  n events        ci    std.err\n## [1,] 0.1408630 36      8 0.2286706 0.07313371\n## [2,] 0.2021363 35      7 0.2114286 0.07429595\n## [3,] 0.2841367 36     10 0.2822421 0.07775400\n## [4,] 0.3876848 35     14 0.3714286 0.08458920\n## [5,] 0.5899486 35     17 0.4857143 0.08757744\n\n这个其实就是内部验证的校准曲线了，看起来还不错，因为是在训练集中，训练集的校准曲线其实说明不了任何问题。\n如果你觉得不好看可以使用给出的数据自己画，或者直接自己计算也可。可信区间是95%CI，可以通过pred.ci计算的。"
  },
  {
    "objectID": "calibration-qhscrnomo.html#列线图",
    "href": "calibration-qhscrnomo.html#列线图",
    "title": "38  竞争风险模型的校准曲线",
    "section": "38.7 列线图",
    "text": "38.7 列线图\n建立列线图，和rms包的使用一模一样：\n\nnomogram.crr(\n  fit = crr,\n  failtime = 36,\n  lp = T,\n  xfrac = 0.65,\n  fun.at = seq(0.2, 0.45, 0.05),\n  funlabel = \"Predicted 3-year risk\"\n)"
  },
  {
    "objectID": "calibration-qhscrnomo.html#生成模型方程",
    "href": "calibration-qhscrnomo.html#生成模型方程",
    "title": "38  竞争风险模型的校准曲线",
    "section": "38.8 生成模型方程",
    "text": "38.8 生成模型方程\n可以直接给出某个时间点的线性预测值的计算方程：\n\nsas.cmprsk(crr,time = 36)\n## Base failure probability by time = 36 is 0.3308 \n## - 0.022279144 * (Sex = \"M\") - 0.012796928 * Age - \n##     6.6881995e-06 * max(Age - 15.6, 0)**3 + 1.140514e-05 * max(Age - \n##     29, 0)**3 - 4.7169407e-06 * max(Age - 48, 0)**3 - 0.36310183 * \n##     (D = \"AML\") + 0.10164664 * (Phase = \"CR2\") + 0.22089946 * \n##     (Phase = \"CR3\") + 1.1358137 * (Phase = \"Relapse\")"
  },
  {
    "objectID": "calibration-qhscrnomo.html#外部验证测试集",
    "href": "calibration-qhscrnomo.html#外部验证测试集",
    "title": "38  竞争风险模型的校准曲线",
    "section": "38.9 外部验证（测试集）",
    "text": "38.9 外部验证（测试集）\n直接predict即可：\n\ntest_df &lt;- head(bmtcrr,50)#取前50个作为测试集\nprob &lt;- predict(crr, time = 36, newdata = test_df)\nhead(prob)\n## [1] 0.4344841 0.2052952 0.3610625 0.2712397 0.2336076 0.6261795\n\n有了概率又可以计算cindex了：\n\ncindex(prob = prob,\n       fstatus = test_df$Status,\n       ftime = test_df$ftime\n       )\n##           N           n      usable  concordant      cindex \n##  50.0000000  50.0000000 630.0000000 454.0000000   0.7206349\n\n还可以绘制校准曲线：\n\ngroupci(x = prob,\n        ftime = test_df$ftime,\n        fstatus = test_df$Status,\n        u = 36,\n        g = 5\n        )\n\n\n\n##              x  n events  ci   std.err\n## [1,] 0.1619231 10      1 0.1 0.1013889\n## [2,] 0.2478567 10      2 0.2 0.1545392\n## [3,] 0.3141252 10      4 0.4 0.1683094\n## [4,] 0.4561951 10      1 0.1 0.1023904\n## [5,] 0.6326698 10      7 0.7 0.1702254\n\n是不是很easy呢。"
  },
  {
    "objectID": "calibration-qhscrnomo.html#参考资料",
    "href": "calibration-qhscrnomo.html#参考资料",
    "title": "38  竞争风险模型的校准曲线",
    "section": "38.10 参考资料",
    "text": "38.10 参考资料\n\nhttps://github.com/ClevelandClinicQHS/QHScrnomo\nvignette(“QHScrnomo”)"
  },
  {
    "objectID": "calibration-lasso.html#安装",
    "href": "calibration-lasso.html#安装",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.1 安装",
    "text": "39.1 安装\n\n# 2选1\ninstall.packages(\"hdnom\")\nremotes::install_github(\"nanxstats/hdnom\")"
  },
  {
    "objectID": "calibration-lasso.html#加载r包和数据",
    "href": "calibration-lasso.html#加载r包和数据",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.2 加载R包和数据",
    "text": "39.2 加载R包和数据\n使用smart数据集进行演示，这是一个生存数据。该数据是经过缺失值插补的，所以没有缺失值。包含3873行，27个预测变量，TEVENT是时间变量，EVENT是结局变量。\n\nlibrary(hdnom)\n\ndata(\"smart\")\nx &lt;- as.matrix(smart[, -c(1, 2)])\ntime &lt;- smart$TEVENT\nevent &lt;- smart$EVENT\ny &lt;- survival::Surv(time, event)"
  },
  {
    "objectID": "calibration-lasso.html#拟合模型",
    "href": "calibration-lasso.html#拟合模型",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.3 拟合模型",
    "text": "39.3 拟合模型\n拟合一个最简单的正则化COX回归，它会自动执行超参数调优的过程：\n\n#suppressMessages(library(\"doParallel\"))\n#registerDoParallel(detectCores())\n\nfit &lt;- fit_lasso(x, y, nfolds = 10, rule = \"lambda.1se\", seed = 1001)\nfit\n## High-Dimensional Cox Model Object\n## Random seed: 1001 \n## Model type: lasso\n## Best lambda: 0.02351754\nnames(fit)\n## [1] \"model\"  \"lambda\" \"type\"   \"seed\"   \"call\"\n\n我们选择的最佳超参数的方法是\"lambda.1se\"，此时选中的最佳lambda是0.02351754。"
  },
  {
    "objectID": "calibration-lasso.html#列线图",
    "href": "calibration-lasso.html#列线图",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.4 列线图",
    "text": "39.4 列线图\n首先要提取模型对象和超参数，不同的正则化模型有不同的超参数，不要搞错了：\n\nmodel &lt;- fit$model\n#alpha &lt;- fit$alpha\nlambda &lt;- fit$lambda\n#adapen &lt;- fit$pen_factor\n\n使用as_nomogram转换一下，就可以绘制列线图了：\n\nnom &lt;- as_nomogram(\n  fit, x, time, event,\n  pred.at = 365 * 2,\n  funlabel = \"2-Year Overall Survival Probability\"\n)\n\nplot(nom)"
  },
  {
    "objectID": "calibration-lasso.html#模型验证",
    "href": "calibration-lasso.html#模型验证",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.5 模型验证",
    "text": "39.5 模型验证\n\n39.5.1 内部验证\n通过validate函数可以实现内部验证，和rms包如出一辙！支持bootstrap、交叉验证、重复交叉验证3种重抽样方法。\n注意这个函数对不同的正则化方法有不同的要求，注意看帮助文档，下面是10次bootstrap的内部验证，时间截点选择的是第一年到第5年，每半年一次（也就是第1，1.5，2，2.5，3，3.5，4，4.5，5年这几个截点）。\n\nval_int &lt;- validate(\n  x, time, event,\n  model.type = \"lasso\",\n  alpha = 1, # lasso的alpha是1\n  lambda = lambda,\n  method = \"bootstrap\", boot.times = 10,\n  tauc.type = \"UNO\", tauc.time = seq(1, 5, 0.5) * 365,\n  seed = 42, trace = FALSE\n)\n\n# 最后一行给出了不同时间截点的time-dependent AUC\nprint(val_int)\n## High-Dimensional Cox Model Validation Object\n## Random seed: 42 \n## Validation method: bootstrap\n## Bootstrap samples: 10 \n## Model type: lasso \n## glmnet model alpha: 1 \n## glmnet model lambda: 0.02351754 \n## glmnet model penalty factor: not specified\n## Time-dependent AUC type: UNO \n## Evaluation time points for tAUC: 365 547.5 730 912.5 1095 1277.5 1460 1642.5 1825\n\n# 给出time-dependent AUC的均值、最大值、最小值等信息\nsummary(val_int)\n## Time-Dependent AUC Summary at Evaluation Time Points\n##                365     547.5       730     912.5      1095    1277.5      1460\n## Mean     0.6621581 0.6954059 0.6790755 0.6735408 0.7087387 0.7318520 0.6849760\n## Min      0.6520262 0.6885520 0.6695303 0.6673154 0.6925704 0.7075978 0.6730413\n## 0.25 Qt. 0.6613887 0.6924443 0.6774962 0.6704223 0.7077576 0.7275127 0.6802230\n## Median   0.6624251 0.6959672 0.6793223 0.6742531 0.7099186 0.7333807 0.6862219\n## 0.75 Qt. 0.6639822 0.6986393 0.6818983 0.6757052 0.7126817 0.7381664 0.6904942\n## Max      0.6682398 0.6998708 0.6895636 0.6792039 0.7194969 0.7481341 0.6951763\n##             1642.5      1825\n## Mean     0.6809777 0.6993722\n## Min      0.6698759 0.6892602\n## 0.25 Qt. 0.6779836 0.6961173\n## Median   0.6810805 0.7018242\n## 0.75 Qt. 0.6852553 0.7026699\n## Max      0.6908602 0.7058832\n\n其中的均值和中位数可以当做矫正过误差的模型表现。\n把time-dependent AUC画出来：\n\nplot(val_int)\n##                365     547.5       730     912.5      1095    1277.5      1460\n## Mean     0.6621581 0.6954059 0.6790755 0.6735408 0.7087387 0.7318520 0.6849760\n## Min      0.6520262 0.6885520 0.6695303 0.6673154 0.6925704 0.7075978 0.6730413\n## 0.25 Qt. 0.6613887 0.6924443 0.6774962 0.6704223 0.7077576 0.7275127 0.6802230\n## Median   0.6624251 0.6959672 0.6793223 0.6742531 0.7099186 0.7333807 0.6862219\n## 0.75 Qt. 0.6639822 0.6986393 0.6818983 0.6757052 0.7126817 0.7381664 0.6904942\n## Max      0.6682398 0.6998708 0.6895636 0.6792039 0.7194969 0.7481341 0.6951763\n##             1642.5      1825\n## Mean     0.6809777 0.6993722\n## Min      0.6698759 0.6892602\n## 0.25 Qt. 0.6779836 0.6961173\n## Median   0.6810805 0.7018242\n## 0.75 Qt. 0.6852553 0.7026699\n## Max      0.6908602 0.7058832\n\n\n\n\n上图中实线表示AUC的平均值，虚线表示AUC的中位数。图中较暗的区间显示AUC的25%和75%分位数，较浅的区间显示AUC的最小值和最大值。\n\n\n39.5.2 外部验证\n也就是用一个新的数据集进行验证，这里我们从smart中随机抽取1000个样本作为外部验证集。\n外部验证使用validate_external，使用方法和validate一模一样，但是没有重抽样了，因为外部验证就是检查模型的，就是要最真实的结果，不需要重抽样：\n\nx_new &lt;- as.matrix(smart[, -c(1, 2)])[1001:2000, ]\ntime_new &lt;- smart$TEVENT[1001:2000]\nevent_new &lt;- smart$EVENT[1001:2000]\n\nval_ext &lt;- validate_external(\n  fit, x, time, event,\n  x_new, time_new, event_new,\n  tauc.type = \"UNO\",\n  tauc.time = seq(0.25, 2, 0.25) * 365 # 时间截点和内部验证不同了\n)\n\n# 3个查看结果的方法，也是内部验证一样的\nprint(val_ext)\n## High-Dimensional Cox Model External Validation Object\n## Model type: lasso \n## Time-dependent AUC type: UNO \n## Evaluation time points for tAUC: 91.25 182.5 273.75 365 456.25 547.5 638.75 730\nsummary(val_ext)\n## Time-Dependent AUC Summary at Evaluation Time Points\n##         91.25     182.5   273.75       365    456.25     547.5    638.75\n## AUC 0.4794879 0.6008266 0.662017 0.6523222 0.6732534 0.6963037 0.7013534\n##           730\n## AUC 0.7042923\nplot(val_ext)\n##         91.25     182.5   273.75       365    456.25     547.5    638.75\n## AUC 0.4794879 0.6008266 0.662017 0.6523222 0.6732534 0.6963037 0.7013534\n##           730\n## AUC 0.7042923"
  },
  {
    "objectID": "calibration-lasso.html#模型校准",
    "href": "calibration-lasso.html#模型校准",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.6 模型校准",
    "text": "39.6 模型校准\n也就是绘制校准曲线，同样是支持内部校准和外部校准。\n\n39.6.1 内部校准曲线\n内部校准通过calibrate实现，也是支持bootstrap、交叉验证、重复交叉验证3种重抽样方法，和rms包类似的。\n下面是10次bootstrap的内部校准，时间截点选择的是第5年：\n\ncal_int &lt;- calibrate(\n  x, time, event,\n  model.type = \"aenet\",\n  alpha = 1, \n  lambda = lambda, \n  #pen.factor = adapen,\n  method = \"bootstrap\", boot.times = 10,\n  pred.at = 365 * 5, ngroup = 3, # 分几组\n  seed = 42, trace = FALSE\n)\n\n# 查看结果\nprint(cal_int)\n## High-Dimensional Cox Model Calibration Object\n## Random seed: 42 \n## Calibration method: bootstrap\n## Bootstrap samples: 10 \n## Model type: aenet \n## glmnet model alpha: 1 \n## glmnet model lambda: 0.02351754 \n## glmnet model penalty factor: not specified\n## Calibration time point: 1825 \n## Number of groups formed for calibration: 3\n\n# 真实值、预测值、可信区间\nsummary(cal_int)\n##   Calibration Summary Table\n##   Predicted  Observed Lower 95% Upper 95%\n## 1 0.8214255 0.7508803 0.7216191 0.7813280\n## 2 0.8696257 0.9021791 0.8812135 0.9236435\n## 3 0.8978577 0.9307660 0.9136491 0.9482034\n\n这个结果的预测值是预测概率的中位数（因为使用了bootstrap，所以会有多个预测值，所以可以计算中位数），真实值是K-M法计算的，这些我们在之前的推文中都详细介绍过的，后台回复校准曲线即可获取相关推文合集了。\n绘制校准曲线：\n\nplot(cal_int, xlim = c(0.7, 1), ylim = c(0.7, 1))\n\n\n\n\n可以多个时间点的画在一起。\n\n\n39.6.2 外部校准\n通过calibrate_external实现：\n\ncal_ext &lt;- calibrate_external(\n  fit, x, time, event,\n  x_new, time_new, event_new,\n  pred.at = 365 * 5, ngroup = 3\n)\n\nprint(cal_ext)\n## High-Dimensional Cox Model External Calibration Object\n## Model type: lasso \n## Calibration time point: 1825 \n## Number of groups formed for calibration: 3\nsummary(cal_ext)\n##   External Calibration Summary Table\n##   Predicted  Observed Lower 95% Upper 95%\n## 1 0.8132618 0.7376980 0.6892529 0.7895482\n## 2 0.8721412 0.8829179 0.8478426 0.9194443\n## 3 0.9021290 0.9381784 0.9113542 0.9657922\nplot(cal_ext, xlim = c(0.5, 1), ylim = c(0.5, 1))"
  },
  {
    "objectID": "calibration-lasso.html#risk-group的k-m生存曲线",
    "href": "calibration-lasso.html#risk-group的k-m生存曲线",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.7 risk group的K-M生存曲线",
    "text": "39.7 risk group的K-M生存曲线\n在进行模型校准时我们是分成了3组，所以这里可以对这3组进行K-M生存分析，log-tank检验的P值也会展示出来。\n内部验证的3个组别：\n\nkmplot(\n  cal_int,\n  group.name = c(\"High risk\", \"Medium risk\", \"Low risk\"),\n  time.at = 1:6 * 365\n)\n\n\n\n\n外部验证的3个组别：\n\nkmplot(\n  cal_ext,\n  group.name = c(\"High risk\", \"Medium risk\", \"Low risk\"),\n  time.at = 1:6 * 365\n)"
  },
  {
    "objectID": "calibration-lasso.html#risk-group的log-rank检验",
    "href": "calibration-lasso.html#risk-group的log-rank检验",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.8 risk group的log-rank检验",
    "text": "39.8 risk group的log-rank检验\n也可以直接进行log-rank检验：\n\n# 内部验证3个组的log-rank检验\ncal_int_logrank &lt;- logrank_test(cal_int)\ncal_int_logrank\n## Call:\n## survdiff(formula = formula(\"Surv(time, event) ~ grp\"))\n## \n## n=3872, 1 observation deleted due to missingness.\n## \n##          N Observed Expected (O-E)^2/E (O-E)^2/V\n## grp=1 1290      279      148     116.1     171.5\n## grp=2 1291      104      155      16.6      25.0\n## grp=3 1291       76      156      41.4      62.8\n## \n##  Chisq= 174  on 2 degrees of freedom, p= &lt;2e-16\ncal_int_logrank$pval\n## [1] 1.511019e-38\n\n# 外部验证3个组的log-rank检验\ncal_ext_logrank &lt;- logrank_test(cal_ext)\ncal_ext_logrank\n## Call:\n## survdiff(formula = formula(\"Surv(time, event) ~ grp\"))\n## \n## n=999, 1 observation deleted due to missingness.\n## \n##         N Observed Expected (O-E)^2/E (O-E)^2/V\n## grp=1 333       86     44.8     37.95     54.76\n## grp=2 333       38     50.1      2.94      4.47\n## grp=3 333       23     52.1     16.23     25.17\n## \n##  Chisq= 57.3  on 2 degrees of freedom, p= 4e-13\ncal_ext_logrank$pval\n## [1] 3.581463e-13"
  },
  {
    "objectID": "calibration-lasso.html#模型比较",
    "href": "calibration-lasso.html#模型比较",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.9 模型比较",
    "text": "39.9 模型比较\n\n39.9.1 通过模型验证比较\n通过validate进行比较，也就是通过计算不同模型的time-dependent AUC进行比较。\n比如这里比较一下lasso和自适应lasso的模型效果：\n\ncmp_val &lt;- compare_by_validate(\n  x, time, event,\n  model.type = c(\"lasso\", \"alasso\"),\n  method = \"cv\", nfolds = 5, tauc.type = \"UNO\",\n  tauc.time = seq(0.25, 2, 0.25) * 365,\n  seed = 42, trace = FALSE\n)\n\n#print(cmp_val)\n#summary(cmp_val)\nplot(cmp_val) #袋外数据的结果\n##              91.25     182.5    273.75       365    456.25     547.5    638.75\n## Mean     0.4862766 0.6235047 0.6554862 0.6645661 0.6709065 0.6905955 0.6859931\n## Min      0.2039134 0.5628978 0.6125746 0.6259622 0.6288554 0.6480062 0.6258751\n## 0.25 Qt. 0.4868413 0.6095040 0.6296803 0.6518906 0.6636125 0.6740643 0.6677914\n## Median   0.5470612 0.6207525 0.6669674 0.6753767 0.6780298 0.6956816 0.7034205\n## 0.75 Qt. 0.5840546 0.6536918 0.6724515 0.6765498 0.6818180 0.6959422 0.7126816\n## Max      0.6095124 0.6706775 0.6957573 0.6930510 0.7022166 0.7392832 0.7201967\n##                730\n## Mean     0.6789647\n## Min      0.6251706\n## 0.25 Qt. 0.6258098\n## Median   0.6935016\n## 0.75 Qt. 0.7180132\n## Max      0.7323283\n##              91.25     182.5    273.75       365    456.25     547.5    638.75\n## Mean     0.4814586 0.6106500 0.6475280 0.6612647 0.6675871 0.6834418 0.6813224\n## Min      0.2479919 0.5538453 0.6009056 0.6208453 0.6310650 0.6504086 0.6354844\n## 0.25 Qt. 0.4747967 0.5918396 0.6394882 0.6539436 0.6509640 0.6548892 0.6608927\n## Median   0.5320552 0.6000809 0.6534756 0.6651832 0.6601551 0.6643520 0.6634682\n## 0.75 Qt. 0.5755143 0.6290886 0.6546811 0.6665424 0.6841356 0.6999959 0.7108894\n## Max      0.5769350 0.6783959 0.6890896 0.6998091 0.7116160 0.7475634 0.7358773\n##                730\n## Mean     0.6774869\n## Min      0.6340813\n## 0.25 Qt. 0.6386258\n## Median   0.6791506\n## 0.75 Qt. 0.6995719\n## Max      0.7360047\n\n\n\n\n内部验证的结果：\n\nplot(cmp_val, interval = TRUE)\n##              91.25     182.5    273.75       365    456.25     547.5    638.75\n## Mean     0.4862766 0.6235047 0.6554862 0.6645661 0.6709065 0.6905955 0.6859931\n## Min      0.2039134 0.5628978 0.6125746 0.6259622 0.6288554 0.6480062 0.6258751\n## 0.25 Qt. 0.4868413 0.6095040 0.6296803 0.6518906 0.6636125 0.6740643 0.6677914\n## Median   0.5470612 0.6207525 0.6669674 0.6753767 0.6780298 0.6956816 0.7034205\n## 0.75 Qt. 0.5840546 0.6536918 0.6724515 0.6765498 0.6818180 0.6959422 0.7126816\n## Max      0.6095124 0.6706775 0.6957573 0.6930510 0.7022166 0.7392832 0.7201967\n##                730\n## Mean     0.6789647\n## Min      0.6251706\n## 0.25 Qt. 0.6258098\n## Median   0.6935016\n## 0.75 Qt. 0.7180132\n## Max      0.7323283\n##              91.25     182.5    273.75       365    456.25     547.5    638.75\n## Mean     0.4814586 0.6106500 0.6475280 0.6612647 0.6675871 0.6834418 0.6813224\n## Min      0.2479919 0.5538453 0.6009056 0.6208453 0.6310650 0.6504086 0.6354844\n## 0.25 Qt. 0.4747967 0.5918396 0.6394882 0.6539436 0.6509640 0.6548892 0.6608927\n## Median   0.5320552 0.6000809 0.6534756 0.6651832 0.6601551 0.6643520 0.6634682\n## 0.75 Qt. 0.5755143 0.6290886 0.6546811 0.6665424 0.6841356 0.6999959 0.7108894\n## Max      0.5769350 0.6783959 0.6890896 0.6998091 0.7116160 0.7475634 0.7358773\n##                730\n## Mean     0.6774869\n## Min      0.6340813\n## 0.25 Qt. 0.6386258\n## Median   0.6791506\n## 0.75 Qt. 0.6995719\n## Max      0.7360047\n\n\n\n\n\n\n39.9.2 通过模型校准验证\n也就是通过校准曲线进行验证。\n\ncmp_cal &lt;- compare_by_calibrate(\n  x, time, event,\n  model.type = c(\"lasso\", \"alasso\"),\n  method = \"cv\", nfolds = 5,\n  pred.at = 365 * 9, ngroup = 5,\n  seed = 42, trace = FALSE\n)\n\n#print(cmp_cal)\n#summary(cmp_cal)\nplot(cmp_cal, xlim = c(0.3, 1), ylim = c(0.3, 1))"
  },
  {
    "objectID": "calibration-lasso.html#预测新数据",
    "href": "calibration-lasso.html#预测新数据",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.10 预测新数据",
    "text": "39.10 预测新数据\n由于在拟合模型时会自动进行超参数选择，所以得到的模型就是最终结果了。可以直接使用predict预测新数据。\n\npredict(fit, x, y, newx = x[101:105, ], pred.at = 1:10 * 365)\n##            365       730      1095      1460      1825      2190      2555\n## [1,] 0.9478345 0.9203830 0.8881359 0.8567453 0.8166540 0.7841083 0.7431914\n## [2,] 0.9605957 0.9396430 0.9148295 0.8904582 0.8590046 0.8331866 0.8003446\n## [3,] 0.9721710 0.9572351 0.9394182 0.9217779 0.8987971 0.8797459 0.8552554\n## [4,] 0.9330930 0.8983106 0.8578385 0.8188522 0.7696659 0.7302518 0.6813769\n## [5,] 0.9664734 0.9485614 0.9272704 0.9062732 0.8790447 0.8565816 0.8278538\n##           2920      3285      3650\n## [1,] 0.7039669 0.6496905 0.6496905\n## [2,] 0.7684344 0.7235347 0.7235347\n## [3,] 0.8311712 0.7967714 0.7967714\n## [4,] 0.6352566 0.5726751 0.5726751\n## [5,] 0.7997693 0.7599497 0.7599497\n\n是不是很简单呢？"
  },
  {
    "objectID": "calibration-lasso.html#shiny版",
    "href": "calibration-lasso.html#shiny版",
    "title": "39  lasso回归列线图、校准曲线、内外部验证",
    "section": "39.11 shiny版",
    "text": "39.11 shiny版\n该包还提供了shiny版本，可以通过点点点使用，感兴趣的可以试用一下：nanx.app/hdnom/"
  },
  {
    "objectID": "dca-logistic.html#方法1rmda",
    "href": "dca-logistic.html#方法1rmda",
    "title": "40  分类数据的决策曲线分析",
    "section": "40.1 方法1：rmda",
    "text": "40.1 方法1：rmda\n使用rmda包。\n\n# 先安装R包\ninstall.packages(\"rmda\")\n\n使用这个包自带的一个dcaData，作为演示，这个数据集一共500行，6列，其中Cancer是结果变量，1代表患病，0代表没病，其余列是预测变量。\n\nlibrary(rmda)\ndata(\"dcaData\")\n\ndim(dcaData) # 500,6\n## [1] 500   6\n\nhead(dcaData)\n## # A tibble: 6 × 6\n##     Age Female Smokes Marker1  Marker2 Cancer\n##   &lt;int&gt;  &lt;dbl&gt; &lt;lgl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;int&gt;\n## 1    33      1 FALSE    0.245  1.02         0\n## 2    29      1 FALSE    0.943 -0.256        0\n## 3    28      1 FALSE    0.774  0.332        0\n## 4    27      0 FALSE    0.406 -0.00569      0\n## 5    23      1 FALSE    0.508  0.208        0\n## 6    35      1 FALSE    0.186  1.41         0\n\nstr(dcaData)\n## tibble [500 × 6] (S3: tbl_df/tbl/data.frame)\n##  $ Age    : int [1:500] 33 29 28 27 23 35 34 29 35 27 ...\n##  $ Female : num [1:500] 1 1 1 0 1 1 1 1 1 1 ...\n##  $ Smokes : logi [1:500] FALSE FALSE FALSE FALSE FALSE FALSE ...\n##  $ Marker1: num [1:500] 0.245 0.943 0.774 0.406 0.508 ...\n##  $ Marker2: num [1:500] 1.02108 -0.25576 0.33184 -0.00569 0.20753 ...\n##  $ Cancer : int [1:500] 0 0 0 0 0 0 0 0 0 0 ...\n\n使用起来非常简单，首先构建dca，然后画图：\n\nset.seed(123)\n \nfit1 &lt;- decision_curve(Cancer ~ Age + Female + Smokes, # R语言里常见的公式类型\n                       data = dcaData, \n                       study.design = \"cohort\", # 选择研究类型\n                       bootstraps = 50 # 重抽样次数\n                       )\n## Note:  The data provided is used to both fit a prediction model and to estimate the respective decision curve. This may cause bias in decision curve estimates leading to over-confidence in model performance.\n\n# 画图\nplot_decision_curve(fit1, curve.names = \"fit1\",\n                    cost.benefit.axis = F, # 是否需要损失：获益比 轴\n                    confidence.intervals = \"none\" # 不画可信区间\n                    )\n\n\n\n\n可以查看模型中的各个数据的值：\n\n# 数据很大，没有展示，大家可以自己运行看看\nsummary(fit1)\n\n多个模型的多条DCA曲线一起绘制也是可以的：\n\n# 新建立1个模型\nset.seed(123)\nfit2 &lt;- decision_curve(Cancer~Age + Female + Smokes + Marker1 + Marker2,\n              data = dcaData, \n              bootstraps = 50\n              )\n## Note:  The data provided is used to both fit a prediction model and to estimate the respective decision curve. This may cause bias in decision curve estimates leading to over-confidence in model performance.\n\n# 画图只要把多个模型放在1个列表中即可，还可以进行很多自定义调整\nplot_decision_curve(list(fit1, fit2),\n                    curve.names = c(\"fit1\", \"fit2\"), \n                    xlim = c(0, 1), # 可以设置x轴范围\n                    legend.position = \"topright\", # 图例位置,\n                    col = c(\"red\",\"blue\"), # 自定义颜色\n                    confidence.intervals = \"none\",\n                    lty = c(1,2), # 线型，注意顺序\n                    lwd = c(3,2,2,1) #注意顺序，先是自己的模型，然后All,然后None\n                    )\n## Note: When multiple decision curves are plotted, decision curves for 'All' are calculated using the prevalence from the first DecisionCurve object in the list provided.\n\n\n\n\n这个包还可以绘制临床影响曲线：\n\n# 1次只能绘制1个模型\nplot_clinical_impact(fit1,\n                     population.size= 1000,\n                     cost.benefit.axis = T,\n                     n.cost.benefits= 8,\n                     col=c('red','blue'),\n                     confidence.intervals= T,\n                     ylim=c(0,1000),\n                     legend.position=\"topright\")\n\n\n\n\n这就是这个包画DCA的例子，效果还是不错的，自定义设置也很多，方便大家画出更好看的图。这个包还有很多其他功能，我们就不演示了，感兴趣的小伙伴可以自己探索哦。美中不足的是不能画生存资料的DCA。"
  },
  {
    "objectID": "dca-logistic.html#方法2dca.r",
    "href": "dca-logistic.html#方法2dca.r",
    "title": "40  分类数据的决策曲线分析",
    "section": "40.2 方法2：dca.r",
    "text": "40.2 方法2：dca.r\n使用这个网站给出的dca.r文件绘制DCA，需要代码的直接去网站下载即可。\n\n\n\n\n\n\n注意\n\n\n\n这个网站已经不再提供该代码的下载，我把dca.r/stdca.r这两段代码已经放在粉丝QQ群文件，需要的加群下载即可。\n\n\n还是使用rmda包的数据，首先我们画一个简单的DCA，结果变量是Cancer，预测变量我们只用一个Smokes。使用起来非常简单，一句代码即可：\n\nsource(\"./datasets/dca.r\")\n\ndf &lt;- as.data.frame(dcaData)\n\ndd &lt;- dca(data = df, # 指定数据集,必须是data.frame类型\n    outcome=\"Cancer\", # 指定结果变量\n    predictors=\"Smokes\", # 指定预测变量\n    probability = F # Smokes这一列是0,1组成的二分类变量，不是概率，所以是F\n    )\n\n\n\n\n但是如果你的预测变量不是0,1这种，或者有多个的话，这个函数就比较蛋疼了，它需要你先把预测概率算出来，才能使用这个函数。\n\n# 建立包含多个自变量的logistic模型\nmodel &lt;- glm(Cancer ~ Age + Female + Smokes + Marker1 + Marker2, \n            family=binomial(),\n            data = df\n            )\n\n# 算出概率\ndf$prob &lt;- predict(model, type=\"response\")\n\n# 绘制多个预测变量的DCA\ndd &lt;- dca(data=df, outcome=\"Cancer\", predictors=\"prob\", \n    probability = T,\n    xstop=0.35 # 控制x轴范围\n    )\n\n\n\n\n把多个模型画在一起的方式也有点奇怪，比如我们下面演示下3个模型画在一起，其中prob代表的是上面的model模型，Marker2代表的是只有一个预测变量Marker2的模型，Smokes代表只有一个预测变量Smokes的模型！\nprob是概率，所以是T，Smokes和Marker2不是概率，所以是F。\n\ndd &lt;- dca(data = df, outcome=\"Cancer\", \n    predictors=c(\"prob\",\"Smokes\",\"Marker2\"), # 这是3个模型哦！\n    probability = c(T,F,F) # 和上面是对应的！\n    )\n## [1] \"Smokes converted to a probability with logistic regression. Due to linearity assumption, miscalibration may occur.\"\n## [1] \"Marker2 converted to a probability with logistic regression. Due to linearity assumption, miscalibration may occur.\""
  },
  {
    "objectID": "dca-logistic.html#方法3diy",
    "href": "dca-logistic.html#方法3diy",
    "title": "40  分类数据的决策曲线分析",
    "section": "40.3 方法3：DIY",
    "text": "40.3 方法3：DIY\n上面的方法自定义选项也很少，不利于美化图形。但是呢，有一个优点就是可以直接返回画图数据，我们只要稍加修改，就能使用ggplot2画图了！而且由于直接给出了源码，我们可以试着自己修改，这样可发挥的地方就太多了！\n下面几个将要介绍的方法，都是可以返回数据的，都支持使用ggplot2画图！\n下面我们返回2个模型的画图数据，自己稍加整理，然后使用ggplot2画DCA，大家如果只有1个模型或者更多的模型，道理都是一样的哦，就是整成ggplot2需要的格式就行了！\n\n# 返回模型1的画图数据\nsource(\"./datasets/dca.r\")\ndca_data1 &lt;- dca(data = df, \n    outcome=\"Cancer\", \n    predictors=\"prob\", \n    probability = T,\n    graph = F\n    )\n\n然后提取数据，数据转换：\n\n# 转换数据\nlibrary(tidyr)\n\ndca_df1 &lt;- dca_data1$net.benefit %&gt;% # 画图数据就藏在这里！\n  # 变成长数据,还不懂长宽数据转换这个超强操作的快去翻一下历史文章！\n  pivot_longer(cols = -threshold, names_to = \"type\", values_to = \"net_benefit\") \n\n# 看下数据结构\nstr(dca_df1)\n## tibble [297 × 3] (S3: tbl_df/tbl/data.frame)\n##  $ threshold  : num [1:297] 0.01 0.01 0.01 0.02 0.02 0.02 0.03 0.03 0.03 0.04 ...\n##  $ type       : chr [1:297] \"all\" \"none\" \"prob\" \"all\" ...\n##  $ net_benefit: num [1:297] 0.111 0 0.11 0.102 0 ...\n\n画图就是非常简单了，先给大家看看只画1个模型的例子：\n\nlibrary(ggplot2)\nlibrary(ggsci)\n\n# 以prob这个模型为例\n\nggplot(dca_df1, aes(threshold, net_benefit, color = type))+\n  geom_line(linewidth = 1.2)+\n  scale_color_jama(name = \"Model Type\")+ # c(\"steelblue\",\"firebrick\",\"green4\")\n  scale_y_continuous(limits = c(-0.03,0.12),name = \"Net Benefit\")+\n  #限定y轴范围是重点，你可以去掉这句看看\n  scale_x_continuous(limits = c(0,1),name = \"Threshold Probility\")+\n  theme_bw(base_size = 16)+\n  theme(legend.position = c(0.8,0.8),\n        legend.background = element_blank()\n        )\n## Warning: Removed 85 rows containing missing values (`geom_line()`).\n\n\n\n\n看着是不是比上面的颜值都高些？是不是已经有了JAMA杂志的味道？\n下面是2个模型画在一起的例子，和上面的思路一模一样！\n\n# 构建模型2\nmod2 &lt;- glm(Cancer ~ Marker1 + Age + Smokes, df, family = binomial)\ndf$model2 &lt;- predict(mod2, type=\"response\")\n\n# 返回两个模型的画图数据\ndca12 &lt;- dca(data = df, \n             outcome=\"Cancer\", \n             predictors=c(\"prob\",\"model2\") ,\n             probability = c(T,T),\n             graph = F\n             )\n\n\n# 合并数据，大家可以打开这2个数据看下，可以直接合并\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n\ndca_df_all &lt;- dca12$net.benefit %&gt;% \n  pivot_longer(cols = -threshold,names_to = \"models\",values_to = \"net_benefit\")\n\nglimpse(dca_df_all)\n## Rows: 396\n## Columns: 3\n## $ threshold   &lt;dbl&gt; 0.01, 0.01, 0.01, 0.01, 0.02, 0.02, 0.02, 0.02, 0.03, 0.03…\n## $ models      &lt;chr&gt; \"all\", \"none\", \"prob\", \"model2\", \"all\", \"none\", \"prob\", \"m…\n## $ net_benefit &lt;dbl&gt; 0.11111111, 0.00000000, 0.10957576, 0.11111111, 0.10204082…\n\n画图也是一样的简单：\n\nggplot(dca_df_all, aes(threshold, net_benefit, color = models))+\n  #geom_line(size = 1.2)+\n  stat_smooth(method = \"loess\", se = FALSE, formula = \"y ~ x\", span = 0.2)+ \n  # 灵感来自于方法5！\n  scale_color_jama(name = \"Model Type\")+\n  scale_y_continuous(limits = c(-0.03,0.12),name = \"Net Benefit\")+\n  scale_x_continuous(limits = c(0,1),name = \"Threshold Probility\")+\n  theme_bw(base_size = 16)+\n  theme(legend.position = c(0.8,0.75),\n        legend.background = element_blank()\n        )\n## Warning: Removed 85 rows containing non-finite values (`stat_smooth()`).\n## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n## : span too small.  fewer data values than degrees of freedom.\n## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n## : pseudoinverse used at 0.00935\n## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n## : neighborhood radius 0.01065\n## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n## : reciprocal condition number 0\n## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric,\n## : There are other near singularities as well. 0.00011342\n\n\n\n\n挺好，还是不错的，能直接返回数据的都是可以高度自定义的，配合ggplot2，你可以尽情发挥。\n在今天推荐的所有方法中，这个方法我是最喜欢的，虽然只有一段代码，连个正经的R包都没有，但是很明显这个方法的潜力最大！只要你会自己修改，那这个方法就是万能的，适合很多模型的DCA绘制！"
  },
  {
    "objectID": "dca-logistic.html#方法4ggdca",
    "href": "dca-logistic.html#方法4ggdca",
    "title": "40  分类数据的决策曲线分析",
    "section": "40.4 方法4：ggDCA",
    "text": "40.4 方法4：ggDCA\n使用ggDCA包，和上面的提取数据再画图有点像，不过它给你简化了，一句代码即可，省事儿！\n\n# 安装R包，使用有问题的小伙伴请安装github版本\ninstall.packages(\"ggDCA\")\n\nremotes::install_github('yikeshu0611/ggDCA')\n\n还是使用rmda包里面自带的dcaData演示。\n\nlibrary(ggDCA)\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\n## \n## Attaching package: 'ggDCA'\n## The following object is masked _by_ '.GlobalEnv':\n## \n##     dca\nlibrary(rmda)\n\n# 构建模型\nfit1 &lt;- glm(Cancer ~ Age + Female + Smokes, data = dcaData, family = binomial())\n\nfit2 &lt;- glm(Cancer~Age + Female + Smokes + Marker1 + Marker2,\n              data = dcaData,family = binomial())\n\n画图，非常简洁！\n\nlibrary(ggplot2)\n\ndca1 &lt;- ggDCA::dca(fit1)\n\nggplot(dca1)\n## Warning: Removed 18 rows containing missing values (`geom_line()`).\n\n\n\n\n大家可以使用ggplot2语法继续修改细节，在此之前先给大家看看这个dca1的数据结构。\n\nstr(dca1)\n## Classes 'dca.lrm' and 'data.frame':  188 obs. of  5 variables:\n##  $ thresholds: num  0.021 0.0228 0.0251 0.0272 0.0298 ...\n##  $ TPR       : num  0.12 0.12 0.12 0.118 0.118 0.116 0.112 0.112 0.112 0.112 ...\n##  $ FPR       : num  0.88 0.85 0.832 0.806 0.784 0.758 0.732 0.7 0.67 0.648 ...\n##  $ NB        : num  0.1011 0.1001 0.0986 0.0955 0.0939 ...\n##  $ model     : Factor w/ 3 levels \"fit1\",\"All\",\"None\": 1 1 1 1 1 1 1 1 1 1 ...\n\n还自动算出了TPR和FPR，如果你想画ROC的话也是一句代码的事，咱就不演示了！就给大家演示下怎么自定义细节。\n\nggplot(dca1,linetype = 1,color = c(\"firebrick\",\"steelblue\",\"green4\"))+\n  theme(legend.position = c(0.8,0.75))\n## Warning: Removed 18 rows containing missing values (`geom_line()`).\n\n\n\n\n多个模型画在一起也是非常简单！\n\n# 2个模型画在一起\ndca12 &lt;- ggDCA::dca(fit1,fit2)\n\nggplot(dca12, linetype = 1,color = c(\"firebrick\",\"steelblue\",\"green4\",\"tomato\"))+\n  theme(legend.position = c(0.8,0.75))\n## Warning: Removed 131 rows containing missing values (`geom_line()`).\n\n\n\n\n简洁强大！"
  },
  {
    "objectID": "dca-logistic.html#方法5dcurves",
    "href": "dca-logistic.html#方法5dcurves",
    "title": "40  分类数据的决策曲线分析",
    "section": "40.5 方法5：dcurves",
    "text": "40.5 方法5：dcurves\n使用dcurves包。\n\n# 安装,2选1\ninstall.packages(\"dcurves\")\n\n# install.packages(\"devtools\")\ndevtools::install_github(\"ddsjoberg/dcurves\")\n\n还是使用rmda包的dcaData数据进行演示。\n和dca.r的使用很像。废话不多说了，直接上 画2个模型DCA 的代码。\n\nlibrary(dcurves)\n## \n## Attaching package: 'dcurves'\n## The following object is masked _by_ '.GlobalEnv':\n## \n##     dca\n## The following object is masked from 'package:ggDCA':\n## \n##     dca\nlibrary(rmda)\ndata(\"dcaData\")\n\n# 建立2个模型，算出概率\nmod1 &lt;- glm(Cancer ~ Marker1 + Age + Smokes, dcaData, family = binomial)\ndcaData$model1 &lt;- predict(mod1, type=\"response\")\n\nmod2 &lt;- glm(Cancer ~ Marker1 + Marker2 + Age + Smokes + Female, \n            dcaData, family = binomial)\ndcaData$model2 &lt;- predict(mod2, type=\"response\")\n  \ndcurves::dca(Cancer ~ model1 + model2,\n             data = dcaData\n             ) %&gt;% \n  plot(smooth = T,\n       show_ggplot_code = T # 显示ggplot2代码，方便大家自己调整\n       )\n## Assuming '1' is [Event] and '0' is [non-Event]\n## # ggplot2 code to create DCA figure -------------------------------\n## as_tibble(x) %&gt;%\n##   dplyr::filter(!is.na(net_benefit)) %&gt;%\n##   ggplot(aes(x = threshold, y = net_benefit, color = label)) +\n##   stat_smooth(method = \"loess\", se = FALSE, formula = \"y ~ x\", \n##     span = 0.2) +\n##   coord_cartesian(ylim = c(-0.012, 0.12)) +\n##   scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n##   labs(x = \"Threshold Probability\", y = \"Net Benefit\", color = \"\") +\n##   theme_bw()\n\n\n\n\n大家看到ggplot2的代码了吧？自己调整就可以了。\n今天就给大家简单介绍下logistic回归DCA的5种画法，其实还有很多，留着以后再介绍吧，实在是写不动了。。除了常见的logistic、cox，其实随机森林、决策树、lasso、xgboost、SVM等很多模型都是可以绘制DCA的，更多进阶内容可以关注公众号查看。"
  },
  {
    "objectID": "dca-cox.html#方法1dcurves",
    "href": "dca-cox.html#方法1dcurves",
    "title": "41  生存数据的决策曲线分析",
    "section": "41.1 方法1：dcurves",
    "text": "41.1 方法1：dcurves\n使用dcurves包,使用的数据集是包自带的df_surv数据集，一共有750行，9列，其中ttcancer是时间，cancer是结局事件，TRUE代表有癌症，FALSE代表没有癌症。\n并不是只有结局事件是生存或者死亡的才叫生存资料哦！只要是time-event类型的，都可以。\n\nrm(list = ls())\n# 加载R包和数据，不知道怎么安装的请看我前面的推文\nlibrary(dcurves)\nlibrary(survival)\ndata(\"df_surv\")\n\n# 查看数据结构\ndim(df_surv)\n## [1] 750   9\nstr(df_surv)\n## tibble [750 × 9] (S3: tbl_df/tbl/data.frame)\n##  $ patientid       : num [1:750] 1 2 3 4 5 6 7 8 9 10 ...\n##  $ cancer          : logi [1:750] FALSE FALSE FALSE FALSE FALSE FALSE ...\n##  $ ttcancer        : num [1:750] 3.009 0.249 1.59 3.457 3.329 ...\n##  $ risk_group      : chr [1:750] \"low\" \"high\" \"low\" \"low\" ...\n##  $ age             : num [1:750] 64 78.5 64.1 58.5 64 ...\n##  $ famhistory      : num [1:750] 0 0 0 0 0 0 0 0 0 0 ...\n##  $ marker          : num [1:750] 0.7763 0.2671 0.1696 0.024 0.0709 ...\n##  $ cancerpredmarker: num [1:750] 0.0372 0.57891 0.02155 0.00391 0.01879 ...\n##  $ cancer_cr       : Factor w/ 3 levels \"censor\",\"diagnosed with cancer\",..: 1 1 1 1 1 1 1 2 1 1 ...\n\n这个包使用起来很别扭，但是可以说它很灵活！\n如果预测变量只有1个，且是0,1表示的，那就很简单，直接用就行；如果有多个预测变量，就需要先计算出预测概率，然后才能使用。\n预测变量是famhistory，这是0,1表示的二分类变量：\n\nlibrary(ggplot2)\n\ndcurves::dca(Surv(ttcancer, cancer) ~ famhistory,\n             data = df_surv,\n             time = 1 # 时间选1年\n             ) %&gt;% \n  plot(smooth = T)\n\n\n\n\n下面展示一个把多个模型的DCA画在一起的例子，和之前介绍的dca.r的用法优点类似。\ncancerpredmarker这一列已经是概率了，marker是数值型的连续性变量，famhistory是0,1表示的二分类变量。\n\ndcurves::dca(Surv(ttcancer, cancer) ~ cancerpredmarker + marker + famhistory,\n    data = df_surv,\n    as_probability = \"marker\", # 只有marker需要转换成概率\n    time = 1,\n    label = list(cancerpredmarker = \"Prediction Model\", marker = \"Biomarker\")) %&gt;%\n  plot(smooth = TRUE,show_ggplot_code = T) +\n  ggplot2::labs(x = \"Treatment Threshold Probability\")\n## # ggplot2 code to create DCA figure -------------------------------\n## as_tibble(x) %&gt;%\n##   dplyr::filter(!is.na(net_benefit)) %&gt;%\n##   ggplot(aes(x = threshold, y = net_benefit, color = label)) +\n##   stat_smooth(method = \"loess\", se = FALSE, formula = \"y ~ x\", \n##     span = 0.2) +\n##   coord_cartesian(ylim = c(-0.0147287067742928, 0.147287067742928\n## )) +\n##   scale_x_continuous(labels = scales::percent_format(accuracy = 1)) +\n##   labs(x = \"Threshold Probability\", y = \"Net Benefit\", color = \"\") +\n##   theme_bw()\n\n\n\n\n可以看到marker这个曲线有点过分了。。结果也给出了ggplot2的代码，大家可以自己修改。\n上面是多个模型在同一个时间点的DCA曲线，如果是同一个模型在不同时间点的DCA，这个包不能直接画出，需要自己整理数据，因为不同时间点进行治疗的风险和获益都是不一样的，所以会出现同一个阈值概率对应多个净获益的情况，所以none和all每个概率阈值下都有1套数据。\n如果你的预测变量是多个，就需要先计算预测概率。\n\n# 构建一个多元cox回归\ncox_model &lt;- coxph(Surv(ttcancer, cancer) ~ age + famhistory + marker, data = df_surv)\n\n# 计算1.5年的概率\ndf_surv$prob1 &lt;- c(1-(summary(survfit(cox_model, newdata=df_surv), times=1.5)$surv))\n\n# 我们分2步，先获取数据，再用ggplot2画图\nx1 &lt;- dcurves::dca(Surv(ttcancer, cancer) ~ prob1,\n    data = df_surv,\n    time = 1.5\n    )%&gt;% \n  dcurves::as_tibble()\n\n# 使用自带的画图代码\nggplot(x1, aes(x=threshold, y=net_benefit,color=variable))+\n  stat_smooth(method = \"loess\", se = FALSE, formula = \"y ~ x\", span = 0.2) +\n  coord_cartesian(ylim = c(-0.03, 0.25)) +\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1)) +\n  labs(x = \"Threshold Probability\", y = \"Net Benefit\", color = \"\") +\n  theme_bw()\n## Warning: Removed 20 rows containing non-finite values (`stat_smooth()`).\n\n\n\n\n大家还可以根据自己的喜好继续调整细节。"
  },
  {
    "objectID": "dca-cox.html#方法2ggdca",
    "href": "dca-cox.html#方法2ggdca",
    "title": "41  生存数据的决策曲线分析",
    "section": "41.2 方法2：ggDCA",
    "text": "41.2 方法2：ggDCA\n使用ggDCA包。是这么多方法里面最简单的一个。对于同一个模型多个时间点、同一个时间点多个模型，都可以非常简单的画出来。\n如果遇到报错：no points selected for one or more curves, consider using …，请安装GitHub版本的ggDCA包，且不要同时加载其它可以做DCA的R包。\n还是使用dcurves里面的df_surv数据集作为演示。\n\nlibrary(ggDCA)\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\n## \n## Attaching package: 'ggDCA'\n## The following object is masked from 'package:dcurves':\n## \n##     dca\n\n# 建立多个模型\ncox_fit1 &lt;- coxph(Surv(ttcancer, cancer) ~ famhistory+marker, \n                  data = df_surv)\ncox_fit2 &lt;- coxph(Surv(ttcancer, cancer) ~ age + famhistory + marker, data = df_surv)\ncox_fit3 &lt;- coxph(Surv(ttcancer, cancer) ~ age + famhistory, data = df_surv)\n\n多个模型同一时间点的DCA：\n\ndf1 &lt;- ggDCA::dca(cox_fit1, cox_fit2, cox_fit3,\n                  times = 1.5 # 1.5年，默认值是中位数\n                  )\n\nlibrary(ggsci)\n\nggplot(df1,linetype = F)+\n  scale_color_jama(name=\"Model Type\",labels=c(\"Cox 1\",\"Cox 2\",\"Cox 3\",\"All\",\"None\"))+\n  theme_bw(base_size = 14)+\n  theme(legend.position = c(0.8,0.75),\n        legend.background = element_blank()\n        )\n## Warning: Removed 498 rows containing missing values (`geom_line()`).\n\n\n\n\n同一个模型多个时间的DCA：\n\ndf2 &lt;- ggDCA::dca(cox_fit2,\n                  times = c(1,2,3)\n                  )\n\nggplot(df2,linetype = F)+\n  scale_color_jama(name=\"Model Type\")+\n  theme_bw()+\n  facet_wrap(~time) # 分面展示，因为不同时间点净获益是不一样的\n## Warning: Removed 1689 rows containing missing values (`geom_line()`).\n\n\n\n\n多个模型多个时间点：\n\ndf3 &lt;- ggDCA::dca(cox_fit1,cox_fit2,cox_fit3,\n                  times = c(1,2,3)\n                  )\n\nggplot(df3,linetype = F)+\n  scale_color_jama(name=\"Model Type\")+\n  theme_bw()+\n  facet_wrap(~time)\n## Warning: Removed 1226 rows containing missing values (`geom_line()`).\n\n\n\n\n非常强！如果你不会自己搞数据，就用这个！"
  },
  {
    "objectID": "dca-cox.html#方法3stdca.r",
    "href": "dca-cox.html#方法3stdca.r",
    "title": "41  生存数据的决策曲线分析",
    "section": "41.3 方法3：stdca.R",
    "text": "41.3 方法3：stdca.R\n使用这个网站给出的stdca.r文件绘制cox的DCA，需要代码的直接去网站下载即可。\n\n\n\n\n\n\n注意\n\n\n\n这个网站已经不再提供该代码的下载，我很早之前就下载过了，所以我把dca.r/stdca.r这两段代码放在粉丝QQ群文件，需要的加群下载即可（免费的，别问我怎么加群）。\n但是原网站下载的stdca.r脚本在某些数据中会遇到以下报错：Error in findrow(fit,times,extend):no points selected for one or more curves, consider using the extend argument，所以我对这段脚本进行了修改，可以解决这个报错。但是需要付费获取，获取链接：适用于一切模型的DCA，没有任何答疑服务，介意勿扰。\n\n\n数据还是用df_surv数据集。\n\nrm(list = ls())\nlibrary(survival)\nlibrary(dcurves)\ndata(\"df_surv\")\n\n# 加载函数,这个是我修改过的\n# 原函数有时会报错:no points selected for one or more curves, consider using...\n# 获取方式：https://mp.weixin.qq.com/s/TZ7MSaPZZ0Pwomyp_7wqFw\nsource(\"E:/R/r-clinical-model/000files/stdca.R\") \n\n# 构建一个多元cox回归\ndf_surv$cancer &lt;- as.numeric(df_surv$cancer) # stdca函数需要结果变量是0,1\ndf_surv &lt;- as.data.frame(df_surv) # stdca函数只接受data.frame\n\ncox_model &lt;- coxph(Surv(ttcancer, cancer) ~ age + famhistory + marker, data = df_surv)\n\n# 计算1.5年的概率\ndf_surv$prob1 &lt;- c(1-(summary(survfit(cox_model, newdata=df_surv), times=1.5)$surv))\n\n# 这个函数我修改过，如果你遇到报错，可以通过添加参数 xstop=0.5 解决\ndd &lt;- stdca(data=df_surv, \n      outcome=\"cancer\", \n      ttoutcome=\"ttcancer\", \n      timepoint=1.5, \n      predictors=\"prob1\",\n      smooth=TRUE\n    )\n\n\n\n\n多个模型在同一个时间点的DCA画法，和第一种方法很类似，也是要分别计算出每个模型的概率。\n\n# 建立多个模型\ncox_fit1 &lt;- coxph(Surv(ttcancer, cancer) ~ famhistory+marker, \n                  data = df_surv)\ncox_fit2 &lt;- coxph(Surv(ttcancer, cancer) ~ age + famhistory + marker, data = df_surv)\ncox_fit3 &lt;- coxph(Surv(ttcancer, cancer) ~ age + famhistory, data = df_surv)\n\n# 计算每个模型的概率\ndf_surv$prob1 &lt;- c(1-(summary(survfit(cox_fit1, newdata=df_surv), times=1.5)$surv))\ndf_surv$prob2 &lt;- c(1-(summary(survfit(cox_fit2, newdata=df_surv), times=1.5)$surv))\ndf_surv$prob3 &lt;- c(1-(summary(survfit(cox_fit3, newdata=df_surv), times=1.5)$surv))\n\n# 画图\ndd &lt;- stdca(data=df_surv, \n      outcome=\"cancer\", \n      ttoutcome=\"ttcancer\", \n      timepoint=1.5, \n      predictors=c(\"prob1\",\"prob2\",\"prob3\"),  \n      smooth=TRUE\n    )"
  },
  {
    "objectID": "dca-cox.html#方法4diy",
    "href": "dca-cox.html#方法4diy",
    "title": "41  生存数据的决策曲线分析",
    "section": "41.4 方法4：DIY",
    "text": "41.4 方法4：DIY\n返回画图数据，再用ggplot2画图：\n\ncox_dca &lt;- stdca(data = df_surv, \n      outcome = \"cancer\", \n      ttoutcome = \"ttcancer\", \n      timepoint = 1.5, \n      predictors = c(\"prob1\",\"prob2\",\"prob3\"),\n      smooth=TRUE,\n      graph = FALSE\n    )\n\nlibrary(tidyr)\n\ncox_dca_df &lt;- cox_dca$net.benefit %&gt;% \n  pivot_longer(cols = c(all,none,contains(\"sm\")),names_to = \"models\",\n               values_to = \"net_benefit\"\n               )\n\n使用ggplot2画图：\n\nlibrary(ggplot2)\nlibrary(ggsci)\n\nggplot(cox_dca_df, aes(x=threshold,y=net_benefit))+\n  geom_line(aes(color=models),linewidth=1.2)+\n  scale_color_jama(name=\"Models Types\",\n                   labels=c(\"All\",\"None\",\"Model1\",\"Model2\",\"Model3\"))+\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1),\n                     name=\"Threshold Probility\")+\n  scale_y_continuous(limits = c(-0.05,0.2),name=\"Net Benefit\")+\n  theme_bw(base_size = 14)+\n  theme(legend.background = element_blank(),\n        legend.position = c(0.85,0.75)\n        )\n## Warning: Removed 83 rows containing missing values (`geom_line()`).\n\n\n\n\n常见的DCA方法都展示了，大家自己选择使用哪个就好。"
  },
  {
    "objectID": "dca-diy.html#多个时间点多个cox模型的数据提取",
    "href": "dca-diy.html#多个时间点多个cox模型的数据提取",
    "title": "42  适用于一切模型的决策曲线分析",
    "section": "42.1 多个时间点多个cox模型的数据提取",
    "text": "42.1 多个时间点多个cox模型的数据提取\n其实ggDCA包完全可以做到，只要1行代码就搞定了，而且功能还很丰富。\n我给大家演示一遍基于stdca.r的方法，给大家开阔思路，代码可能不够简洁，但是思路没问题，无非就是各种数据整理与转换。\n而且很定会有人对默认结果不满意，想要各种修改，下面介绍的这个方法非常适合自己进行各种自定义！\n\nrm(list = ls())\nlibrary(survival)\nlibrary(dcurves)\ndata(\"df_surv\")\n\n# 加载函数\n# 原函数有问题，这个是我修改过的\n# 获取方式：https://mp.weixin.qq.com/s/TZ7MSaPZZ0Pwomyp_7wqFw\nsource(\"E:/R/r-clinical-model/000files/stdca.R\") # 原函数有问题\n\n# 构建一个多元cox回归\ndf_surv$cancer &lt;- as.numeric(df_surv$cancer) # stdca函数需要结果变量是0,1\ndf_surv &lt;- as.data.frame(df_surv) # stdca函数只接受data.frame\n\n建立多个模型，计算每个模型在不同时间点的概率：\n\n# 建立多个模型\ncox_fit1 &lt;- coxph(Surv(ttcancer, cancer) ~ famhistory+marker, data = df_surv)\ncox_fit2 &lt;- coxph(Surv(ttcancer, cancer) ~ age + famhistory + marker, data = df_surv)\ncox_fit3 &lt;- coxph(Surv(ttcancer, cancer) ~ age + famhistory, data = df_surv)\n\n# 计算每个模型在不同时间点的概率\ndf_surv$prob11 &lt;- c(1-(summary(survfit(cox_fit1, newdata=df_surv), times=1)$surv))\ndf_surv$prob21 &lt;- c(1-(summary(survfit(cox_fit2, newdata=df_surv), times=1)$surv))\ndf_surv$prob31 &lt;- c(1-(summary(survfit(cox_fit3, newdata=df_surv), times=1)$surv))\n\ndf_surv$prob12 &lt;- c(1-(summary(survfit(cox_fit1, newdata=df_surv), times=2)$surv))\ndf_surv$prob22 &lt;- c(1-(summary(survfit(cox_fit2, newdata=df_surv), times=2)$surv))\ndf_surv$prob32 &lt;- c(1-(summary(survfit(cox_fit3, newdata=df_surv), times=2)$surv))\n\ndf_surv$prob13 &lt;- c(1-(summary(survfit(cox_fit1, newdata=df_surv), times=3)$surv))\ndf_surv$prob23 &lt;- c(1-(summary(survfit(cox_fit2, newdata=df_surv), times=3)$surv))\ndf_surv$prob33 &lt;- c(1-(summary(survfit(cox_fit3, newdata=df_surv), times=3)$surv))\n\n计算threshold和net benefit：\n\ncox_dca1 &lt;- stdca(data = df_surv, \n      outcome = \"cancer\", \n      ttoutcome = \"ttcancer\", \n      timepoint = 1, \n      predictors = c(\"prob11\",\"prob21\",\"prob31\"),\n      smooth=TRUE,\n      graph = FALSE\n    )\n## [1] \"prob31: No observations with risk greater than 99%, and therefore net benefit not calculable in this range.\"\n\ncox_dca2 &lt;- stdca(data = df_surv, \n      outcome = \"cancer\", \n      ttoutcome = \"ttcancer\", \n      timepoint = 2, \n      predictors = c(\"prob12\",\"prob22\",\"prob32\"),\n      smooth=TRUE,\n      graph = FALSE\n    )\n\ncox_dca3 &lt;- stdca(data = df_surv, \n      outcome = \"cancer\", \n      ttoutcome = \"ttcancer\", \n      timepoint = 3, \n      predictors = c(\"prob13\",\"prob23\",\"prob33\"),\n      smooth=TRUE,\n      graph = FALSE\n    )\n\n\nlibrary(tidyr)\nlibrary(dplyr)\n## \n## Attaching package: 'dplyr'\n## The following objects are masked from 'package:stats':\n## \n##     filter, lag\n## The following objects are masked from 'package:base':\n## \n##     intersect, setdiff, setequal, union\n\n\n42.1.1 第一种数据整理方法\n\ncox_dca_df1 &lt;- cox_dca1$net.benefit\ncox_dca_df2 &lt;- cox_dca2$net.benefit\ncox_dca_df3 &lt;- cox_dca3$net.benefit\n\nnames(cox_dca_df1)[2] &lt;- \"all1\"\nnames(cox_dca_df2)[2] &lt;- \"all2\"\nnames(cox_dca_df3)[2] &lt;- \"all3\"\n\ntmp &lt;- cox_dca_df1 %&gt;% \n  left_join(cox_dca_df2) %&gt;% \n  left_join(cox_dca_df3) %&gt;% \n  pivot_longer(cols = contains(c(\"all\",\"sm\",\"none\")),\n               names_to = \"models\",\n               values_to = \"net_benefit\"\n               )\n## Joining with `by = join_by(threshold, none)`\n## Joining with `by = join_by(threshold, none)`\n\n画图：\n\nlibrary(ggplot2)\nlibrary(ggsci)\n\nggplot(tmp, aes(x=threshold,y=net_benefit))+\n  geom_line(aes(color=models),linewidth=1.2)+\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1),\n                     name=\"Threshold Probility\")+\n  scale_y_continuous(limits = c(-0.05,0.3),name=\"Net Benefit\")+\n  theme_bw(base_size = 14)\n## Warning: Removed 235 rows containing missing values (`geom_line()`).\n\n\n\n\n\n\n42.1.2 第二种数据整理方法\n\ncox_dca_df1 &lt;- cox_dca1$net.benefit\ncox_dca_df2 &lt;- cox_dca2$net.benefit\ncox_dca_df3 &lt;- cox_dca3$net.benefit\n\ncox_dca_long_df1 &lt;- cox_dca_df1 %&gt;% \n  rename(mod1 = prob11_sm,\n         mod2 = prob21_sm,\n         mod3 = prob31_sm\n         ) %&gt;% \n  select(-4:-6) %&gt;% \n  mutate(time = \"1\") %&gt;% \n  pivot_longer(cols = c(all,none,contains(\"mod\")),names_to = \"models\",\n               values_to = \"net_benefit\"\n               )\n\ncox_dca_long_df2 &lt;- cox_dca_df2 %&gt;% \n  rename(mod1 = prob12_sm,\n         mod2 = prob22_sm,\n         mod3 = prob32_sm\n         ) %&gt;% \n  select(-4:-6) %&gt;% \n  mutate(time = \"2\") %&gt;% \n  pivot_longer(cols = c(all,none,contains(\"mod\")),names_to = \"models\",\n               values_to = \"net_benefit\"\n               )\n\n\ncox_dca_long_df3 &lt;- cox_dca_df3 %&gt;% \n  rename(mod1 = prob13_sm,\n         mod2 = prob23_sm,\n         mod3 = prob33_sm\n         ) %&gt;% \n  select(-4:-6) %&gt;% \n  mutate(time = \"3\") %&gt;% \n  pivot_longer(cols = c(all,none,contains(\"mod\")),names_to = \"models\",\n               values_to = \"net_benefit\"\n               )\n\ntes &lt;- bind_rows(cox_dca_long_df1,cox_dca_long_df2,cox_dca_long_df3)\n\n画图：\n\nggplot(tes,aes(x=threshold,y=net_benefit))+\n  geom_line(aes(color=models,linetype=time),linewidth=1.2)+\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1),\n                     name=\"Threshold Probility\")+\n  scale_y_continuous(limits = c(-0.05,0.3),name=\"Net Benefit\")+\n  theme_bw(base_size = 14)\n## Warning: Removed 235 rows containing missing values (`geom_line()`).\n\n\n\n\n这种方法可以分面。\n\nggplot(tes,aes(x=threshold,y=net_benefit))+\n  geom_line(aes(color=models),linewidth=1.2)+\n  scale_y_continuous(limits = c(-0.05,0.3),name=\"Net Benefit\")+\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1),\n                     name=\"Threshold Probility\")+\n  scale_y_continuous(limits = c(-0.05,0.3),name=\"Net Benefit\")+\n  theme_bw(base_size = 14)+\n  facet_wrap(~time)\n## Scale for y is already present.\n## Adding another scale for y, which will replace the existing scale.\n## Warning: Removed 73 rows containing missing values (`geom_line()`).\n\n\n\n\n接下来演示其他模型的DCA实现方法，这里就以二分类变量为例，生存资料的DCA也是一样的，就是需要一个概率而已！"
  },
  {
    "objectID": "dca-diy.html#lasso回归",
    "href": "dca-diy.html#lasso回归",
    "title": "42  适用于一切模型的决策曲线分析",
    "section": "42.2 lasso回归",
    "text": "42.2 lasso回归\n\nrm(list = ls())\nsuppressMessages(library(glmnet))\nsuppressPackageStartupMessages(library(tidyverse))\n\n准备数据，这是从TCGA下载的一部分数据，其中sample_type是样本类型，1代表tumor，0代表normal，我们首先把因变量变为0,1。然后划分训练集和测试集。\n\ndf &lt;- readRDS(file = \"./datasets/df_example.rds\")\n\ndf &lt;- df %&gt;% \n  select(-c(2:3)) %&gt;% \n  mutate(sample_type = ifelse(sample_type==\"Tumor\",1,0))\n\nind &lt;- sample(1:nrow(df),nrow(df)*0.6)\n\ntrain_df &lt;- df[ind,]\ntest_df &lt;- df[-ind,]\n\n构建lasso回归需要的参数值。\n\nx &lt;- as.matrix(train_df[,-1])\ny &lt;- train_df$sample_type\n\n在训练集建立lasso回归模型：\n\ncvfit = cv.glmnet(x, y, family = \"binomial\")\nplot(cvfit)\n\n\n\n\n在测试集上查看模型表现：\n\nprob_lasso &lt;- predict(cvfit,\n                      newx = as.matrix(test_df[,-1]),\n                      s=\"lambda.1se\",\n                      type=\"response\") #返回概率\n\n然后进行DCA，也是基于测试集的：\n\nsource(\"./datasets/dca.r\")\n\ntest_df$lasso &lt;- prob_lasso\n\ndf_lasso &lt;- dca(data = test_df, # 指定数据集,必须是data.frame类型\n    outcome=\"sample_type\", # 指定结果变量\n    predictors=\"lasso\", # 指定预测变量\n    probability = T\n    )\n\n\n\n\n这就是lasso的DCA，由于数据和模型原因，这个DCA看起来很诡异，大家千万要理解实现方法！\n\nlibrary(ggplot2)\nlibrary(ggsci)\nlibrary(tidyr)\n\ndf_lasso$net.benefit %&gt;% \n  pivot_longer(cols = -threshold, \n               names_to = \"type\", \n               values_to = \"net_benefit\") %&gt;% \n  ggplot(aes(threshold, net_benefit, color = type))+\n  geom_line(linewidth = 1.2)+\n  scale_color_jama(name = \"Model Type\")+ \n  scale_y_continuous(limits = c(-0.02,1),name = \"Net Benefit\")+ \n  scale_x_continuous(limits = c(0,1),name = \"Threshold Probility\")+\n  theme_bw(base_size = 16)+\n  theme(legend.position = c(0.2,0.3),\n        legend.background = element_blank()\n        )\n## Warning: Removed 10 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "dca-diy.html#随机森林",
    "href": "dca-diy.html#随机森林",
    "title": "42  适用于一切模型的决策曲线分析",
    "section": "42.3 随机森林",
    "text": "42.3 随机森林\n\nlibrary(ranger)\n\nrf &lt;- ranger(sample_type ~ ., data = train_df)\n\nprob_rf &lt;- predict(rf,test_df[,-1],type = \"response\")$predictions\n\ntest_df$rf &lt;- prob_rf\n\ndf_rf &lt;- dca(data = test_df, # 指定数据集,必须是data.frame类型\n    outcome=\"sample_type\", # 指定结果变量\n    predictors=\"rf\", # 指定预测变量\n    probability = T,\n    graph = F\n    )\n\n\ndf_rf$net.benefit %&gt;% \n  pivot_longer(cols = -threshold, \n               names_to = \"type\", \n               values_to = \"net_benefit\") %&gt;% \n  ggplot(aes(threshold, net_benefit, color = type))+\n  geom_line(linewidth = 1.2)+\n  scale_color_jama(name = \"Model Type\")+ \n  scale_y_continuous(limits = c(-0.02,1),name = \"Net Benefit\")+ \n  scale_x_continuous(limits = c(0,1),name = \"Threshold Probility\")+\n  theme_bw(base_size = 16)+\n  theme(legend.position = c(0.2,0.3),\n        legend.background = element_blank()\n        )\n## Warning: Removed 10 rows containing missing values (`geom_line()`)."
  },
  {
    "objectID": "dca-diy.html#logistic",
    "href": "dca-diy.html#logistic",
    "title": "42  适用于一切模型的决策曲线分析",
    "section": "42.4 logistic",
    "text": "42.4 logistic\n\nlogis &lt;- glm(sample_type ~ ., data = train_df,family = binomial())\n## Warning: glm.fit: algorithm did not converge\n## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nprob_logis &lt;- predict(logis, test_df[,-1],type = \"response\")\n\n\ntest_df$logis &lt;- prob_logis\n\ndf_logis &lt;- dca(data = test_df, # 指定数据集,必须是data.frame类型\n    outcome=\"sample_type\", # 指定结果变量\n    predictors=\"logis\", # 指定预测变量\n    probability = T,\n    graph = T\n    )\n\n\n\n\n还有其他比如支持向量机等，就不一一介绍了，实现原理都是一样的，就是需要一个概率而已。"
  },
  {
    "objectID": "DCA彩色条带.html",
    "href": "DCA彩色条带.html",
    "title": "43  决策曲线添加彩色条带",
    "section": "",
    "text": "前段时间给大家介绍了如何给列线图增加彩色的危险分层条带，详情请见：列线图增加彩色风险分层\n\n并且还介绍了如何根据列线图计算每个患者的得分，并根据得分进行最佳危险分层，请见：根据列线图计算分数并进行危险分层\n今天介绍如何为决策曲线添加彩色条带，实现类似于下图的效果：\n\n这张列线图来自于这篇文章：Nomograms in oncology: more than meets the eye，DOI：10.1016/S1470-2045(14)71116-7\n这张图在底部增加了两种颜色的条带，分别是Nomogram relevant和Nomogram not relevant。\n通常在解读列线图时，我们需要汇报到底在哪个阈值区间内，我们的模型是更有临床意义的，在这个区间内也就是上图中的Nomogram relevant，不在这个区间内就是Nomogram not relevant。\n理解这点之后，画图就很简单了。\n首先我们看看常规的决策曲线画法。先看看默认的出图：\n\nrm(list = ls())\nlibrary(survival)\nlibrary(dcurves)\ndata(\"df_surv\")\n\n# 加载函数\nsource(\"E:/R/r-clinical-model/000files/stdca.R\") \n\n# 构建一个多元cox回归\ndf_surv$cancer &lt;- as.numeric(df_surv$cancer) # stdca函数需要结果变量是0,1\ndf_surv &lt;- as.data.frame(df_surv) # stdca函数只接受data.frame\n\ncox_model &lt;- coxph(Surv(ttcancer, cancer) ~ famhistory + marker, data = df_surv)\n\n# 计算1.5年的概率\ndf_surv$prob1 &lt;- c(1-(summary(survfit(cox_model, newdata=df_surv), times=1.5)$surv))\n\n# 这个函数我修改过，如果你遇到报错，可以通过添加参数 xstop=0.5 解决\ncox_dca &lt;- stdca(data=df_surv, \n      outcome=\"cancer\", \n      ttoutcome=\"ttcancer\", \n      timepoint=1.5, \n      predictors=\"prob1\",\n      smooth=TRUE,\n      xby = 0.001,\n      ymin = -0.1\n    )\n\n\n\n\n这个默认的出图不是很好看，我们可以自己提取数据，然后使用ggplot2画：\n\nlibrary(tidyr)\n\ncox_dca_df &lt;- cox_dca$net.benefit %&gt;% \n  pivot_longer(cols = c(all,none,prob1_sm),names_to = \"models\",\n               values_to = \"net_benefit\"\n               )\n\n使用ggplot2画图：\n\nlibrary(ggplot2)\nlibrary(ggsci)\n\np1 &lt;- ggplot(cox_dca_df)+\n  geom_line(aes(x=threshold,y=net_benefit,color=models),linewidth=1.2)+\n  scale_color_jama(name=\"Models Types\",\n                   labels=c(\"All\",\"None\",\"Model1\"))+\n  scale_x_continuous(labels = scales::label_percent(accuracy = 1),\n                     name=\"Threshold Probility\")+\n  scale_y_continuous(limits = c(-0.1,0.2),name=\"Net Benefit\")+\n  theme_bw(base_size = 14)+\n  theme(legend.background = element_blank(),\n        legend.position = c(0.85,0.75)\n        )\np1\n## Warning: Removed 731 rows containing missing values (`geom_line()`).\n\n\n\n\n添加彩色条带的思路也很简单，就是在合适的位置添加矩形色块即可。\n首先我们查看下这个合适的阈值区间是多少。左侧的阈值应该就是在相同的横坐标下，Model1的net-benefit开始大于All的位置，右侧阈值应该是Model1的net-benefit开始小于0的位置。\n我们直接查看下数据即可，最简单的方法，肉眼看一下。前面在使用ggplot2画图时已经把数据都提取好了，就是cox_dca_df这个数据。\n左侧的阈值大概在0.1左右，右侧的阈值大概在0.7左右。因为没有恰好相等的情况，所以只能取个大概的阈值。\n左侧阈值：\n\n右侧阈值，0.707的净获益还是正数，0.708的净获益就是负数了，所以这个阈值应该是在0.707~0.708之间，我们就取个大约的数字，0.7。\n\n有了这两个阈值就是有了矩形的坐标，所以下面再准备3个矩形的坐标即可，如果是ggplot2画的图，就用geom_rect()添加矩形。\n\n# 准备3个矩形的坐标\nrect_df1 &lt;- data.frame(xmin = 0,\n                      xmax = 0.1,\n                      ymin = -0.05,\n                      ymax = -0.1\n                      )\nrect_df2 &lt;- data.frame(xmin = 0.1,\n                      xmax = 0.7,\n                      ymin = -0.05,\n                      ymax = -0.1\n                      )\nrect_df3 &lt;- data.frame(xmin = 0.7,\n                      xmax = 1,\n                      ymin = -0.05,\n                      ymax = -0.1\n                      )\n\n解释说明的文字直接使用annotate()函数添加即可：\n\np2 &lt;- p1+\n  geom_rect(data = rect_df1, \n            mapping = aes(xmin = xmin, xmax=xmax,ymin=ymin,ymax=ymax),\n            fill = \"#0151a2\")+\n  geom_rect(data = rect_df2, \n            mapping = aes(xmin = xmin, xmax=xmax,ymin=ymin,ymax=ymax),\n            fill = \"#c01e35\")+\n  geom_rect(data = rect_df3, \n            mapping = aes(xmin = xmin, xmax=xmax,ymin=ymin,ymax=ymax),\n            fill = \"#0151a2\")+\n  annotate(geom = \"text\", label = \"Nomogram relevant\", \n           x = 0.4, y = -0.075,\n           size = 4.5, color = \"white\")+\n  annotate(geom = \"text\", label = \"Nomogram irrelevant\", \n           x = 0.85, y = -0.075,\n           size = 4.5, color = \"white\")\np2\n## Warning: Removed 731 rows containing missing values (`geom_line()`).\n\n\n\n\n是不是很easy呢？\n还可以再添加2条竖线，使图形更容易理解：\n\n# 准备2条竖线坐标\nline_df &lt;- data.frame(x = c(0.1,0.7),\n                      ymin = c(-0.05,-0.05),\n                      ymax = c(0.13,0)\n                      )\n\n添加竖线：\n\np2+geom_linerange(data = line_df, mapping = aes(x=x,ymin=ymin,ymax=ymax),\n                  linetype = 2, linewidth = 1.2\n                  )\n## Warning: Removed 731 rows containing missing values (`geom_line()`).\n\n\n\n\n如果是默认的出图，那就更简单了，因为都是R语言默认的画图引擎，支持所有base r画图语法，和之前列线图添加彩色条带的方法一模一样：\n\ncox_dca &lt;- stdca(data=df_surv, \n      outcome=\"cancer\", \n      ttoutcome=\"ttcancer\", \n      timepoint=1.5, \n      predictors=\"prob1\",\n      smooth=TRUE,\n      xby = 0.001,\n      ymin = -0.1\n    )\n# 添加条带\nrect(xleft = 0,xright = 0.1,ybottom = -0.05,ytop = -0.1,col = \"#0151a2\",border = NA)\nrect(xleft = 0.1,xright = 0.7,ybottom = -0.05,ytop = -0.1,col = \"#c01e35\",border = NA)\nrect(xleft = 0.7,xright = 1,ybottom = -0.05,ytop = -0.1,col = \"#0151a2\",border = NA)\n\n# 添加文字\ntext(x = 0.4, y = -0.075, labels=\"Nomogram relevant\",col=\"white\")\ntext(x = 0.85, y = -0.075, labels=\"Nomogram relevant\",col=\"white\")\n\n\n\n\n搞定！"
  },
  {
    "objectID": "DCA测试集.html#生存数据测试集",
    "href": "DCA测试集.html#生存数据测试集",
    "title": "44  测试集的决策曲线分析",
    "section": "44.1 生存数据测试集",
    "text": "44.1 生存数据测试集\n使用的数据集是包自带的df_surv数据集，一共有750行，9列，其中ttcancer是时间，cancer是结局事件，TRUE代表有癌症，FALSE代表没有癌症。\n\ndata(\"df_surv\",package = \"dcurves\")\n\ndim(df_surv)\n## [1] 750   9\nstr(df_surv)\n## Classes 'tbl_df', 'tbl' and 'data.frame':    750 obs. of  9 variables:\n##  $ patientid       : num  1 2 3 4 5 6 7 8 9 10 ...\n##  $ cancer          : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n##  $ ttcancer        : num  3.009 0.249 1.59 3.457 3.329 ...\n##  $ risk_group      : chr  \"low\" \"high\" \"low\" \"low\" ...\n##  $ age             : num  64 78.5 64.1 58.5 64 ...\n##  $ famhistory      : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ marker          : num  0.7763 0.2671 0.1696 0.024 0.0709 ...\n##  $ cancerpredmarker: num  0.0372 0.57891 0.02155 0.00391 0.01879 ...\n##  $ cancer_cr       : Factor w/ 3 levels \"censor\",\"diagnosed with cancer\",..: 1 1 1 1 1 1 1 2 1 1 ...\n\n划分训练集测试集：\n\ndf_surv$cancer &lt;- as.numeric(df_surv$cancer) # stdca函数需要结果变量是0,1\ndf_surv &lt;- as.data.frame(df_surv) # stdca函数只接受data.frame\n\ntrain &lt;- sample(1:nrow(df_surv),nrow(df_surv) * 0.7)\ntrain_df &lt;- df_surv[train,]\ntest_df &lt;- df_surv[- train,]\n\ndim(train_df)\n## [1] 525   9\ndim(test_df)\n## [1] 225   9\n\n\n44.1.1 方法1：stdca.R\n我这里选择方法是stdca.R，因为这种方法比较灵活，非常适合各种DIY。但是要注意：\n\n这个网站之前可以免费下载dca.r/stdca.r这两段脚本，但是现在已经不再提供该代码的下载，我把dca.r/stdca.r这两段代码已经放在粉丝QQ群文件，需要的加群下载即可。\n但是原网站下载的stdca.r脚本在某些数据中会遇到以下报错：Error in findrow(fit,times,extend):no points selected for one or more curves, consider using the extend argument，所以我对这段脚本进行了修改，可以解决这个报错。但是需要额外付费获取，获取链接：适用于一切模型的DCA，赞赏后加我微信获取，或者加我微信转账获取。没有任何答疑服务，介意勿扰。\n另外，我在之前的推文中介绍过超多种绘制决策曲线的方法，你也可以选择其他方法。\n\n\nlibrary(survival)\n\n# 加载函数\nsource(\"E:/R/r-clinical-model/000files/stdca.R\") \n\n# 构建一个多元cox回归\ncox_model &lt;- coxph(Surv(ttcancer, cancer) ~ age + famhistory + marker, \n                   data = train_df)\n\n# 计算1.5年的事件概率\ntrain_df$prob1 &lt;- c(1-(summary(survfit(cox_model, newdata=train_df), times=1.5)$surv))\n\n# 这个函数我修改过，如果你遇到报错，可以通过添加参数 xstop=0.5 解决\naa &lt;- stdca(data=train_df, \n      outcome=\"cancer\", \n      ttoutcome=\"ttcancer\", \n      timepoint=1.5, \n      predictors=\"prob1\",\n      smooth=TRUE\n    )\n\n\n\n\n这个是训练集的决策曲线，这个是可以提取数据然后自己使用ggplot2绘制的，详情请参考上面的推文。\n下面就是测试集的决策曲线分析了，非常简单：\n\n# 计算测试集1.5年的事件概率\ntest_df$prob1 &lt;- c(1-(summary(survfit(cox_model, newdata=test_df), times=1.5)$surv))\n\n# 这个函数我修改过，如果你遇到报错，可以通过添加参数 xstop=0.5 解决\naa &lt;- stdca(data=test_df, \n      outcome=\"cancer\", \n      ttoutcome=\"ttcancer\", \n      timepoint=1.5, \n      predictors=\"prob1\",\n      smooth=TRUE\n    )\n\n\n\n\n这个就是测试集的决策曲线分析了。\n\n\n44.1.2 方法2：ggDCA\n初学者最适合的方法了。对于同一个模型多个时间点、同一个时间点多个模型，都可以非常简单的画出来。\n\nlibrary(ggDCA)\n\n# 构建一个多元cox回归\ncox_model &lt;- coxph(Surv(ttcancer, cancer) ~ age + famhistory + marker, \n                   data = train_df)\n\n1行代码解决：\n\ndf &lt;- ggDCA::dca(cox_model,\n                  times = 1.5 # 1.5年，默认值是中位数\n                  )\nggplot(df)\n## Warning: Removed 128 rows containing missing values (`geom_line()`).\n\n\n\n\n这个图也是可以美化的，具体还是参考上面的推文。这个图和我们用stdca.r画出来的图是一模一样的哈，只是图形的长宽比例和坐标轴的范围不同而已（stdca.r画出来的图还进行了平滑处理）。\n测试集也是1行代码解决：\n\ndf &lt;- ggDCA::dca(cox_model,\n                times = 1.5, # 1.5年，默认值是中位数\n                new.data = test_df\n                )\nggplot(df)\n## Warning: Removed 184 rows containing missing values (`geom_line()`).\n\n\n\n\n红色的那条是测试集的，validate是训练集的，你如果不想要可以自己提取数据画图。\n是不是很easy呢？"
  },
  {
    "objectID": "DCA测试集.html#分类数据测试集",
    "href": "DCA测试集.html#分类数据测试集",
    "title": "44  测试集的决策曲线分析",
    "section": "44.2 分类数据测试集",
    "text": "44.2 分类数据测试集\n这个数据集一共500行，6列，其中Cancer是结果变量，1代表患病，0代表没病，其余列是预测变量。\n\nrm(list = ls())\ndata(\"dcaData\",package = \"rmda\")\ndcaData &lt;- as.data.frame(dcaData)\ndim(dcaData) # 500,6\n## [1] 500   6\nstr(dcaData)\n## 'data.frame':    500 obs. of  6 variables:\n##  $ Age    : int  33 29 28 27 23 35 34 29 35 27 ...\n##  $ Female : num  1 1 1 0 1 1 1 1 1 1 ...\n##  $ Smokes : logi  FALSE FALSE FALSE FALSE FALSE FALSE ...\n##  $ Marker1: num  0.245 0.943 0.774 0.406 0.508 ...\n##  $ Marker2: num  1.02108 -0.25576 0.33184 -0.00569 0.20753 ...\n##  $ Cancer : int  0 0 0 0 0 0 0 0 0 0 ...\n\n划分训练集测试集：\n\ntrain &lt;- sample(1:nrow(dcaData), nrow(dcaData)*0.7)\ntrain_df &lt;- dcaData[train,]\ntest_df &lt;- dcaData[- train,]\ndim(train_df)\n## [1] 350   6\ndim(test_df)\n## [1] 150   6\n\n\n44.2.1 方法1：dca.r\n训练集的DCA：\n\nsource(\"./datasets/dca.r\")\n\n# 建立包含多个自变量的logistic模型\nmodel &lt;- glm(Cancer ~ Age + Female + Smokes + Marker1 + Marker2, \n            family=binomial(),\n            data = train_df\n            )\n\n# 算出概率\ntrain_df$prob &lt;- predict(model, type=\"response\")\n\n# 绘制多个预测变量的DCA\naa &lt;- dca(data=train_df, outcome=\"Cancer\", predictors=\"prob\", \n    probability = T,\n    xstop=0.35 # 控制x轴范围\n    )\n\n\n\n\n测试集的DCA:\n\n# 算出概率\ntest_df$prob &lt;- predict(model, type=\"response\", newdata = test_df)\n\n# 绘制多个预测变量的DCA\naa &lt;- dca(data=test_df, outcome=\"Cancer\", predictors=\"prob\", \n    probability = T,\n    xstop=0.35 # 控制x轴范围\n    )\n\n\n\n\n\n\n44.2.2 方法2：ggDCA\n训练集的DCA：\n\nlibrary(ggDCA)\n\n# 建立包含多个自变量的logistic模型\nmodel &lt;- glm(Cancer ~ Age + Female + Smokes + Marker1 + Marker2, \n            family=binomial(),\n            data = train_df\n            )\n\naa &lt;- ggDCA::dca(model)\n\nggplot(aa)\n## Warning: Removed 73 rows containing missing values (`geom_line()`).\n\n\n\n\n测试集的DCA：\n\naa &lt;- ggDCA::dca(model,new.data=test_df)\n\nggplot(aa)\n## Warning: Removed 120 rows containing missing values (`geom_line()`).\n\n\n\n\n红色的是测试集的DCA曲线，因为坐标范围问题，看起来不太好看，但是你可以自己提取数据重新画。这里就不演示了。\n后台回复决策曲线即可获取决策曲线推文合集；回复校准曲线即可获取校准曲线推文合集~"
  },
  {
    "objectID": "校准曲线和决策曲线的概率.html",
    "href": "校准曲线和决策曲线的概率.html",
    "title": "45  校准曲线和决策曲线的概率",
    "section": "",
    "text": "不知道大家有没有注意过，绘制生存曲线和决策曲线都要用到概率，那到底是生存概率还是死亡概率呢？\n我在学习绘制随机生存森林的校准曲线和决策曲线时，遇到了这个问题，然后认真探索了一下，发现校准曲线和决策曲线用的概率竟然是不一样的。\n画COX模型的校准曲线需要实际生存概率和预测生存概率，但是关于生存概率到底是怎么算出来的，还有算出来的到底是死亡概率还是生存概率，一直搞不清楚，所以写这篇明确一下。\n首先把数据集划分为训练集、测试集。\n\nrm(list = ls())\nlibrary(survival)\n\nlung$status &lt;- ifelse(lung$status == 2,1,0)\nlung &lt;- na.omit(lung)\n\nset.seed(123)\nind &lt;- sample(1:nrow(lung),nrow(lung)*0.7)\n\ntrain_df &lt;- lung[ind,]\ntest_df &lt;- lung[- ind, ]\n\n生存/死亡概率这种说法在某些数据集是成立的，比如这里的数据，其结局就是生存或者死亡。\n但是某些数据的结局不是“生存/死亡”这种，可能是“患病/不患病、复发/不复发”等等情况，此时再叫生存/死亡概率就不对了，所以应该明确，使用event-probability，即终点事件发生的概率，通常终点事件是用数字1表示的，这种说法更加明确，不容易出错。（event-probability有时也被称为failure-probability）\n而且生存分析作为一种在医学领域常见的方法，在其他领域应用也很广泛。\n\n生存分析（英语：Survival analysis）是指根据试验或调查得到的数据对生物或人的生存时间进行分析和推断，研究生存时间和结局与众多影响因素间关系及其程度大小的方法，也称生存率分析或存活率分析，例如生物有机体的死亡和机械系统的故障。该主题在工程学中称为可靠性理论或可靠性分析，在经济学中称为持续时间分析或持续时间建模，在社会学中称为事件历史分析。–维基百科\n\n对于lung这个数据集来说，event-probability就是死亡概率！\n下面我们通过cox模型计算训练集的生存概率，以下是2种方法：\n\ncoxph_fit &lt;- coxph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n                   data = train_df,\n                   x = T, y = T)\n\n# 生存概率，不是event-probability \nc(summary(survfit(coxph_fit), times = 100)$surv)^exp(coxph_fit$linear.predictors)\n##   [1] 0.6360808 0.9131269 0.8501491 0.7630865 0.8537127 0.8450658 0.9375469\n##   [8] 0.9032144 0.8687140 0.7228365 0.7951666 0.8733291 0.9487428 0.8544090\n##  [15] 0.8621303 0.8893680 0.7778119 0.9096080 0.9151944 0.8478416 0.8618016\n##  [22] 0.7442135 0.9244979 0.8678144 0.8184844 0.8413455 0.8541395 0.9222329\n##  [29] 0.8044921 0.8517422 0.8429372 0.9178130 0.9299924 0.9269818 0.8302337\n##  [36] 0.7211115 0.6815169 0.8089742 0.9250284 0.8259474 0.9060193 0.8827809\n##  [43] 0.9246432 0.7770621 0.7342281 0.8908638 0.8525153 0.8338191 0.8875361\n##  [50] 0.7784958 0.8967697 0.8773235 0.8527416 0.8331108 0.8530900 0.8855674\n##  [57] 0.8864006 0.8375345 0.7679824 0.9137828 0.8919230 0.9210974 0.8159934\n##  [64] 0.7815514 0.8881980 0.7694322 0.9388796 0.9078595 0.8463180 0.8917698\n##  [71] 0.7426331 0.8408116 0.8245471 0.8595367 0.7287534 0.8235475 0.8371881\n##  [78] 0.7906085 0.8396755 0.8791217 0.8724587 0.8955971 0.8426875 0.8104841\n##  [85] 0.8843599 0.8926688 0.8196833 0.9133560 0.8602637 0.8579585 0.8131528\n##  [92] 0.7767679 0.8461060 0.8504478 0.8981712 0.9002254 0.9223448 0.7819553\n##  [99] 0.8076380 0.8351175 0.8007977 0.8134723 0.9354309 0.9260623 0.6978048\n## [106] 0.9013489 0.8112944 0.6854877 0.8853881 0.8183476 0.7124062 0.8235475\n## [113] 0.9066584 0.8161488 0.8612663 0.8121238\n\nc((summary(survfit(coxph_fit, newdata=train_df), times=100)$surv))\n##   [1] 0.6360808 0.9131269 0.8501491 0.7630865 0.8537127 0.8450658 0.9375469\n##   [8] 0.9032144 0.8687140 0.7228365 0.7951666 0.8733291 0.9487428 0.8544090\n##  [15] 0.8621303 0.8893680 0.7778119 0.9096080 0.9151944 0.8478416 0.8618016\n##  [22] 0.7442135 0.9244979 0.8678144 0.8184844 0.8413455 0.8541395 0.9222329\n##  [29] 0.8044921 0.8517422 0.8429372 0.9178130 0.9299924 0.9269818 0.8302337\n##  [36] 0.7211115 0.6815169 0.8089742 0.9250284 0.8259474 0.9060193 0.8827809\n##  [43] 0.9246432 0.7770621 0.7342281 0.8908638 0.8525153 0.8338191 0.8875361\n##  [50] 0.7784958 0.8967697 0.8773235 0.8527416 0.8331108 0.8530900 0.8855674\n##  [57] 0.8864006 0.8375345 0.7679824 0.9137828 0.8919230 0.9210974 0.8159934\n##  [64] 0.7815514 0.8881980 0.7694322 0.9388796 0.9078595 0.8463180 0.8917698\n##  [71] 0.7426331 0.8408116 0.8245471 0.8595367 0.7287534 0.8235475 0.8371881\n##  [78] 0.7906085 0.8396755 0.8791217 0.8724587 0.8955971 0.8426875 0.8104841\n##  [85] 0.8843599 0.8926688 0.8196833 0.9133560 0.8602637 0.8579585 0.8131528\n##  [92] 0.7767679 0.8461060 0.8504478 0.8981712 0.9002254 0.9223448 0.7819553\n##  [99] 0.8076380 0.8351175 0.8007977 0.8134723 0.9354309 0.9260623 0.6978048\n## [106] 0.9013489 0.8112944 0.6854877 0.8853881 0.8183476 0.7124062 0.8235475\n## [113] 0.9066584 0.8161488 0.8612663 0.8121238\n\n这两种方法计算出来的生存概率是一模一样的。\n这个方法也是之前在绘制决策曲线时使用的方法：生存资料的决策曲线分析DCA\n在画COX模型的决策曲线时，需要使用概率，使用的是1-summary(xxxx)。这一点在官方的说法也是很明确的，来自stdca.r脚本的官方机构：纪念斯隆-凯特琳癌症中心：\n\n\n# failure probability \nc(1-(summary(survfit(coxph_fit, newdata=train_df), times=100)$surv))\n##   [1] 0.36391922 0.08687312 0.14985089 0.23691353 0.14628733 0.15493424\n##   [7] 0.06245312 0.09678563 0.13128605 0.27716348 0.20483344 0.12667093\n##  [13] 0.05125721 0.14559099 0.13786967 0.11063201 0.22218806 0.09039195\n##  [19] 0.08480558 0.15215836 0.13819836 0.25578648 0.07550208 0.13218564\n##  [25] 0.18151559 0.15865446 0.14586054 0.07776714 0.19550795 0.14825781\n##  [31] 0.15706277 0.08218697 0.07000765 0.07301820 0.16976631 0.27888853\n##  [37] 0.31848310 0.19102579 0.07497155 0.17405255 0.09398071 0.11721909\n##  [43] 0.07535682 0.22293786 0.26577192 0.10913621 0.14748469 0.16618091\n##  [49] 0.11246388 0.22150420 0.10323032 0.12267652 0.14725839 0.16688923\n##  [55] 0.14691005 0.11443259 0.11359944 0.16246550 0.23201760 0.08621721\n##  [61] 0.10807703 0.07890259 0.18400656 0.21844859 0.11180203 0.23056777\n##  [67] 0.06112043 0.09214054 0.15368200 0.10823019 0.25736689 0.15918841\n##  [73] 0.17545294 0.14046334 0.27124664 0.17645248 0.16281191 0.20939151\n##  [79] 0.16032453 0.12087828 0.12754126 0.10440294 0.15731251 0.18951593\n##  [85] 0.11564013 0.10733123 0.18031666 0.08664403 0.13973627 0.14204145\n##  [91] 0.18684720 0.22323209 0.15389400 0.14955219 0.10182876 0.09977463\n##  [97] 0.07765520 0.21804473 0.19236203 0.16488251 0.19920229 0.18652767\n## [103] 0.06456910 0.07393773 0.30219524 0.09865107 0.18870555 0.31451234\n## [109] 0.11461188 0.18165243 0.28759375 0.17645248 0.09334163 0.18385122\n## [115] 0.13873375 0.18787620\n\n所以，绘制决策曲线用的是event-probability，或者叫failure-probability。在以死亡为终点事件的数据中，这个概率就是死亡概率，但是某些数据的终点不是死亡，此时再叫死亡概率就容易引起混淆了。\n除此之外，pec包的predictSurvProb()函数可以计算多个模型的不同数据集的event-probability，非常方便，目前此包的大部分功能都已经转移到riskRegression包中，且predictSurvProb()的功能已经被更为强大的predictRisk替代。\n帮助文档写的很清楚，就是：Extract event probabilities…：\n\nlibrary(riskRegression)\n## Warning in .recacheSubclasses(def@className, def, env): undefined subclass\n## \"ndiMatrix\" of class \"replValueSp\"; definition not updated\n## riskRegression version 2023.09.08\n\n# event probability 这个例子是死亡概率\nc(predictRisk(coxph_fit, newdata = train_df, times = 100))\n##   [1] 0.36391922 0.08687312 0.14985089 0.23691353 0.14628733 0.15493424\n##   [7] 0.06245312 0.09678563 0.13128605 0.27716348 0.20483344 0.12667093\n##  [13] 0.05125721 0.14559099 0.13786967 0.11063201 0.22218806 0.09039195\n##  [19] 0.08480558 0.15215836 0.13819836 0.25578648 0.07550208 0.13218564\n##  [25] 0.18151559 0.15865446 0.14586054 0.07776714 0.19550795 0.14825781\n##  [31] 0.15706277 0.08218697 0.07000765 0.07301820 0.16976631 0.27888853\n##  [37] 0.31848310 0.19102579 0.07497155 0.17405255 0.09398071 0.11721909\n##  [43] 0.07535682 0.22293786 0.26577192 0.10913621 0.14748469 0.16618091\n##  [49] 0.11246388 0.22150420 0.10323032 0.12267652 0.14725839 0.16688923\n##  [55] 0.14691005 0.11443259 0.11359944 0.16246550 0.23201760 0.08621721\n##  [61] 0.10807703 0.07890259 0.18400656 0.21844859 0.11180203 0.23056777\n##  [67] 0.06112043 0.09214054 0.15368200 0.10823019 0.25736689 0.15918841\n##  [73] 0.17545294 0.14046334 0.27124664 0.17645248 0.16281191 0.20939151\n##  [79] 0.16032453 0.12087828 0.12754126 0.10440294 0.15731251 0.18951593\n##  [85] 0.11564013 0.10733123 0.18031666 0.08664403 0.13973627 0.14204145\n##  [91] 0.18684720 0.22323209 0.15389400 0.14955219 0.10182876 0.09977463\n##  [97] 0.07765520 0.21804473 0.19236203 0.16488251 0.19920229 0.18652767\n## [103] 0.06456910 0.07393773 0.30219524 0.09865107 0.18870555 0.31451234\n## [109] 0.11461188 0.18165243 0.28759375 0.17645248 0.09334163 0.18385122\n## [115] 0.13873375 0.18787620\n\n结果和上面的1-summary(xxx)计算的是一样的。\n再看一个cph模型，计算生存概率（非终点事件概率）：\n\nsuppressMessages(library(rms))\ndd &lt;- datadist(train_df)\noptions(datadist = \"dd\")\n\ncph_fit &lt;- cph(Surv(time, status) ~ age + sex + ph.ecog + ph.karno + pat.karno,\n                   data = train_df,\n                   x = T, y = T\n               ,surv = T,\n               time.inc = 100\n               )\n\nc(cph_fit$surv.summary[2,1,1]^exp(cph_fit$linear.predictors))\n##       218        22        69       163        60       227       225       205 \n## 0.6360808 0.9131269 0.8501491 0.7630865 0.8537127 0.8450658 0.9375469 0.9032144 \n##       123       124       216       125       186       135        94        38 \n## 0.8687140 0.7228365 0.7951666 0.8733291 0.9487428 0.8544090 0.8621303 0.8893680 \n##        10       211       107       111       222       141       162       102 \n## 0.7778119 0.9096080 0.9151944 0.8478416 0.8618016 0.7442135 0.9244979 0.8678144 \n##       194        45       148       203       202        99        34        72 \n## 0.8184844 0.8413455 0.8541395 0.9222329 0.8044920 0.8517422 0.8429372 0.9178130 \n##       182       185        47        90       219       195        84        21 \n## 0.9299924 0.9269818 0.8302337 0.7211115 0.6815168 0.8089742 0.9250285 0.8259475 \n##       112       131       166       213        37        53        31       109 \n## 0.9060193 0.8827809 0.9246432 0.7770621 0.7342281 0.8908638 0.8525153 0.8338191 \n##        57        65       228        81       128        26       127         9 \n## 0.8875361 0.7784958 0.8967697 0.8773235 0.8527416 0.8331108 0.8530899 0.8855674 \n##       145       177       118       190        54        43       200       226 \n## 0.8864006 0.8375345 0.7679824 0.9137828 0.8919230 0.9210974 0.8159934 0.7815514 \n##       180         7       174       179       169        71        32       122 \n## 0.8881980 0.7694322 0.9388796 0.9078595 0.8463180 0.8917698 0.7426331 0.8408116 \n##       191       208       119        48        55        42        19       144 \n## 0.8245471 0.8595367 0.7287534 0.8235475 0.8371881 0.7906085 0.8396755 0.8791217 \n##       196        85       221        93        88       188       165        50 \n## 0.8724587 0.8955971 0.8426875 0.8104841 0.8843599 0.8926688 0.8196833 0.9133560 \n##        11        70       187       142       106        64        27        83 \n## 0.8602637 0.8579586 0.8131528 0.7767679 0.8461060 0.8504478 0.8981712 0.9002254 \n##       101        73       212        35       189       173       204       168 \n## 0.9223448 0.7819553 0.8076380 0.8351175 0.8007977 0.8134723 0.9354309 0.9260623 \n##       116       160       113        39        59         8       155       159 \n## 0.6978048 0.9013489 0.8112945 0.6854876 0.8853881 0.8183476 0.7124062 0.8235475 \n##        89       149       171        18 \n## 0.9066584 0.8161488 0.8612663 0.8121238\n\n校准曲线的使用的是非终点事件概率，在lung这个数据集中就是生存概率，关于它的详细介绍，请参考推文：Cox回归校准曲线(测试集)的实现方法（下）\n参考文章：\n\npec:https://stats.stackexchange.com/questions/36015/prediction-in-cox-regression\n维基百科：https://zh.wikipedia.org/zh-cn/%E7%94%9F%E5%AD%98%E5%88%86%E6%9E%90\n生存分析：https://zhuanlan.zhihu.com/p/367879967\ndca官方文档：dca-tutorial-2015-2-26.pdf\n失效事件failure-event：https://www.jianshu.com/p/1a8ee973b45f"
  },
  {
    "objectID": "model-compare_tidymodels.html#加载数据和r包",
    "href": "model-compare_tidymodels.html#加载数据和r包",
    "title": "46  tidymodels实现多模型比较",
    "section": "46.1 加载数据和R包",
    "text": "46.1 加载数据和R包\n没有安装的R包的自己安装下~\n\nrm(list = ls())\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\ntidymodels_prefer()\n\n由于要做演示用，肯定要一份比较好的数据才能说明问题，今天用的这份数据，结果变量是一个二分类的。\n一共有91976行，26列，其中play_type是结果变量，因子型，其余列都是预测变量。\n\nall_plays &lt;- read_rds(\"./datasets/all_plays.rds\")\nglimpse(all_plays)\n## Rows: 91,976\n## Columns: 26\n## $ game_id                    &lt;dbl&gt; 2017090700, 2017090700, 2017090700, 2017090…\n## $ posteam                    &lt;chr&gt; \"NE\", \"NE\", \"NE\", \"NE\", \"NE\", \"NE\", \"NE\", \"…\n## $ play_type                  &lt;fct&gt; pass, pass, run, run, pass, run, pass, pass…\n## $ yards_gained               &lt;dbl&gt; 0, 8, 8, 3, 19, 5, 16, 0, 2, 7, 0, 3, 10, 0…\n## $ ydstogo                    &lt;dbl&gt; 10, 10, 2, 10, 7, 10, 5, 2, 2, 10, 10, 10, …\n## $ down                       &lt;ord&gt; 1, 2, 3, 1, 2, 1, 2, 1, 2, 1, 1, 2, 3, 1, 2…\n## $ game_seconds_remaining     &lt;dbl&gt; 3595, 3589, 3554, 3532, 3506, 3482, 3455, 3…\n## $ yardline_100               &lt;dbl&gt; 73, 73, 65, 57, 54, 35, 30, 2, 2, 75, 32, 3…\n## $ qtr                        &lt;ord&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n## $ posteam_score              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 7, 7, 7, 7…\n## $ defteam                    &lt;chr&gt; \"KC\", \"KC\", \"KC\", \"KC\", \"KC\", \"KC\", \"KC\", \"…\n## $ defteam_score              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0…\n## $ score_differential         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, -7, 7, 7, 7, 7, …\n## $ shotgun                    &lt;fct&gt; 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0…\n## $ no_huddle                  &lt;fct&gt; 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n## $ posteam_timeouts_remaining &lt;fct&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n## $ defteam_timeouts_remaining &lt;fct&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n## $ wp                         &lt;dbl&gt; 0.5060180, 0.4840546, 0.5100098, 0.5529816,…\n## $ goal_to_go                 &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0…\n## $ half_seconds_remaining     &lt;dbl&gt; 1795, 1789, 1754, 1732, 1706, 1682, 1655, 1…\n## $ total_runs                 &lt;dbl&gt; 0, 0, 0, 1, 2, 2, 3, 3, 3, 0, 4, 4, 4, 5, 5…\n## $ total_pass                 &lt;dbl&gt; 0, 1, 2, 2, 2, 3, 3, 4, 5, 0, 5, 6, 7, 7, 8…\n## $ previous_play              &lt;fct&gt; First play of Drive, pass, pass, run, run, …\n## $ in_red_zone                &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1…\n## $ in_fg_range                &lt;fct&gt; 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1…\n## $ two_min_drill              &lt;fct&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "model-compare_tidymodels.html#数据划分",
    "href": "model-compare_tidymodels.html#数据划分",
    "title": "46  tidymodels实现多模型比较",
    "section": "46.2 数据划分",
    "text": "46.2 数据划分\n把75%的数据用于训练集，剩下的做测试集。\n\nset.seed(20220520)\n\n# 数据划分，根据play_type分层\nsplit_pbp &lt;- initial_split(all_plays, 0.75, strata = play_type)\n\ntrain_data &lt;- training(split_pbp) # 训练集\ntest_data &lt;- testing(split_pbp) # 测试集"
  },
  {
    "objectID": "model-compare_tidymodels.html#数据预处理",
    "href": "model-compare_tidymodels.html#数据预处理",
    "title": "46  tidymodels实现多模型比较",
    "section": "46.3 数据预处理",
    "text": "46.3 数据预处理\n\npbp_rec &lt;- recipe(play_type ~ ., data = train_data)  %&gt;%\n  step_rm(half_seconds_remaining,yards_gained, game_id) %&gt;% # 移除这3列\n  step_string2factor(posteam, defteam) %&gt;%  # 变为因子类型\n  #update_role(yards_gained, game_id, new_role = \"ID\") %&gt;% \n  # 去掉高度相关的变量\n  step_corr(all_numeric(), threshold = 0.7) %&gt;% \n  step_center(all_numeric()) %&gt;%  # 中心化\n  step_zv(all_predictors())  # 去掉零方差变量"
  },
  {
    "objectID": "model-compare_tidymodels.html#建立多个模型",
    "href": "model-compare_tidymodels.html#建立多个模型",
    "title": "46  tidymodels实现多模型比较",
    "section": "46.4 建立多个模型",
    "text": "46.4 建立多个模型\n\n46.4.1 logistic\n选择模型，连接数据预处理步骤。\n\nlm_spec &lt;- logistic_reg(mode = \"classification\",engine = \"glm\")\nlm_wflow &lt;- workflow() %&gt;% \n  add_recipe(pbp_rec) %&gt;% \n  add_model(lm_spec)\n\n建立模型：\n\nfit_lm &lt;- lm_wflow %&gt;% fit(data = train_data)\n\n应用于测试集：\n\npred_lm &lt;- select(test_data, play_type) %&gt;% \n  bind_cols(predict(fit_lm, test_data, type = \"prob\")) %&gt;% \n  bind_cols(predict(fit_lm, test_data))\n\npred_lm\n## # A tibble: 22,995 × 4\n##    play_type .pred_pass .pred_run .pred_class\n##    &lt;fct&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;      \n##  1 pass           0.399     0.601 run        \n##  2 pass           0.817     0.183 pass       \n##  3 pass           0.754     0.246 pass       \n##  4 pass           0.683     0.317 pass       \n##  5 run            0.327     0.673 run        \n##  6 run            0.615     0.385 pass       \n##  7 pass           0.591     0.409 pass       \n##  8 run            0.669     0.331 pass       \n##  9 pass           0.767     0.233 pass       \n## 10 pass           0.437     0.563 run        \n## # ℹ 22,985 more rows\n\n查看模型表现：\n\n# 选择多种评价指标\nmetricsets &lt;- metric_set(accuracy, mcc, f_meas, j_index)\n\npred_lm %&gt;% metricsets(truth = play_type, estimate = .pred_class)\n## # A tibble: 4 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.724\n## 2 mcc      binary         0.423\n## 3 f_meas   binary         0.774\n## 4 j_index  binary         0.416\n\n大家最喜欢的AUC：\n\npred_lm %&gt;% roc_auc(truth = play_type, .pred_pass)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc binary         0.781\n\n可视化结果，首先是大家喜闻乐见的ROC曲线：\n\npred_lm %&gt;% roc_curve(truth = play_type, .pred_pass) %&gt;% \n  autoplot()\n\n\n\n\npr曲线：\n\npred_lm %&gt;% pr_curve(truth = play_type, .pred_pass) %&gt;% \n  autoplot()\n\n\n\n\ngain_curve：\n\npred_lm %&gt;% gain_curve(truth = play_type, .pred_pass) %&gt;% \n  autoplot()\n\n\n\n\nlift_curve：\n\npred_lm %&gt;% lift_curve(truth = play_type, .pred_pass) %&gt;% \n  autoplot()\n\n\n\n\n混淆矩阵：\n\npred_lm %&gt;% \n  conf_mat(play_type,.pred_class) %&gt;% \n  autoplot()\n\n\n\n\n\n\n46.4.2 knn\nk最近邻法，和上面的逻辑回归一模一样的流程。\n首先也是选择模型，连接数据预处理步骤：\n\nknn_spec &lt;- nearest_neighbor(mode = \"classification\", engine = \"kknn\")\n\nknn_wflow &lt;- workflow() %&gt;% \n  add_recipe(pbp_rec) %&gt;% \n  add_model(knn_spec)\n\n建立模型：\n\nlibrary(kknn)\nfit_knn &lt;- knn_wflow %&gt;% \n  fit(train_data)\n\n#saveRDS(fit_knn,file = \"datasets/fit_knn.rds\")\n\n应用于测试集：\n\npred_knn &lt;- test_data %&gt;% select(play_type) %&gt;% \n  bind_cols(predict(fit_knn, test_data, type = \"prob\")) %&gt;% \n  bind_cols(predict(fit_knn, test_data, type = \"class\"))\n#saveRDS(pred_knn, file=\"./datasets/pred_knn.rds\")\n\n查看模型表现：\n\nmetricsets &lt;- metric_set(accuracy, mcc, f_meas, j_index)\n\npred_knn %&gt;% metricsets(truth = play_type, estimate = .pred_class)\n## # A tibble: 4 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.672\n## 2 mcc      binary         0.317\n## 3 f_meas   binary         0.727\n## 4 j_index  binary         0.315\n\n\npred_knn %&gt;% roc_auc(play_type, .pred_pass)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc binary         0.718\n\n可视化模型的部分就不说了，和上面的一模一样！\n\n\n46.4.3 随机森林\n同样的流程来第3遍！\n\nrf_spec &lt;- rand_forest(mode = \"classification\") %&gt;% \n  set_engine(\"ranger\",importance = \"permutation\")\nrf_wflow &lt;- workflow() %&gt;% \n  add_recipe(pbp_rec) %&gt;% \n  add_model(rf_spec)\n\n建立模型：\n\nfit_rf &lt;- rf_wflow %&gt;% \n  fit(train_data)\n\n#saveRDS(fit_rf,file = \"datasets/fit_rf.rds\")\n\n应用于测试集：\n\npred_rf &lt;- test_data %&gt;% select(play_type) %&gt;% \n  bind_cols(predict(fit_rf, test_data, type = \"prob\")) %&gt;% \n  bind_cols(predict(fit_rf, test_data, type = \"class\"))\n\n查看模型表现：\n\npred_rf %&gt;% metricsets(truth = play_type, estimate = .pred_class)\n## # A tibble: 4 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.731\n## 2 mcc      binary         0.441\n## 3 f_meas   binary         0.774\n## 4 j_index  binary         0.439\n\n\npred_rf %&gt;% conf_mat(truth = play_type, estimate = .pred_class)\n##           Truth\n## Prediction  pass   run\n##       pass 10622  3226\n##       run   2962  6185\n\n\npred_rf %&gt;% roc_auc(play_type, .pred_pass)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc binary         0.799\n\n下面给大家手动画一个校准曲线。\n两种画法，差别不大，主要是分组方法不一样，第2种分组方法是大家常见的哦~\n\ncalibration_df &lt;- pred_rf %&gt;% \n   mutate(pass = if_else(play_type == \"pass\", 1, 0),\n          pred_rnd = round(.pred_pass, 2)\n          ) %&gt;% \n  group_by(pred_rnd) %&gt;% \n  dplyr::summarize(mean_pred = mean(.pred_pass),\n            mean_obs = mean(pass),\n            n = n()\n            )\n\nggplot(calibration_df, aes(mean_pred, mean_obs))+ \n  geom_point(aes(size = n), alpha = 0.5)+\n  geom_abline(linetype = \"dashed\")+\n  theme_minimal()\n\n\n\n\n第2种方法：\n\ncali_df &lt;- pred_rf %&gt;% \n  arrange(.pred_pass) %&gt;% \n  mutate(pass = if_else(play_type == \"pass\", 1, 0),\n         group = c(rep(1:249,each=92), rep(250,87))\n         ) %&gt;% \n  group_by(group) %&gt;% \n  dplyr::summarise(mean_pred = mean(.pred_pass),\n            mean_obs = mean(pass)\n            )\n\n\ncali_plot &lt;- ggplot(cali_df, aes(mean_pred, mean_obs))+ \n  geom_point(alpha = 0.5)+\n  geom_abline(linetype = \"dashed\")+\n  theme_minimal()\n\ncali_plot\n\n\n\n\n随机森林这种方法是可以计算变量重要性的，当然也是能把结果可视化的。\n给大家演示下如何可视化随机森林结果的变量重要性：\n\nlibrary(vip)\n\nfit_rf %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip(num_features = 10)\n\n\n\n\n\n\n46.4.4 决策树\n同样的流程来第4遍！不知道你看懂了没有。。。\n\ntree_spec &lt;- decision_tree(mode = \"classification\",engine = \"rpart\")\ntree_wflow &lt;- workflow() %&gt;% \n  add_recipe(pbp_rec) %&gt;% \n  add_model(tree_spec)\n\n建立模型：\n\nfit_tree &lt;- tree_wflow %&gt;% \n  fit(train_data)\n\n应用于测试集：\n\npred_tree &lt;- test_data %&gt;% select(play_type) %&gt;% \n  bind_cols(predict(fit_tree, test_data, type = \"prob\")) %&gt;% \n  bind_cols(predict(fit_tree, test_data, type = \"class\"))\n\n查看结果：\n\npred_tree %&gt;% roc_auc(play_type, .pred_pass)\n## # A tibble: 1 × 3\n##   .metric .estimator .estimate\n##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n## 1 roc_auc binary         0.706\n\n\npred_tree %&gt;% metricsets(truth = play_type, estimate = .pred_class)\n## # A tibble: 4 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.721\n## 2 mcc      binary         0.417\n## 3 f_meas   binary         0.770\n## 4 j_index  binary         0.411"
  },
  {
    "objectID": "model-compare_tidymodels.html#交叉验证",
    "href": "model-compare_tidymodels.html#交叉验证",
    "title": "46  tidymodels实现多模型比较",
    "section": "46.5 交叉验证",
    "text": "46.5 交叉验证\n交叉验证也是大家喜闻乐见的，就用随机森林给大家顺便演示下交叉验证。\n首先要选择重抽样方法，这里我们选择10折交叉验证：\n\nset.seed(20220520)\n\nfolds &lt;- vfold_cv(train_data, v = 10)\nfolds\n## #  10-fold cross-validation \n## # A tibble: 10 × 2\n##    splits               id    \n##    &lt;list&gt;               &lt;chr&gt; \n##  1 &lt;split [62082/6899]&gt; Fold01\n##  2 &lt;split [62083/6898]&gt; Fold02\n##  3 &lt;split [62083/6898]&gt; Fold03\n##  4 &lt;split [62083/6898]&gt; Fold04\n##  5 &lt;split [62083/6898]&gt; Fold05\n##  6 &lt;split [62083/6898]&gt; Fold06\n##  7 &lt;split [62083/6898]&gt; Fold07\n##  8 &lt;split [62083/6898]&gt; Fold08\n##  9 &lt;split [62083/6898]&gt; Fold09\n## 10 &lt;split [62083/6898]&gt; Fold10\n\n然后就是让模型在训练集上跑起来：\n\nkeep_pred &lt;- control_resamples(save_pred = T, verbose = T)\n\nset.seed(20220520)\n\nlibrary(doParallel)\n\ncl &lt;- makePSOCKcluster(12) # 加速，用12个线程\nregisterDoParallel(cl)\n\nrf_res &lt;- fit_resamples(rf_wflow, resamples = folds, control = keep_pred)\n\nstopCluster(cl)\n\n#saveRDS(rf_res,file = \"datasets/rf_res.rds\")\n\n查看模型表现：\n\nrf_res %&gt;% \n  collect_metrics(summarize = T)\n## # A tibble: 2 × 6\n##   .metric  .estimator  mean     n std_err .config             \n##   &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n## 1 accuracy binary     0.732    10 0.00157 Preprocessor1_Model1\n## 2 roc_auc  binary     0.799    10 0.00193 Preprocessor1_Model1\n\n查看具体的结果：\n\nrf_res %&gt;% collect_predictions()\n## # A tibble: 68,981 × 7\n##    id     .pred_pass .pred_run  .row .pred_class play_type .config             \n##    &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;     &lt;chr&gt;               \n##  1 Fold01      0.572    0.428      6 pass        pass      Preprocessor1_Model1\n##  2 Fold01      0.470    0.530      8 run         pass      Preprocessor1_Model1\n##  3 Fold01      0.898    0.102     22 pass        pass      Preprocessor1_Model1\n##  4 Fold01      0.915    0.0847    69 pass        pass      Preprocessor1_Model1\n##  5 Fold01      0.841    0.159     97 pass        pass      Preprocessor1_Model1\n##  6 Fold01      0.931    0.0688   112 pass        pass      Preprocessor1_Model1\n##  7 Fold01      0.727    0.273    123 pass        pass      Preprocessor1_Model1\n##  8 Fold01      0.640    0.360    129 pass        pass      Preprocessor1_Model1\n##  9 Fold01      0.740    0.260    136 pass        pass      Preprocessor1_Model1\n## 10 Fold01      0.902    0.0979   143 pass        pass      Preprocessor1_Model1\n## # ℹ 68,971 more rows\n\n可视化结果也是和上面的一模一样，就不一一介绍了，简单说下训练集的校准曲线画法，其实也是和上面一样的~\n\nres_calib_plot &lt;- collect_predictions(rf_res) %&gt;% \n  mutate(\n    pass = if_else(play_type == \"pass\", 1, 0),\n    pred_rnd = round(.pred_pass, 2)\n    ) %&gt;% \n  group_by(pred_rnd) %&gt;%\n  dplyr::summarize(\n    mean_pred = mean(.pred_pass),\n    mean_obs = mean(pass),\n    n = n()\n    ) %&gt;% \n  ggplot(aes(x = mean_pred, y = mean_obs)) +\n  geom_abline(linetype = \"dashed\") +\n  geom_point(aes(size = n), alpha = 0.5) +\n  theme_minimal() +\n  labs(\n    x = \"Predicted Pass\", \n    y = \"Observed Pass\"\n    ) +\n  coord_cartesian(\n    xlim = c(0,1), ylim = c(0, 1)\n    )\n\nres_calib_plot\n\n\n\n\n然后就是应用于测试集，并查看测试集上的表现：\n\nrf_last_fit &lt;- last_fit(rf_wflow, split_pbp)\n#saveRDS(rf_last_fit,file = \"./datasets/rf_last_fit.rds\")\n\n查看测试集表现：\n\nrf_test_res &lt;- rf_last_fit %&gt;% \n  collect_metrics()\nrf_test_res\n## # A tibble: 2 × 4\n##   .metric  .estimator .estimate .config             \n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n## 1 accuracy binary         0.730 Preprocessor1_Model1\n## 2 roc_auc  binary         0.799 Preprocessor1_Model1\n\n多种指标：\n\nmetricsets &lt;- metric_set(accuracy, mcc, f_meas, j_index)\n\ncollect_predictions(rf_last_fit) %&gt;% \n  metricsets(truth = play_type, estimate = .pred_class)\n## # A tibble: 4 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.730\n## 2 mcc      binary         0.439\n## 3 f_meas   binary         0.774\n## 4 j_index  binary         0.437"
  },
  {
    "objectID": "model-compare_tidymodels.html#roc曲线画一起",
    "href": "model-compare_tidymodels.html#roc曲线画一起",
    "title": "46  tidymodels实现多模型比较",
    "section": "46.6 ROC曲线画一起",
    "text": "46.6 ROC曲线画一起\n其实非常简单，就是把结果拼在一起画个图就行了~\n\nroc_lm &lt;- pred_lm %&gt;% roc_curve(play_type, .pred_pass) %&gt;% \n  mutate(model = \"logistic\")\n\nroc_knn &lt;- pred_knn %&gt;% roc_curve(play_type, .pred_pass) %&gt;% \n  mutate(model = \"kknn\")\n\nroc_rf &lt;- pred_rf %&gt;% roc_curve(play_type, .pred_pass) %&gt;% \n  mutate(model = \"randomforest\")\n\nroc_tree &lt;- pred_tree %&gt;% roc_curve(play_type, .pred_pass) %&gt;% \n  mutate(model = \"decision tree\")\n\n\nrocs &lt;- bind_rows(roc_lm,roc_knn,roc_rf,roc_tree) %&gt;% \n  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model))+\n  geom_path(lwd = 1.2, alpha = 0.6)+\n  geom_abline(lty = 3)+\n  coord_fixed()+\n  scale_color_brewer(palette = \"Set1\")+\n  theme_minimal()\n\nrocs\n\n\n\n\n是不是很简单呢？二分类资料常见的各种评价指标都有了，图也有了，还比较了多个模型，一举多得，tidymodels，你值得拥有！"
  },
  {
    "objectID": "model-compare_workflow.html#加载数据和r包",
    "href": "model-compare_workflow.html#加载数据和r包",
    "title": "47  workflow实现多模型比较",
    "section": "47.1 加载数据和R包",
    "text": "47.1 加载数据和R包\n首先还是加载数据和R包，和前面的一模一样的操作，数据也没变。\n\nrm(list = ls())\nsuppressPackageStartupMessages(library(tidyverse))\nsuppressPackageStartupMessages(library(tidymodels))\nlibrary(kknn)\ntidymodels_prefer()\n\nall_plays &lt;- read_rds(\"./datasets/all_plays.rds\")\n\nset.seed(20220520)\n\nsplit_pbp &lt;- initial_split(all_plays, 0.75, strata = play_type)\n\ntrain_data &lt;- training(split_pbp)\ntest_data &lt;- testing(split_pbp)"
  },
  {
    "objectID": "model-compare_workflow.html#数据预处理",
    "href": "model-compare_workflow.html#数据预处理",
    "title": "47  workflow实现多模型比较",
    "section": "47.2 数据预处理",
    "text": "47.2 数据预处理\n\npbp_rec &lt;- recipe(play_type ~ ., data = train_data)  %&gt;%\n  step_rm(half_seconds_remaining,yards_gained, game_id) %&gt;% \n  step_string2factor(posteam, defteam) %&gt;%  \n  step_corr(all_numeric(), threshold = 0.7) %&gt;% \n  step_center(all_numeric()) %&gt;%  \n  step_zv(all_predictors())"
  },
  {
    "objectID": "model-compare_workflow.html#选择模型",
    "href": "model-compare_workflow.html#选择模型",
    "title": "47  workflow实现多模型比较",
    "section": "47.3 选择模型",
    "text": "47.3 选择模型\n直接选择4个模型，你想选几个都是可以的。\n\nlm_mod &lt;- logistic_reg(mode = \"classification\",engine = \"glm\")\nknn_mod &lt;- nearest_neighbor(mode = \"classification\", engine = \"kknn\")\nrf_mod &lt;- rand_forest(mode = \"classification\", engine = \"ranger\")\ntree_mod &lt;- decision_tree(mode = \"classification\",engine = \"rpart\")"
  },
  {
    "objectID": "model-compare_workflow.html#选择重抽样方法",
    "href": "model-compare_workflow.html#选择重抽样方法",
    "title": "47  workflow实现多模型比较",
    "section": "47.4 选择重抽样方法",
    "text": "47.4 选择重抽样方法\n\nset.seed(20220520)\n\nfolds &lt;- vfold_cv(train_data, v = 10)\nfolds\n## #  10-fold cross-validation \n## # A tibble: 10 × 2\n##    splits               id    \n##    &lt;list&gt;               &lt;chr&gt; \n##  1 &lt;split [62082/6899]&gt; Fold01\n##  2 &lt;split [62083/6898]&gt; Fold02\n##  3 &lt;split [62083/6898]&gt; Fold03\n##  4 &lt;split [62083/6898]&gt; Fold04\n##  5 &lt;split [62083/6898]&gt; Fold05\n##  6 &lt;split [62083/6898]&gt; Fold06\n##  7 &lt;split [62083/6898]&gt; Fold07\n##  8 &lt;split [62083/6898]&gt; Fold08\n##  9 &lt;split [62083/6898]&gt; Fold09\n## 10 &lt;split [62083/6898]&gt; Fold10"
  },
  {
    "objectID": "model-compare_workflow.html#构建workflow",
    "href": "model-compare_workflow.html#构建workflow",
    "title": "47  workflow实现多模型比较",
    "section": "47.5 构建workflow",
    "text": "47.5 构建workflow\n这一步就是不用重复写代码的关键，把所有模型和数据预处理步骤自动连接起来。\n\nlibrary(workflowsets)\n\nfour_mods &lt;- workflow_set(list(rec = pbp_rec), \n                          list(lm = lm_mod,\n                               knn = knn_mod,\n                               rf = rf_mod,\n                               tree = tree_mod\n                               ),\n                          cross = T\n                          )\nfour_mods\n## # A workflow set/tibble: 4 × 4\n##   wflow_id info             option    result    \n##   &lt;chr&gt;    &lt;list&gt;           &lt;list&gt;    &lt;list&gt;    \n## 1 rec_lm   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 2 rec_knn  &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 3 rec_rf   &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;\n## 4 rec_tree &lt;tibble [1 × 4]&gt; &lt;opts[0]&gt; &lt;list [0]&gt;"
  },
  {
    "objectID": "model-compare_workflow.html#运行模型",
    "href": "model-compare_workflow.html#运行模型",
    "title": "47  workflow实现多模型比较",
    "section": "47.6 运行模型",
    "text": "47.6 运行模型\n首先是一些运行过程中的参数设置：\n\nkeep_pred &lt;- control_resamples(save_pred = T, verbose = T)\n\n然后就是运行4个模型（目前一直是在训练集中），我们给它加速一下：\n\nlibrary(doParallel) \n\ncl &lt;- makePSOCKcluster(12) # 加速，用12个线程\nregisterDoParallel(cl)\n\nfour_fits &lt;- four_mods %&gt;% \n  workflow_map(\"fit_resamples\",\n               seed = 0520,\n               verbose = T,\n               resamples = folds,\n               control = keep_pred\n               )\n\ni 1 of 4 resampling: rec_lm\n✔ 1 of 4 resampling: rec_lm (26.6s)\ni 2 of 4 resampling: rec_knn\n✔ 2 of 4 resampling: rec_knn (3m 44.1s)\ni 3 of 4 resampling: rec_rf\n✔ 3 of 4 resampling: rec_rf (1m 10.9s)\ni 4 of 4 resampling: rec_tree\n✔ 4 of 4 resampling: rec_tree (4.5s)\n\n#saveRDS(four_fits,file=\"datasets/four_fits.rds\")\nstopCluster(cl)\n\nfour_fits\n\n需要很长时间！大家笔记本如果内存不够可能会失败哦~"
  },
  {
    "objectID": "model-compare_workflow.html#查看结果",
    "href": "model-compare_workflow.html#查看结果",
    "title": "47  workflow实现多模型比较",
    "section": "47.7 查看结果",
    "text": "47.7 查看结果\n查看模型在训练集中的表现：\n\ncollect_metrics(four_fits)\n## # A tibble: 8 × 9\n##   wflow_id .config          preproc model .metric .estimator  mean     n std_err\n##   &lt;chr&gt;    &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n## 1 rec_lm   Preprocessor1_M… recipe  logi… accura… binary     0.724    10 1.91e-3\n## 2 rec_lm   Preprocessor1_M… recipe  logi… roc_auc binary     0.781    10 1.88e-3\n## 3 rec_knn  Preprocessor1_M… recipe  near… accura… binary     0.671    10 7.31e-4\n## 4 rec_knn  Preprocessor1_M… recipe  near… roc_auc binary     0.716    10 1.28e-3\n## 5 rec_rf   Preprocessor1_M… recipe  rand… accura… binary     0.732    10 1.48e-3\n## 6 rec_rf   Preprocessor1_M… recipe  rand… roc_auc binary     0.799    10 1.90e-3\n## 7 rec_tree Preprocessor1_M… recipe  deci… accura… binary     0.720    10 1.97e-3\n## 8 rec_tree Preprocessor1_M… recipe  deci… roc_auc binary     0.704    10 2.01e-3\n\n查看每一个预测结果，这个就不运行了，毕竟好几万行，太多了。。。\n\ncollect_predictions(four_fits)"
  },
  {
    "objectID": "model-compare_workflow.html#可视化结果",
    "href": "model-compare_workflow.html#可视化结果",
    "title": "47  workflow实现多模型比较",
    "section": "47.8 可视化结果",
    "text": "47.8 可视化结果\n直接可视化4个模型的结果，感觉比ROC曲线更好看，还给出了可信区间。\n这个图可以自己用ggplot2语法修改。\n\nfour_fits %&gt;% autoplot(metric = \"roc_auc\")+theme_bw()"
  },
  {
    "objectID": "model-compare_workflow.html#选择最好的模型用于测试集",
    "href": "model-compare_workflow.html#选择最好的模型用于测试集",
    "title": "47  workflow实现多模型比较",
    "section": "47.9 选择最好的模型用于测试集",
    "text": "47.9 选择最好的模型用于测试集\n选择表现最好的应用于测试集：\n\nrand_res &lt;- last_fit(rf_mod,pbp_rec,split_pbp)\nsaveRDS(rand_res,file = \"./datasets/rand_res.rds\")\n\n查看在测试集的模型表现：\n\nrand_res &lt;- readRDS(file = \"./datasets/rand_res.rds\")\n\n\ncollect_metrics(rand_res) # test 中的模型表现\n## # A tibble: 2 × 4\n##   .metric  .estimator .estimate .config             \n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n## 1 accuracy binary         0.731 Preprocessor1_Model1\n## 2 roc_auc  binary         0.799 Preprocessor1_Model1\n\n使用其他指标查看模型表现：\n\nmetricsets &lt;- metric_set(accuracy, mcc, f_meas, j_index)\n\ncollect_predictions(rand_res) %&gt;% \n  metricsets(truth = play_type, estimate = .pred_class)\n## # A tibble: 4 × 3\n##   .metric  .estimator .estimate\n##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n## 1 accuracy binary         0.731\n## 2 mcc      binary         0.440\n## 3 f_meas   binary         0.774\n## 4 j_index  binary         0.438\n\n可视化结果，喜闻乐见的混淆矩阵：\n\ncollect_predictions(rand_res) %&gt;% \n  conf_mat(play_type,.pred_class) %&gt;% \n  autoplot()\n\n\n\n\n喜闻乐见的ROC曲线：\n\ncollect_predictions(rand_res) %&gt;% \n  roc_curve(play_type,.pred_pass) %&gt;% \n  autoplot()\n\n\n\n\n还有非常多曲线和评价指标可选，大家可以看我之前的介绍推文~"
  },
  {
    "objectID": "model-compare_mlr3.html#加载r包",
    "href": "model-compare_mlr3.html#加载r包",
    "title": "48  mlr3实现多模型比较",
    "section": "48.1 加载R包",
    "text": "48.1 加载R包\n首先还是加载数据和R包，和之前的数据一样的。\n\nrm(list = ls())\nlibrary(mlr3verse)\n## Loading required package: mlr3\nlibrary(mlr3pipelines)\nlibrary(mlr3filters)"
  },
  {
    "objectID": "model-compare_mlr3.html#建立任务",
    "href": "model-compare_mlr3.html#建立任务",
    "title": "48  mlr3实现多模型比较",
    "section": "48.2 建立任务",
    "text": "48.2 建立任务\n然后是对数据进行划分训练集和测试集，对数据进行预处理，为了和之前的tidymodels进行比较，这里使用的数据和预处理步骤都是和之前一样的。\n\n# 读取数据\nall_plays &lt;- readRDS(\"./datasets/all_plays.rds\")\n\n# 建立任务\npbp_task &lt;- as_task_classif(all_plays, target=\"play_type\")\n\n# 数据划分\nsplit_task &lt;- partition(pbp_task, ratio=0.75)\n\ntask_train &lt;- pbp_task$clone()$filter(split_task$train)\ntask_test &lt;- pbp_task$clone()$filter(split_task$test)"
  },
  {
    "objectID": "model-compare_mlr3.html#数据预处理",
    "href": "model-compare_mlr3.html#数据预处理",
    "title": "48  mlr3实现多模型比较",
    "section": "48.3 数据预处理",
    "text": "48.3 数据预处理\n建立任务后就是建立数据预处理步骤，这里采用和上篇推文tidymodels中一样的预处理步骤：\n\n# 数据预处理\npbp_prep &lt;- po(\"select\", # 去掉3列\n               selector = selector_invert(\n                 selector_name(c(\"half_seconds_remaining\",\"yards_gained\",\"game_id\")))\n               ) %&gt;&gt;%\n  po(\"colapply\", # 把这两列变成因子类型\n     affect_columns = selector_name(c(\"posteam\",\"defteam\")),\n     applicator = as.factor) %&gt;&gt;% \n  po(\"filter\", # 去除高度相关的列\n     filter = mlr3filters::flt(\"find_correlation\"), filter.cutoff=0.3) %&gt;&gt;%\n  po(\"scale\", scale = F) %&gt;&gt;% # 中心化\n  po(\"removeconstants\") # 去掉零方差变量\n\n可以看到mlr3的数据预处理与tidymodels相比，在语法上确实是有些复杂了，而且由于使用的R6，很多语法看起来很别扭，文档也说的不清楚，对于新手来说还是tidymodels更好些。目前来说最大的优势可能就是速度了吧。。。\n如果你想把预处理步骤应用于数据，得到预处理之后的数据，可以用以下代码：\n\ntask_prep &lt;- pbp_prep$clone()$train(task_train)[[1]]\ndim(task_train$data())\n## [1] 68982    26\n\ntask_prep$feature_types\n##                             id    type\n##  1:                    defteam  factor\n##  2:              defteam_score numeric\n##  3: defteam_timeouts_remaining  factor\n##  4:                       down ordered\n##  5:                 goal_to_go  factor\n##  6:                in_fg_range  factor\n##  7:                in_red_zone  factor\n##  8:                  no_huddle  factor\n##  9:                    posteam  factor\n## 10:              posteam_score numeric\n## 11: posteam_timeouts_remaining  factor\n## 12:              previous_play  factor\n## 13:                        qtr ordered\n## 14:         score_differential numeric\n## 15:                    shotgun  factor\n## 16:                 total_pass numeric\n## 17:              two_min_drill  factor\n## 18:               yardline_100 numeric\n## 19:                    ydstogo numeric\n\n这样就得到了处理好的数据，但是对于mlr3pipelines来说，这一步做不做都可以。"
  },
  {
    "objectID": "model-compare_mlr3.html#选择多个模型",
    "href": "model-compare_mlr3.html#选择多个模型",
    "title": "48  mlr3实现多模型比较",
    "section": "48.4 选择多个模型",
    "text": "48.4 选择多个模型\n还是选择和之前一样的4个模型：逻辑回归、随机森林、决策树、k最近邻：\n\n# 随机森林\nrf_glr &lt;- as_learner(pbp_prep %&gt;&gt;% lrn(\"classif.ranger\", predict_type=\"prob\")) \nrf_glr$id &lt;- \"randomForest\"\n\n# 逻辑回归\nlog_glr &lt;-as_learner(pbp_prep %&gt;&gt;% lrn(\"classif.log_reg\", predict_type=\"prob\")) \nlog_glr$id &lt;- \"logistic\"\n\n# 决策树\ntree_glr &lt;- as_learner(pbp_prep %&gt;&gt;% lrn(\"classif.rpart\", predict_type=\"prob\")) \ntree_glr$id &lt;- \"decisionTree\"\n\n# k近邻\nkknn_glr &lt;- as_learner(pbp_prep %&gt;&gt;% lrn(\"classif.kknn\", predict_type=\"prob\")) \nkknn_glr$id &lt;- \"kknn\""
  },
  {
    "objectID": "model-compare_mlr3.html#建立benchmark_grid",
    "href": "model-compare_mlr3.html#建立benchmark_grid",
    "title": "48  mlr3实现多模型比较",
    "section": "48.5 建立benchmark_grid",
    "text": "48.5 建立benchmark_grid\n类似于tidymodels中的workflow_set。\n选择10折交叉验证，建立多个模型，语法也是很简单了。\n\nset.seed(0520)\n\n# 10折交叉验证\ncv &lt;- rsmp(\"cv\",folds=10)\n\nset.seed(0520)\n\n# 建立多个模型\ndesign &lt;- benchmark_grid(\n  tasks = task_train,\n  learners = list(rf_glr,log_glr,tree_glr,kknn_glr),\n  resampling = cv\n)\n\n在训练集中，使用10折交叉验证，运行4个模型，看这语法是不是也很简单清稀？"
  },
  {
    "objectID": "model-compare_mlr3.html#开始计算",
    "href": "model-compare_mlr3.html#开始计算",
    "title": "48  mlr3实现多模型比较",
    "section": "48.6 开始计算",
    "text": "48.6 开始计算\n下面就是开始计算，和tidymodels相比，这一块语法更加简单一点，就是建立benchmark_grid，然后使用benchmark()函数即可。\n\n# 加速\nlibrary(future)\nplan(\"multisession\",workers=12)\n\n# 减少屏幕输出\nlgr::get_logger(\"mlr3\")$set_threshold(\"warn\")\nlgr::get_logger(\"bbotk\")$set_threshold(\"warn\")\n\n# 开始运行\nbmr &lt;- benchmark(design,store_models = T) # 速度比tidymodels快很多\n\n#saveRDS(bmr,file = \"datasets/bmr.rds\")\nbmr"
  },
  {
    "objectID": "model-compare_mlr3.html#查看模型表现",
    "href": "model-compare_mlr3.html#查看模型表现",
    "title": "48  mlr3实现多模型比较",
    "section": "48.7 查看模型表现",
    "text": "48.7 查看模型表现\n查看结果，也是支持同时查看多个结果的：\n\n# 默认结果\nbmr$aggregate()\n##    nr   task_id   learner_id resampling_id iters classif.ce\n## 1:  1 all_plays randomForest            cv    10  0.2696791\n## 2:  2 all_plays     logistic            cv    10  0.2768839\n## 3:  3 all_plays decisionTree            cv    10  0.2801601\n## 4:  4 all_plays         kknn            cv    10  0.3227799\n## Hidden columns: resample_result\n\n\n# 查看多个结果\nmeasures &lt;- msrs(c(\"classif.auc\",\"classif.acc\",\"classif.bbrier\"))\n\nbmr_res &lt;- bmr$aggregate(measures)\nbmr_res[,c(4,7:9)]\n##      learner_id classif.auc classif.acc classif.bbrier\n## 1: randomForest   0.7983164   0.7303209      0.1789120\n## 2:     logistic   0.7801990   0.7231161      0.1864776\n## 3: decisionTree   0.7038680   0.7198399      0.2003211\n## 4:         kknn   0.7311780   0.6772201      0.2215549"
  },
  {
    "objectID": "model-compare_mlr3.html#结果可视化",
    "href": "model-compare_mlr3.html#结果可视化",
    "title": "48  mlr3实现多模型比较",
    "section": "48.8 结果可视化",
    "text": "48.8 结果可视化\n支持ggplot2语法，使用起来和tidymodels差不多，也是对结果直接autoplot()即可。\n\nlibrary(ggplot2)\nautoplot(bmr)+theme(axis.text.x = element_text(angle = 45))\n\n\n\n\n喜闻乐见的ROC曲线：\n\nautoplot(bmr,type = \"roc\")"
  },
  {
    "objectID": "model-compare_mlr3.html#选择最好的模型用于测试集",
    "href": "model-compare_mlr3.html#选择最好的模型用于测试集",
    "title": "48  mlr3实现多模型比较",
    "section": "48.9 选择最好的模型用于测试集",
    "text": "48.9 选择最好的模型用于测试集\n通过比较结果可以发现还是随机森林效果最好~，下面选择随机森林，在训练集上训练，在测试集上测试结果。\n这一步并没有使用10折交叉验证，如果你想用，也是可以的~\n\n# 训练\nrf_glr$train(task_train)\n\n训练好之后就是在测试集上测试并查看结果：\n\n# 测试\nprediction &lt;- rf_glr$predict(task_test)\nhead(as.data.table(prediction))\n##    row_ids truth response prob.pass  prob.run\n## 1:       3   run     pass 0.8161656 0.1838344\n## 2:       4   run     pass 0.7097711 0.2902289\n## 3:      10   run      run 0.3917451 0.6082549\n## 4:      12  pass     pass 0.5781400 0.4218600\n## 5:      19  pass     pass 0.5834923 0.4165077\n## 6:      22   run     pass 0.7099366 0.2900634\n\n混淆矩阵：\n\nprediction$confusion\n##         truth\n## response  pass   run\n##     pass 10570  3265\n##     run   3014  6145\n\n混淆矩阵可视化：\n\nautoplot(prediction)\n\n\n\n\n查看其他结果：\n\nprediction$score(msrs(c(\"classif.auc\",\"classif.acc\",\"classif.bbrier\")))\n##    classif.auc    classif.acc classif.bbrier \n##      0.7955435      0.7269288      0.1801018\n\n喜闻乐见ROC曲线：\n\nautoplot(prediction,type = \"roc\")\n\n\n\n\n简单吗？"
  },
  {
    "objectID": "model-compare_caret.html#数据划分",
    "href": "model-compare_caret.html#数据划分",
    "title": "49  caret实现多模型比较",
    "section": "49.1 数据划分",
    "text": "49.1 数据划分\n下面是一个分类数据的演示。\n\n# 使用的数据集\nlibrary(mlbench)\ndata(Sonar)\nstr(Sonar[, 1:10])\n## 'data.frame':    208 obs. of  10 variables:\n##  $ V1 : num  0.02 0.0453 0.0262 0.01 0.0762 0.0286 0.0317 0.0519 0.0223 0.0164 ...\n##  $ V2 : num  0.0371 0.0523 0.0582 0.0171 0.0666 0.0453 0.0956 0.0548 0.0375 0.0173 ...\n##  $ V3 : num  0.0428 0.0843 0.1099 0.0623 0.0481 ...\n##  $ V4 : num  0.0207 0.0689 0.1083 0.0205 0.0394 ...\n##  $ V5 : num  0.0954 0.1183 0.0974 0.0205 0.059 ...\n##  $ V6 : num  0.0986 0.2583 0.228 0.0368 0.0649 ...\n##  $ V7 : num  0.154 0.216 0.243 0.11 0.121 ...\n##  $ V8 : num  0.16 0.348 0.377 0.128 0.247 ...\n##  $ V9 : num  0.3109 0.3337 0.5598 0.0598 0.3564 ...\n##  $ V10: num  0.211 0.287 0.619 0.126 0.446 ...\n\n用caret包实现boosted tree模型。\n\n# 加载R包，划分数据集\nlibrary(caret)\n## Loading required package: ggplot2\n## Loading required package: lattice\n\n# 训练集、测试集划分，比例为0.75\nset.seed(998)\ninTraining &lt;- createDataPartition(Sonar$Class, p = .75, list = FALSE)\ntraining &lt;- Sonar[inTraining,]\ntesting  &lt;- Sonar[-inTraining,]"
  },
  {
    "objectID": "model-compare_caret.html#重抽样",
    "href": "model-compare_caret.html#重抽样",
    "title": "49  caret实现多模型比较",
    "section": "49.2 重抽样",
    "text": "49.2 重抽样\ntrainControl()选择重抽样方法。\n\n# 选择重抽样方法，重复10折交叉验证\nfitControl &lt;- trainControl(method = \"repeatedcv\", #默认是simple boost\n                           number = 10,\n                           repeats = 10,\n                           classProbs = T # 计算概率\n                           )\n\n# 借助 gbm 包实现 boosted tree\nset.seed(825)\ngbmFit1 &lt;- train(Class ~ ., \n                 data = training, \n                 method = \"gbm\", \n                 trControl = fitControl,\n                 verbose = FALSE)\ngbmFit1\n## Stochastic Gradient Boosting \n## \n## 157 samples\n##  60 predictor\n##   2 classes: 'M', 'R' \n## \n## No pre-processing\n## Resampling: Cross-Validated (10 fold, repeated 10 times) \n## Summary of sample sizes: 141, 142, 141, 142, 141, 142, ... \n## Resampling results across tuning parameters:\n## \n##   interaction.depth  n.trees  Accuracy   Kappa    \n##   1                   50      0.7935784  0.5797839\n##   1                  100      0.8171078  0.6290208\n##   1                  150      0.8219608  0.6383173\n##   2                   50      0.8041912  0.6027771\n##   2                  100      0.8296176  0.6544713\n##   2                  150      0.8283627  0.6520181\n##   3                   50      0.8110343  0.6170317\n##   3                  100      0.8301275  0.6551379\n##   3                  150      0.8310343  0.6577252\n## \n## Tuning parameter 'shrinkage' was held constant at a value of 0.1\n## \n## Tuning parameter 'n.minobsinnode' was held constant at a value of 10\n## Accuracy was used to select the optimal model using the largest value.\n## The final values used for the model were n.trees = 150, interaction.depth =\n##  3, shrinkage = 0.1 and n.minobsinnode = 10.\n\n结果很详细，就不做解释了。\n上面的例子也展示了caret包建模的基本语法，就是一个train()就可以了，method参数选择模型，trControl选择重抽样方法，preProcess选择数据预处理方法（上面这个例子没有进行数据预处理）。"
  },
  {
    "objectID": "model-compare_caret.html#超参数调整",
    "href": "model-compare_caret.html#超参数调整",
    "title": "49  caret实现多模型比较",
    "section": "49.3 超参数调整",
    "text": "49.3 超参数调整\n上面是交叉验证的例子，并没有加入超参数调优的环节，下面将加入超参数调优的过程。\n但是说实话caret虽然是一个整合包，但是对于每一种算法，它支持调整的超参数都很有限！\n\n# 网格搜索，首先设定超参数范围\ngbmGrid &lt;-  expand.grid(interaction.depth = c(1, 5, 9), \n                        n.trees = (1:30)*50, \n                        shrinkage = 0.1,\n                        n.minobsinnode = 20)\n\nnrow(gbmGrid)\n## [1] 90\nhead(gbmGrid)\n##   interaction.depth n.trees shrinkage n.minobsinnode\n## 1                 1      50       0.1             20\n## 2                 5      50       0.1             20\n## 3                 9      50       0.1             20\n## 4                 1     100       0.1             20\n## 5                 5     100       0.1             20\n## 6                 9     100       0.1             20\n\n\n# 设置种子数，进行建模\nset.seed(825)\ngbmFit2 &lt;- train(Class ~ ., \n                 data = training, \n                 method = \"gbm\", \n                 trControl = fitControl, \n                 verbose = FALSE, \n                 tuneGrid = gbmGrid # 设定网格范围\n                 )\ngbmFit2\n#saveRDS(gbmFit2,file = \"./datasets/gbmFit2.rds\")\n\n上面这个结果非常详细。\n除了网格搜索，还提供常见的其他方法，大家感兴趣的自己探索即可，我这里只是简单演示基本用法。\n在探索这个结果之前，让我们先看看caret强大的模型结果的可视化功能。\n\n# 展示不同参数下的模型性能\ntrellis.par.set(caretTheme())\nplot(gbmFit2)  \n\n\n\n\n\n# 更改性能指标\ntrellis.par.set(caretTheme())\nplot(gbmFit2, metric = \"Kappa\")\n\n\n\n\n\n#?plot.train 获取更多细节！\ntrellis.par.set(caretTheme())\nplot(gbmFit2, metric = \"Kappa\", plotType = \"level\",\n     scales = list(x = list(rot = 90)))\n\n\n\n\n\n# 支持ggplot2\nggplot(gbmFit2)+theme_bw()  # ?xyplot.train\n\n\n\n\ntrainControl()可用来选择重抽样方法，选择是否需要计算概率（分类数据）等，这个函数非常重要，可以使用?trainControl查看细节。\n\n# trainControl函数用来设置非常多的东西，很重要\nfitControl &lt;- trainControl(method = \"repeatedcv\",\n                           number = 10,\n                           repeats = 10,\n                           classProbs = TRUE, # 计算概率\n                           summaryFunction = twoClassSummary # 二分类变量指标\n                           )\n# 选择好之后开始调优\nset.seed(825)\ngbmFit3 &lt;- train(Class ~ ., \n                 data = training, \n                 method = \"gbm\", \n                 trControl = fitControl, \n                 verbose = FALSE, \n                 tuneGrid = gbmGrid,\n                 metric = \"ROC\" # 选择指标\n                 )\ngbmFit3\n#saveRDS(gbmFit3,file = \"./datasets/gbmFit3.rds\")\n\ntrain()函数中的metric参数可以指定调优的指标，默认分类模型是accuracy和Kappa，回归模型是RMSE/R^2/MAE。\ntrainControl()中的summaryFunction参数还提供了额外的调优指标选项，比如上面这个twoClassSummary，内含3种指标：敏感度、特异度、ROC。"
  },
  {
    "objectID": "model-compare_caret.html#选择最终模型",
    "href": "model-compare_caret.html#选择最终模型",
    "title": "49  caret实现多模型比较",
    "section": "49.4 选择最终模型",
    "text": "49.4 选择最终模型\n其实经过上面的train()训练后得到的gbmFit3就已经包含了我们最终的模型，如果你不需要额外的操作，那现在这个gbmFit3就可以直接用于测试集了。\n\npredict(gbmFit3, newdata = head(testing), type = \"prob\")\n##              M            R\n## 1 3.215213e-02 9.678479e-01\n## 2 1.000000e+00 3.965815e-08\n## 3 6.996088e-13 1.000000e+00\n## 4 9.070652e-01 9.293483e-02\n## 5 2.029754e-03 9.979702e-01\n## 6 9.999662e-01 3.377548e-05\npredict(gbmFit3, newdata = head(testing))\n## [1] R M R M R M\n## Levels: M R\n\n同时你也可以用$符号查看各种结果，比如：\n\ngbmFit3$finalModel # 最终模型\ngbmFit3$bestTune # 选择的超参数\ngbmFit3$results # 包含各种指标的详细结果\n\n除此之外，caret还提供了另外3个函数帮助你自定义选择最终的模型： - best:根据某一指标选择，选择使某个指标最大或最小的模型 - oneSE:使用1倍标准差法选择最终模型 - tolerance:根据某一指标选择最简单的模型\n下面是一个tolerance的演示，其他两个也是一样的使用方法：\n\nwhichTwoPct &lt;- tolerance(gbmFit3$results, \n                         metric = \"ROC\", \n                         tol = 2, # 这个参数是tolerance函数特有的\n                         maximize = TRUE) \n\ngbmFit3$results[whichTwoPct,1:6]\n##    shrinkage interaction.depth n.minobsinnode n.trees       ROC      Sens\n## 32       0.1                 5             20     100 0.9139707 0.8645833"
  },
  {
    "objectID": "model-compare_caret.html#应用于测试集",
    "href": "model-compare_caret.html#应用于测试集",
    "title": "49  caret实现多模型比较",
    "section": "49.5 应用于测试集",
    "text": "49.5 应用于测试集\n上面已经介绍过了，直接使用即可。\ncaret对predict()函数进行了优化，type=prob计算概率，type=class计算类别。tidymodels完整继承了这个优点。\n\npredict(gbmFit3, newdata = head(testing))\n## [1] R M R M R M\n## Levels: M R\npredict(gbmFit3, newdata = head(testing), type = \"prob\")\n##              M            R\n## 1 3.215213e-02 9.678479e-01\n## 2 1.000000e+00 3.965815e-08\n## 3 6.996088e-13 1.000000e+00\n## 4 9.070652e-01 9.293483e-02\n## 5 2.029754e-03 9.979702e-01\n## 6 9.999662e-01 3.377548e-05"
  },
  {
    "objectID": "model-compare_caret.html#多个模型的比较",
    "href": "model-compare_caret.html#多个模型的比较",
    "title": "49  caret实现多模型比较",
    "section": "49.6 多个模型的比较",
    "text": "49.6 多个模型的比较\n\n49.6.1 多建立几个模型\n\nset.seed(825)\nsvmFit &lt;- train(Class ~ ., \n                data = training, \n                method = \"svmRadial\", \n                trControl = fitControl, \n                preProc = c(\"center\", \"scale\"),\n                tuneLength = 8,\n                metric = \"ROC\")\n\nset.seed(825)\nrdaFit &lt;- train(Class ~ ., \n                data = training, \n                method = \"rda\", \n                trControl = fitControl, \n                tuneLength = 4,\n                metric = \"ROC\")\n#save(svmFit, rdaFit,file = \"./datasets/svm_rda_fit.rdata\")\n\n一起放入resamples()函数里面：\n\nresamps &lt;- resamples(list(GBM = gbmFit3,\n                          SVM = svmFit,\n                          RDA = rdaFit))\nresamps\n## \n## Call:\n## resamples.default(x = list(GBM = gbmFit3, SVM = svmFit, RDA = rdaFit))\n## \n## Models: GBM, SVM, RDA \n## Number of resamples: 100 \n## Performance metrics: ROC, Sens, Spec \n## Time estimates for: everything, final model fit\nsummary(resamps)\n## \n## Call:\n## summary.resamples(object = resamps)\n## \n## Models: GBM, SVM, RDA \n## Number of resamples: 100 \n## \n## ROC \n##          Min.  1st Qu.    Median      Mean   3rd Qu. Max. NA's\n## GBM 0.6964286 0.874504 0.9454365 0.9216468 0.9821429    1    0\n## SVM 0.7321429 0.905878 0.9464286 0.9339658 0.9821429    1    0\n## RDA 0.5625000 0.812500 0.8750000 0.8698115 0.9392361    1    0\n## \n## Sens \n##          Min.   1st Qu.    Median      Mean 3rd Qu. Max. NA's\n## GBM 0.5555556 0.7777778 0.8750000 0.8787500       1    1    0\n## SVM 0.5000000 0.7777778 0.8888889 0.8730556       1    1    0\n## RDA 0.4444444 0.7777778 0.8750000 0.8604167       1    1    0\n## \n## Spec \n##          Min.   1st Qu.    Median      Mean   3rd Qu. Max. NA's\n## GBM 0.4285714 0.7142857 0.8571429 0.8119643 1.0000000    1    0\n## SVM 0.4285714 0.7142857 0.8571429 0.8205357 0.9062500    1    0\n## RDA 0.1428571 0.5714286 0.7142857 0.6941071 0.8571429    1    0\n\n结果就很强！分别给出了3种指标下的每种模型的统计值。\n\n\n49.6.2 多个模型可视化\n喜闻乐见的结果可视化也是必不可少的。主要包括以下几种：density plots, box-whisker plots, scatterplot matrices and scatterplots\n\n# 设主题\ntheme1 &lt;- trellis.par.get()\ntheme1$plot.symbol$col = rgb(.2, .2, .2, .4)\ntheme1$plot.symbol$pch = 16\ntheme1$plot.line$col = rgb(1, 0, 0, .7)\ntheme1$plot.line$lwd &lt;- 2\n\n# 画图，箱线图\ntrellis.par.set(theme1)\nbwplot(resamps, layout = c(3, 1))\n\n\n\n\n\n# 密度图\ntrellis.par.set(theme1)\ndensityplot(resamps)\n\n\n\n\n\n# 换个指标，点线图\ntrellis.par.set(caretTheme())\ndotplot(resamps, metric = \"ROC\")\n\n\n\n\n\n# 散点图\ntrellis.par.set(theme1)\nxyplot(resamps, what = \"BlandAltman\")\n\n\n\n\n\n# 散点图矩阵\nsplom(resamps)\n\n\n\n\n\n\n49.6.3 多个模型的显著性检验\n除此之外，我们还可以对不同模型之间的差异进行显著性检验，比如t检验。\n\ndifValues &lt;- diff(resamps)\ndifValues\n## \n## Call:\n## diff.resamples(x = resamps)\n## \n## Models: GBM, SVM, RDA \n## Metrics: ROC, Sens, Spec \n## Number of differences: 3 \n## p-value adjustment: bonferroni\nsummary(difValues)\n## \n## Call:\n## summary.diff.resamples(object = difValues)\n## \n## p-value adjustment: bonferroni \n## Upper diagonal: estimates of the difference\n## Lower diagonal: p-value for H0: difference = 0\n## \n## ROC \n##     GBM       SVM       RDA     \n## GBM           -0.01232   0.05184\n## SVM 0.3408               0.06415\n## RDA 5.356e-07 2.638e-10         \n## \n## Sens \n##     GBM    SVM      RDA     \n## GBM        0.005694 0.018333\n## SVM 1.0000          0.012639\n## RDA 0.4253 1.0000           \n## \n## Spec \n##     GBM       SVM       RDA      \n## GBM           -0.008571  0.117857\n## SVM 1                    0.126429\n## RDA 8.230e-07 1.921e-10\n\n结果的可视化：\n\ntrellis.par.set(theme1)\nbwplot(difValues, layout = c(3, 1))\n\n\n\n\n\ntrellis.par.set(caretTheme())\ndotplot(difValues)\n\n\n\n\n是不是很强！"
  },
  {
    "objectID": "model-compare_caret.html#使用默认超参数",
    "href": "model-compare_caret.html#使用默认超参数",
    "title": "49  caret实现多模型比较",
    "section": "49.7 使用默认超参数",
    "text": "49.7 使用默认超参数\n直接提供超参数的值，这种情况不能进行重抽样，把超参数的值直接提供给tuneGrid参数即可。\n\nfitControl &lt;- trainControl(method = \"none\", classProbs = TRUE)\n\nset.seed(825)\ngbmFit4 &lt;- train(Class ~ ., \n                 data = training, \n                 method = \"gbm\", \n                 trControl = fitControl, \n                 verbose = FALSE, \n                 # 直接提供超参数的值\n                 tuneGrid = data.frame(interaction.depth = 4,\n                                       n.trees = 100,\n                                       shrinkage = .1,\n                                       n.minobsinnode = 20),\n                 metric = \"ROC\")\ngbmFit4\n\npredict(gbmFit4, newdata = head(testing))\npredict(gbmFit4, newdata = head(testing), type = \"prob\")\n\n以上就是caret典型使用的演示，更多的例子我们慢慢介绍，逐渐深入。"
  },
  {
    "objectID": "data-preprocess.html#加载r包和数据",
    "href": "data-preprocess.html#加载r包和数据",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.1 加载R包和数据",
    "text": "A.1 加载R包和数据\n\nlibrary(AppliedPredictiveModeling)\nlibrary(caret)\n## Loading required package: ggplot2\n## Loading required package: lattice\n\ndata(\"segmentationOriginal\")\n\nsegData &lt;- subset(segmentationOriginal, Case == \"Train\")\ncellID &lt;- segData$Cell\ncalss &lt;- segData$Class\ncase &lt;- segData$Case\nsegData &lt;- segData[ ,  -(1:3)]\nstatusColNum &lt;- grep(\"Status\", names(segData))\nstatusColNum\n##  [1]   2   4   9  10  11  12  14  16  20  21  22  26  27  28  30  32  34  36  38\n## [20]  40  43  44  46  48  51  52  55  56  59  60  63  64  68  69  70  72  73  74\n## [39]  76  78  80  82  84  86  88  92  93  94  97  98 103 104 105 106 110 111 112\n## [58] 114\n\nsegData &lt;- segData[ , -statusColNum]\n\nstr(segData)\n## 'data.frame':    1009 obs. of  58 variables:\n##  $ AngleCh1               : num  133.8 106.6 69.2 109.4 104.3 ...\n##  $ AreaCh1                : int  819 431 298 256 258 358 158 315 246 223 ...\n##  $ AvgIntenCh1            : num  31.9 28 19.5 18.8 17.6 ...\n##  $ AvgIntenCh2            : num  206 115 101 126 124 ...\n##  $ AvgIntenCh3            : num  69.9 63.9 28.2 13.6 22.5 ...\n##  $ AvgIntenCh4            : num  164.2 106.7 31 46.8 71.2 ...\n##  $ ConvexHullAreaRatioCh1 : num  1.26 1.05 1.2 1.08 1.08 ...\n##  $ ConvexHullPerimRatioCh1: num  0.797 0.935 0.866 0.92 0.931 ...\n##  $ DiffIntenDensityCh1    : num  31.9 32.5 26.7 28 27.9 ...\n##  $ DiffIntenDensityCh3    : num  43.1 36 22.9 14.9 16.1 ...\n##  $ DiffIntenDensityCh4    : num  79.3 51.4 26.4 32.7 36.2 ...\n##  $ EntropyIntenCh1        : num  6.09 5.88 5.42 5.38 5.18 ...\n##  $ EntropyIntenCh3        : num  6.64 6.68 5.44 4.15 5.49 ...\n##  $ EntropyIntenCh4        : num  7.88 7.14 5.78 6.19 6.62 ...\n##  $ EqCircDiamCh1          : num  32.3 23.4 19.5 18.1 18.2 ...\n##  $ EqEllipseLWRCh1        : num  1.56 1.38 3.39 1.38 1.62 ...\n##  $ EqEllipseOblateVolCh1  : num  2233 802 725 368 404 ...\n##  $ EqEllipseProlateVolCh1 : num  1433 583 214 267 250 ...\n##  $ EqSphereAreaCh1        : num  3279 1727 1195 1027 1036 ...\n##  $ EqSphereVolCh1         : num  17654 6751 3884 3096 3134 ...\n##  $ FiberAlign2Ch3         : num  0.488 0.301 0.22 0.364 0.359 ...\n##  $ FiberAlign2Ch4         : num  0.352 0.522 0.733 0.481 0.244 ...\n##  $ FiberLengthCh1         : num  64.3 21.1 43.1 22.3 26.5 ...\n##  $ FiberWidthCh1          : num  13.2 21.1 7.4 12.1 10.2 ...\n##  $ IntenCoocASMCh3        : num  0.02805 0.00686 0.03096 0.10816 0.01303 ...\n##  $ IntenCoocASMCh4        : num  0.01259 0.00614 0.01103 0.00995 0.00896 ...\n##  $ IntenCoocContrastCh3   : num  8.23 14.45 7.3 6.16 9.4 ...\n##  $ IntenCoocContrastCh4   : num  6.98 16.7 13.39 10.59 10.3 ...\n##  $ IntenCoocEntropyCh3    : num  6.82 7.58 6.31 5.04 6.96 ...\n##  $ IntenCoocEntropyCh4    : num  7.1 7.67 7.2 7.13 7.14 ...\n##  $ IntenCoocMaxCh3        : num  0.1532 0.0284 0.1628 0.3153 0.0739 ...\n##  $ IntenCoocMaxCh4        : num  0.0739 0.0232 0.0775 0.0586 0.0348 ...\n##  $ KurtIntenCh1           : num  -0.249 -0.293 0.626 -0.365 -0.556 ...\n##  $ KurtIntenCh3           : num  -0.331 1.051 0.128 1.083 -0.512 ...\n##  $ KurtIntenCh4           : num  -0.265 0.151 -0.347 -0.626 -0.647 ...\n##  $ LengthCh1              : num  47.2 28.1 37.9 23.1 26.3 ...\n##  $ NeighborAvgDistCh1     : num  174 158 206 264 231 ...\n##  $ NeighborMinDistCh1     : num  30.1 34.9 33.1 38.4 29.8 ...\n##  $ NeighborVarDistCh1     : num  81.4 90.4 116.9 88.5 103.5 ...\n##  $ PerimCh1               : num  154.9 84.6 101.1 68.7 73.4 ...\n##  $ ShapeBFRCh1            : num  0.54 0.724 0.589 0.635 0.557 ...\n##  $ ShapeLWRCh1            : num  1.47 1.33 2.83 1.31 1.49 ...\n##  $ ShapeP2ACh1            : num  2.26 1.27 2.55 1.4 1.59 ...\n##  $ SkewIntenCh1           : num  0.399 0.472 0.882 0.547 0.443 ...\n##  $ SkewIntenCh3           : num  0.62 0.971 1 1.432 0.556 ...\n##  $ SkewIntenCh4           : num  0.527 0.325 0.604 0.704 0.137 ...\n##  $ SpotFiberCountCh3      : int  4 2 4 0 1 1 4 2 2 2 ...\n##  $ SpotFiberCountCh4      : int  11 6 7 5 4 5 4 2 5 1 ...\n##  $ TotalIntenCh1          : int  24964 11552 5545 4613 4340 14461 4743 88725 136957 79885 ...\n##  $ TotalIntenCh2          : int  160997 47510 28869 30855 30719 74259 15434 148012 57421 62235 ...\n##  $ TotalIntenCh3          : int  54675 26344 8042 3332 5548 14474 6265 58224 20304 23878 ...\n##  $ TotalIntenCh4          : int  128368 43959 8843 11466 17588 23099 17534 120536 15482 98948 ...\n##  $ VarIntenCh1            : num  18.8 17.3 13.8 13.9 12.3 ...\n##  $ VarIntenCh3            : num  56.7 37.7 30 18.6 17.7 ...\n##  $ VarIntenCh4            : num  118.4 49.5 24.7 40.3 41.9 ...\n##  $ WidthCh1               : num  32.2 21.2 13.4 17.5 17.7 ...\n##  $ XCentroid              : int  215 371 487 211 172 276 239 95 438 386 ...\n##  $ YCentroid              : int  347 252 295 495 207 385 404 95 16 14 ..."
  },
  {
    "objectID": "data-preprocess.html#中心化和标准化",
    "href": "data-preprocess.html#中心化和标准化",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.2 中心化和标准化",
    "text": "A.2 中心化和标准化\n某些算法对预测变量是有要求的，比如需要预测变量具有相同的尺度，如果有的预测变量范围是0.10.2，但是有的却是1000020000，这种变量间的绝大差距会影像某些模型的稳定性，所以需要想办法把它们变成差不多的范围（有个专有名词：无量纲化）。\n中心化和标准化可以解决这样的问题。\n中心化是将所有变量减去其均值，其结果是变换后的变量均值为0；标准化是将每个变量除以其自身的标准差，标准化迫使变量的标准差为1。\nR语言中scale()函数可实现中心化和标准化，就不多做介绍了。"
  },
  {
    "objectID": "data-preprocess.html#偏度问题",
    "href": "data-preprocess.html#偏度问题",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.3 偏度问题",
    "text": "A.3 偏度问题\n无偏分布类似我们常说的正态分布，有偏分布又分为右偏和左偏，分别类似正偏态分布和负偏态分布。\n一个判断数据有偏的黄金标准：如果最大值与最小值的比例超过20，那么我们认为数据有偏。\n可以通过计算偏度统计量来衡量偏度。如果预测变量分布是大致对称的，那么偏度将接近于0，右偏分布偏度大于0，越大说明偏的越厉害；左偏分布偏度小于0，越小说明偏的越厉害。\n计算偏度的包很多。\n使用e1071包查看变量的偏度\n\nlibrary(e1071)\n# 查看偏度\nskewness(segData$AngleCh1)\n## [1] -0.02426252\n## [1] -0.02426252\n\n# 查看每一列的偏度\nskewValues &lt;- apply(segData, 2, skewness)\nhead(skewValues)\n##    AngleCh1     AreaCh1 AvgIntenCh1 AvgIntenCh2 AvgIntenCh3 AvgIntenCh4 \n## -0.02426252  3.52510745  2.95918524  0.84816033  2.20234214  1.90047128\n\n也可以通过psych包查看：\n\npsych::skew(segData$AngleCh1) # 偏度\n## [1] -0.02426252\n\npsych::kurtosi(segData$AngleCh1) # 峰度\n## [1] -0.8594789\n\n通过对数据进行变换可以一定程度解决偏度的问题，常用的方法有：取对数(log)，平方根，倒数，Box&Cox法等。\nlog、平方根、倒数这些很简单，就不演示了，下面演示下BoxCox变换。\n\n# 准备对数据进行BoxCox变换\nCh1AreaTrans &lt;- BoxCoxTrans(segData$AreaCh1)\nCh1AreaTrans\n## Box-Cox Transformation\n## \n## 1009 data points used to estimate Lambda\n## \n## Input data summary:\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   150.0   194.0   256.0   325.1   376.0  2186.0 \n## \n## Largest/Smallest: 14.6 \n## Sample Skewness: 3.53 \n## \n## Estimated Lambda: -0.9\n\n# 进行变换\nAreaCh1_transed &lt;- predict(Ch1AreaTrans, segData$AreaCh1)\n\n# 查看变换前、后的数据\nhead(segData$AreaCh1)\n## [1] 819 431 298 256 258 358\nhead(AreaCh1_transed)\n## [1] 1.108458 1.106383 1.104520 1.103554 1.103607 1.105523\n\n这里可以看到caret对数据预处理的方式，首先是选择方法，然后使用predict()函数把变换应用到具体的变量上。这是caret的基本操作，大家一定要记住！\n对于变换前后的数据变化，只看数字没有直观的感受，下面给大家画图演示。\n\n# 画图看变换前后\nopar &lt;- par(mfrow=c(1,2))\nhist(segData$AreaCh1)\nhist(AreaCh1_transed)\n\n\n\npar(opar)\n\n可以明显看到变换前是右偏分布，变换后基本接近无偏，可以再次计算偏度看看：\n\npsych::skew(AreaCh1_transed)\n## [1] 0.0976087\n\n下面是BoxCox变换的一点点扩展，不看也影响不大。\nBoxCox变换需要一个参数lambda，这个参数需要我们计算并指定，如上使用caret进行变换时，它会自动帮我们处理好，其中一句代码显示Estimated Lambda: -0.9，也就是lambda=0.9。\n还有很多R包可以实现BoxCox变换，其中比较简单的是forecast，简单演示如下：\n\nlibrary(forecast)\n## Registered S3 method overwritten by 'quantmod':\n##   method            from\n##   as.zoo.data.frame zoo\n\nbest.lambda &lt;- BoxCox.lambda(segData$AreaCh1) # 计算lambda\nbest.lambda\n## [1] -0.9999264\n\nAreaCh1.transformed &lt;- BoxCox(segData$AreaCh1, lambda = best.lambda) # 变换\nhead(AreaCh1.transformed)\n## [1] 0.9988519 0.9977522 0.9967163 0.9961655 0.9961958 0.9972789\n\ny0 &lt;- InvBoxCox(AreaCh1.transformed,lambda=best.lambda) # 还原"
  },
  {
    "objectID": "data-preprocess.html#解决离群值",
    "href": "data-preprocess.html#解决离群值",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.4 解决离群值",
    "text": "A.4 解决离群值\n离群值其实是有明确定义的，通常我们会选择直接删除离群值，但是还是要根据实际情况来看，有的离群值是非常有意义的，这样的离群值不能直接删除。\n\n有的离群值可能是数据录入时不小心输错了，比如错把收缩压132mmHg录成了 -132mmHg，只需要改正即可；\n在样本量较小时，不宜直接删除离群值，有的离群值可能是因为数据来自一个明显有偏的分布，只是因为我们的样本量太小无法观测到这个偏度；\n有些离群值可能来自一个特殊的子集，只是这个子集才刚开始被收集到。\n\n有些模型对离群值很敏感，比如线性模型，这样是需要处理的，一个常见的方法是空间表示变换，该变换将预测变量取值映射到高纬的球上，它会把所有样本变换到离球心相等的球面上。在caret中可以实现。关于它的具体数学运算过程，感兴趣的自己了解即可，我不太感兴趣。\n在进行空间表示变换前，最好先进行中心化和标准化，这也和它的数学计算有关，我也不太感兴趣。\n\n# 变换前的图形\ndata(mdrr)\ntransparentTheme(trans = .4)\n\nplotSubset &lt;- data.frame(scale(mdrrDescr[, c(\"nC\", \"X4v\")])) \nxyplot(nC ~ X4v,\n       data = plotSubset,\n       groups = mdrrClass, \n       auto.key = list(columns = 2))\n\n\n\n\n\n# 变换后的图形\ntransformed &lt;- spatialSign(plotSubset)\ntransformed &lt;- as.data.frame(transformed)\nxyplot(nC ~ X4v, \n       data = transformed, \n       groups = mdrrClass, \n       auto.key = list(columns = 2)) \n\n\n\n\n是不是很神奇？"
  },
  {
    "objectID": "data-preprocess.html#降维和特征提取",
    "href": "data-preprocess.html#降维和特征提取",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.5 降维和特征提取",
    "text": "A.5 降维和特征提取\n有很多方法，比如PCA, ICA, PLS, UMAP等，最流行的还是PCA，主要是它给出的主成分是彼此不相关的，这恰好符合一些模型的需求。\n对数据进行PCA变换之前，最好先解决偏度问题，然后进行中心化和标准化，和它的数学计算过程有关，感兴趣的自己了解。\n可视化前后不同：\n\n# 主成分分析，可参考我之前的推文\npr &lt;- prcomp(~ AvgIntenCh1 + EntropyIntenCh1, \n             data = segData, \n             scale. = TRUE)\n\n# 可视化前后图形\nlibrary(ggplot2)\n\np1 &lt;- ggplot(segData, aes(AvgIntenCh1,EntropyIntenCh1))+\n  geom_point()+\n  labs(x=\"Channel 1 Fiber Width\",y=\"Intensity Entropy Channel 1\")+\n  theme_bw()\np2 &lt;- ggplot(as.data.frame(pr$x), aes(PC1,PC2))+\n  geom_point()+\n  theme_bw()\ncowplot::plot_grid(p1,p2)"
  },
  {
    "objectID": "data-preprocess.html#处理缺失值",
    "href": "data-preprocess.html#处理缺失值",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.6 处理缺失值",
    "text": "A.6 处理缺失值\n处理缺失值主要有两种方法，直接删除或者进行插补，使用哪种方法应取决于对数据的理解！\n一些常见的缺失值处理方法可以参考我之前的推文：我常用的缺失值插补方法"
  },
  {
    "objectID": "data-preprocess.html#过滤",
    "href": "data-preprocess.html#过滤",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.7 过滤",
    "text": "A.7 过滤\n这里的过滤和解决共线性，其实部分属于特征选择的范围，就是大家常见的自变量选择问题，这个问题在以后的推文中还会详细介绍。\n\n冗余的变量通常增加了模型的复杂度而非信息量\n\n主要是过滤两种变量：(近)零方差变量和高度相关变量。\n如果一个变量只有1个值，那么这个变量的方差为0；如果一个变量只有少量不重复的取值，这种变量称为近零方差变量；这2种变量包含的信息太少了，应当过滤；\n检测近零方差变量的准则是：\n\n不重复取值的数目与样本量的比值低（比如10%）；\n最高频数和次高频数的比值高（比如20%）\n\n如果两个变量相关性太高，那么它们携带的信息可能很多是重叠的，会对某些模型产生较大的影响，应当解决。\n移除共线变量的方法如下：\n\n计算预测变量的相关系数矩阵\n找出相关系数绝对值最大的那对预测变量（记为变量A和B）\n分别计算A和B和其他预测变量的相关系数\n如果A的平均相关系数更大，移除A，否则移除B\n重复步骤2-4，直至所有相关系数的绝对值都低于设定的阈值\n\ncaret可以轻松实现以上过程。\n使用mdrr数据集演示。其中一列nR11大部分都是501，这种变量方差是很小的！\n\ndata(mdrr)\ntable(mdrrDescr$nR11) # 大部分值都是0\n## \n##   0   1   2 \n## 501   4  23\n\nsd(mdrrDescr$nR11)^2 # 方差很小！\n## [1] 0.1731787\n\n使用nearZeroVar()找出零方差和近零方差变量，结果中会给出zeroVar和nzv两列，用逻辑值表示是不是近零方差变量或者零方差变量。\n\nnzv &lt;- nearZeroVar(mdrrDescr, saveMetrics= TRUE)\nnzv[nzv$nzv,][1:10,]\n##        freqRatio percentUnique zeroVar  nzv\n## nTB     23.00000     0.3787879   FALSE TRUE\n## nBR    131.00000     0.3787879   FALSE TRUE\n## nI     527.00000     0.3787879   FALSE TRUE\n## nR03   527.00000     0.3787879   FALSE TRUE\n## nR08   527.00000     0.3787879   FALSE TRUE\n## nR11    21.78261     0.5681818   FALSE TRUE\n## nR12    57.66667     0.3787879   FALSE TRUE\n## D.Dr03 527.00000     0.3787879   FALSE TRUE\n## D.Dr07 123.50000     5.8712121   FALSE TRUE\n## D.Dr08 527.00000     0.3787879   FALSE TRUE\n\n去掉近零方差变量：\n\ndim(mdrrDescr)\n## [1] 528 342\n\nnzv &lt;- nearZeroVar(mdrrDescr)\nfilteredDescr &lt;- mdrrDescr[, -nzv]\ndim(filteredDescr)\n## [1] 528 297\n\n下面是处理高度相关的变量。\n\n# 相关系数矩阵\ncorrelations &lt;- cor(segData)\ndim(correlations)\n## [1] 58 58\n\n# 可视化相关系数矩阵，中间几个颜色深的就是高度相关的变量\nlibrary(corrplot)\n## corrplot 0.92 loaded\ncorrplot(correlations, order = \"hclust\",tl.col = \"black\")\n\n\n\n\n去掉高度相关的变量：\n\n# 阈值设为0.75\nhighCorr &lt;- findCorrelation(correlations, cutoff = 0.75)\nlength(highCorr)\n## [1] 32\nhead(highCorr)\n## [1] 23 40 43 36  7 15\n\n# 去掉高度相关的变量\nfilteredSegData &lt;- segData[, -highCorr]"
  },
  {
    "objectID": "data-preprocess.html#共线性",
    "href": "data-preprocess.html#共线性",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.8 共线性",
    "text": "A.8 共线性\n假设一个下面这种的数据，其中第2列和第3列的值加起来和第1列一样，第4,5,6列的值起来也和第1列一样。这种数据的某些变量间是有高度共线性的。\n\nltfrDesign &lt;- matrix(0, nrow=6, ncol=6)\nltfrDesign[,1] &lt;- c(1, 1, 1, 1, 1, 1)\nltfrDesign[,2] &lt;- c(1, 1, 1, 0, 0, 0)\nltfrDesign[,3] &lt;- c(0, 0, 0, 1, 1, 1)\nltfrDesign[,4] &lt;- c(1, 0, 0, 1, 0, 0)\nltfrDesign[,5] &lt;- c(0, 1, 0, 0, 1, 0)\nltfrDesign[,6] &lt;- c(0, 0, 1, 0, 0, 1)\n\nltfrDesign\n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]    1    1    0    1    0    0\n## [2,]    1    1    0    0    1    0\n## [3,]    1    1    0    0    0    1\n## [4,]    1    0    1    1    0    0\n## [5,]    1    0    1    0    1    0\n## [6,]    1    0    1    0    0    1\n\nfindLinearCombos()可以通过算法给出需要去除的变量，关于具体的方法可以官网查看。\n\ncomboInfo &lt;- findLinearCombos(ltfrDesign)\ncomboInfo\n## $linearCombos\n## $linearCombos[[1]]\n## [1] 3 1 2\n## \n## $linearCombos[[2]]\n## [1] 6 1 4 5\n## \n## \n## $remove\n## [1] 3 6\n\n结果给出了需要去除的变量是第3列和第6列。\n\n# 去除第3列和第6列\nltfrDesign[, -comboInfo$remove]\n##      [,1] [,2] [,3] [,4]\n## [1,]    1    1    1    0\n## [2,]    1    1    0    1\n## [3,]    1    1    0    0\n## [4,]    1    0    1    0\n## [5,]    1    0    0    1\n## [6,]    1    0    0    0"
  },
  {
    "objectID": "data-preprocess.html#构建虚拟变量",
    "href": "data-preprocess.html#构建虚拟变量",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.9 构建虚拟变量",
    "text": "A.9 构建虚拟变量\n最常见的回归分析中的哑变量设置，可以参考之前的推文，详细介绍了常见的分类变量的编码方式：分类变量进行回归分析时的编码方案\n这里介绍下独热编码（one-hot encoding），和哑变量编码稍有不同，哑变量是变成k-1个变量，独热编码是变成k个变量。\n使用以下数据进行演示\n\ndata(\"cars\", package = \"caret\")\nhead(cars)\n##      Price Mileage Cylinder Doors Cruise Sound Leather Buick Cadillac Chevy\n## 1 22661.05   20105        6     4      1     0       0     1        0     0\n## 2 21725.01   13457        6     2      1     1       0     0        0     1\n## 3 29142.71   31655        4     2      1     1       1     0        0     0\n## 4 30731.94   22479        4     2      1     0       0     0        0     0\n## 5 33358.77   17590        4     2      1     1       1     0        0     0\n## 6 30315.17   23635        4     2      1     0       0     0        0     0\n##   Pontiac Saab Saturn convertible coupe hatchback sedan wagon\n## 1       0    0      0           0     0         0     1     0\n## 2       0    0      0           0     1         0     0     0\n## 3       0    1      0           1     0         0     0     0\n## 4       0    1      0           1     0         0     0     0\n## 5       0    1      0           1     0         0     0     0\n## 6       0    1      0           1     0         0     0     0\n\ntype &lt;- c(\"convertible\", \"coupe\", \"hatchback\", \"sedan\", \"wagon\")\ncars$Type &lt;- factor(apply(cars[, 14:18], 1, function(x) type[which(x == 1)]))\n\ncarSubset &lt;- cars[sample(1:nrow(cars), 20), c(1, 2, 19)]\n\n# 上面是数据生成过程，不重要，记住下面这个数据的样子即可！！\nhead(carSubset)\n##        Price Mileage        Type\n## 249 12553.07   32844       coupe\n## 37  35580.33   21167 convertible\n## 745 43374.05   25199       sedan\n## 331 11179.95   23121   hatchback\n## 266 10777.05   27906   hatchback\n## 155 15253.87   20917       coupe\nlevels(carSubset$Type) # Type是一个因子型变量\n## [1] \"convertible\" \"coupe\"       \"hatchback\"   \"sedan\"       \"wagon\"\n\n现在把Type这个变量进行独热编码。\n使用dummyVars构建虚拟变量：\n\nsimpleMod &lt;- dummyVars(~Mileage + Type, # 用mileage和Type对价格进行预测\n                       data = carSubset,\n                       levelsOnly = TRUE) # 从列名中移除因子变量的名称\nsimpleMod\n## Dummy Variable Object\n## \n## Formula: ~Mileage + Type\n## 2 variables, 1 factors\n## Factor variable names will be removed\n## A less than full rank encoding is used\n\n接下来就可以使用predict和simpleMod对训练集进行生成虚拟变量的操作了：\n\npredict(simpleMod, head(carSubset))\n##     Mileage convertible coupe hatchback sedan wagon\n## 249   32844           0     1         0     0     0\n## 37    21167           1     0         0     0     0\n## 745   25199           0     0         0     1     0\n## 331   23121           0     0         1     0     0\n## 266   27906           0     0         1     0     0\n## 155   20917           0     1         0     0     0\n\n可以看到Type变量没有了，完成了虚拟变量的转换。\n假如你认为车型和里程有交互影响，则可以使用:表示：\n\nwithInteraction &lt;- dummyVars(~Mileage + Type + Mileage:Type,\n                             data = carSubset,\n                             levelsOnly = TRUE)\nwithInteraction\n## Dummy Variable Object\n## \n## Formula: ~Mileage + Type + Mileage:Type\n## 2 variables, 1 factors\n## Factor variable names will be removed\n## A less than full rank encoding is used\n\n应用于新的数据集：\n\npredict(withInteraction, head(carSubset))\n##     Mileage convertible coupe hatchback sedan wagon Mileage:Typeconvertible\n## 249   32844           0     1         0     0     0                       0\n## 37    21167           1     0         0     0     0                   21167\n## 745   25199           0     0         0     1     0                       0\n## 331   23121           0     0         1     0     0                       0\n## 266   27906           0     0         1     0     0                       0\n## 155   20917           0     1         0     0     0                       0\n##     Mileage:Typecoupe Mileage:Typehatchback Mileage:Typesedan Mileage:Typewagon\n## 249             32844                     0                 0                 0\n## 37                  0                     0                 0                 0\n## 745                 0                     0             25199                 0\n## 331                 0                 23121                 0                 0\n## 266                 0                 27906                 0                 0\n## 155             20917                     0                 0                 0"
  },
  {
    "objectID": "data-preprocess.html#区间化预测变量",
    "href": "data-preprocess.html#区间化预测变量",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.10 区间化预测变量",
    "text": "A.10 区间化预测变量\n主要是为了好解释结果，比如把血压分为高血压1级、2级、3级，把贫血分为轻中重极重等，这样比如你做logistic回归，可以说血压每增高一个等级，因变量的风险增加多少，但是你如果说血压值每增加1mmHg，因变量增加多少倍，这就有点扯了。"
  },
  {
    "objectID": "data-preprocess.html#多个预处理步骤放一起",
    "href": "data-preprocess.html#多个预处理步骤放一起",
    "title": "附录 A — 常见的数据预处理方法",
    "section": "A.11 多个预处理步骤放一起",
    "text": "A.11 多个预处理步骤放一起\n在caret中是通过preProcess()函数里面的method参数实现的，把不同的预处理步骤按照顺序写好即可。\n\nlibrary(AppliedPredictiveModeling)\ndata(schedulingData)\nstr(schedulingData)\n## 'data.frame':    4331 obs. of  8 variables:\n##  $ Protocol   : Factor w/ 14 levels \"A\",\"C\",\"D\",\"E\",..: 4 4 4 4 4 4 4 4 4 4 ...\n##  $ Compounds  : num  997 97 101 93 100 100 105 98 101 95 ...\n##  $ InputFields: num  137 103 75 76 82 82 88 95 91 92 ...\n##  $ Iterations : num  20 20 10 20 20 20 20 20 20 20 ...\n##  $ NumPending : num  0 0 0 0 0 0 0 0 0 0 ...\n##  $ Hour       : num  14 13.8 13.8 10.1 10.4 ...\n##  $ Day        : Factor w/ 7 levels \"Mon\",\"Tue\",\"Wed\",..: 2 2 4 5 5 3 5 5 5 3 ...\n##  $ Class      : Factor w/ 4 levels \"VF\",\"F\",\"M\",\"L\": 2 1 1 1 1 1 1 1 1 1 ...\n\n# 中心化、标准化、YeoJohnson变换\npp_hpc &lt;- preProcess(schedulingData[, -8], \n                     method = c(\"center\", \"scale\", \"YeoJohnson\"))\npp_hpc\n## Created from 4331 samples and 7 variables\n## \n## Pre-processing:\n##   - centered (5)\n##   - ignored (2)\n##   - scaled (5)\n##   - Yeo-Johnson transformation (5)\n## \n## Lambda estimates for Yeo-Johnson transformation:\n## -0.08, -0.03, -1.05, -1.1, 1.44\n\n# 应用于数据\ntransformed &lt;- predict(pp_hpc, newdata = schedulingData[, -8])\nhead(transformed)\n##   Protocol  Compounds InputFields Iterations NumPending         Hour Day\n## 1        E  1.2289592  -0.6324580 -0.0615593  -0.554123  0.004586516 Tue\n## 2        E -0.6065826  -0.8120473 -0.0615593  -0.554123 -0.043733201 Tue\n## 3        E -0.5719534  -1.0131504 -2.7894869  -0.554123 -0.034967177 Thu\n## 4        E -0.6427737  -1.0047277 -0.0615593  -0.554123 -0.964170752 Fri\n## 5        E -0.5804713  -0.9564504 -0.0615593  -0.554123 -0.902085020 Fri\n## 6        E -0.5804713  -0.9564504 -0.0615593  -0.554123  0.698108782 Wed\n\nmean(schedulingData$NumPending == 0)\n## [1] 0.7561764\n\n# 进行中心化、标准化、YeoJohnson、nzv\npp_no_nzv &lt;- preProcess(schedulingData[, -8], \n                        method = c(\"center\", \"scale\", \"YeoJohnson\", \"nzv\"))\npp_no_nzv\n## Created from 4331 samples and 7 variables\n## \n## Pre-processing:\n##   - centered (4)\n##   - ignored (2)\n##   - removed (1)\n##   - scaled (4)\n##   - Yeo-Johnson transformation (4)\n## \n## Lambda estimates for Yeo-Johnson transformation:\n## -0.08, -0.03, -1.05, 1.44\n\npredict(pp_no_nzv, newdata = schedulingData[1:6, -8])\n##   Protocol  Compounds InputFields Iterations         Hour Day\n## 1        E  1.2289592  -0.6324580 -0.0615593  0.004586516 Tue\n## 2        E -0.6065826  -0.8120473 -0.0615593 -0.043733201 Tue\n## 3        E -0.5719534  -1.0131504 -2.7894869 -0.034967177 Thu\n## 4        E -0.6427737  -1.0047277 -0.0615593 -0.964170752 Fri\n## 5        E -0.5804713  -0.9564504 -0.0615593 -0.902085020 Fri\n## 6        E -0.5804713  -0.9564504 -0.0615593  0.698108782 Wed\n\n如果你用过tidymodels，那你应该知道里面的数据预处理步骤是通过recipes包完成的，每一步都是step_xx，说实话我觉得caret的这种方式更加简洁易懂！\n以上就是数据预处理的一般过程，一个caret包可以解决上面所有的问题，有兴趣的小伙伴可以自行学习。\n数据预处理是一个非常系统且专业的过程，如同开头说的那样：最有效的编码数据的方法来自于建模者对数据的理解，而不是通过任何数学方法，在对数据进行预处理之前，一定要仔细理解自己的数据哦，结果导向的思维是不对的哦！\n本文简单介绍了常见的数据预处理方法和简单的实现方法，目前在R中实现数据预处理是非常方便的，这部分内容可参考：\n\nmlr3数据预处理\ntidymodels菜谱：数据预处理\nR语言机器学习caret-02：数据预处理"
  },
  {
    "objectID": "data-split.html#留出法holdout",
    "href": "data-split.html#留出法holdout",
    "title": "附录 B — 常见的数据划分方法",
    "section": "B.1 留出法(holdout)",
    "text": "B.1 留出法(holdout)\n大家最常使用的，把数据集随机划分为训练集(train)/测试集(test)的做法就是holdout，其中训练集用于建模，测试集用于评估模型表现。\n有时还会把数据划分为训练集(train)/测试集(test)/验证集(validation)，训练集用来训练模型，测试集查看模型表现，不断进行调整，然后训练集和测试集一起训练出一个模型，最后用验证集评估模型表现。"
  },
  {
    "objectID": "data-split.html#交叉验证cross-validation",
    "href": "data-split.html#交叉验证cross-validation",
    "title": "附录 B — 常见的数据划分方法",
    "section": "B.2 交叉验证(cross validation)",
    "text": "B.2 交叉验证(cross validation)\n交叉验证，意思就是一份数据既用作训练，也用作验证，互相交叉，主要有以下几种：\nK折交叉验证(K fold cross validation)，就是把数据集随机分为K个样本量基本相同的子数据集。比如5折交叉验证，就是把数据集分为5个子集（比如分成A,B,C,D,E,5份），在建模时，首先会使用其中A,B,C,D,4份数据进行建模，然后用剩下的E数据评估模型表现，接下来使用A,B,C,E，4份数据建模，用剩下的D评估模型表现。这样依次进行5个循环，每份数据都会用来评估模型表现。最后将得到的5个模型表现结果进行汇总。\n下面是一个10折交叉验证的示意图：\n\n留一交叉验证(LOOCV, leave one out cross validation)，是K折交叉验证的特例。每次都只留1个样本用于评估模型表现，所以这里的K其实就等于样本量，每一个样本都会被用来评估模型表现。\n重复交叉验证(repeated cross validation)，也是K折交叉验证的扩展版本，比如，重复10次的5折交叉验证，就是把5折交叉验证这个过程重复10遍。\n蒙特卡洛交叉验证(Monte Carlo cross validation)，也是交叉验证的一个变种。留出法是将数据集划分1次，而蒙特卡洛交叉验证就是将留出法进行多次。"
  },
  {
    "objectID": "data-split.html#bootstrap",
    "href": "data-split.html#bootstrap",
    "title": "附录 B — 常见的数据划分方法",
    "section": "B.3 bootstrap",
    "text": "B.3 bootstrap\n自助法，即有放回的随机抽样法。具体做法如下：\n比如，一个数据集有100个样本，每次随机抽取1个，然后放回，再随机抽取1个，这样的过程重复100次，就得到了一个和原数据集样本量相等的抽样数据集，这个抽样数据集就叫做自助集。\n由于每次都是有放回然后再随机抽取，所以一个自助集中可能有多个同一样本！所以就有可能在100次随机抽取中，有一些没被抽中过的样本，这些样本就被称为袋外样本(out of bag)，其中被抽中的样本(也就是自助集)用于训练模型，袋外样本用来评估模型表现。随机森林算法就是使用这种方法的！"
  },
  {
    "objectID": "data-split.html#其他方法",
    "href": "data-split.html#其他方法",
    "title": "附录 B — 常见的数据划分方法",
    "section": "B.4 其他方法",
    "text": "B.4 其他方法\n除了以上方法，其实还有非常多没有介绍，比如在mlr3中经常使用的嵌套重抽样，这些大家感兴趣可以自行了解。"
  },
  {
    "objectID": "data-split.html#重抽样的目的",
    "href": "data-split.html#重抽样的目的",
    "title": "附录 B — 常见的数据划分方法",
    "section": "B.5 重抽样的目的",
    "text": "B.5 重抽样的目的\n经常有粉丝问我：为什么我用了各种方法，10折交叉验证、10折重复交叉验证、自助法，都用过了，为什么最后模型的表现还是很差？\n看到类似的问题，我想这部分朋友可能把重抽样的目的搞错了，重抽样的目的不是为了提高模型表现，重抽样也确实不能提高模型表现！开头我已说过，重抽样技术是为了让模型更好的认识数据而已，这样能够得到更加稳健、无偏的结果，但是对于提高模型表现没有直接的影响哦~\n你可以这么理解，如果你不重抽样，可能某次结果的AUC是0.8，再做一次可能就变成0.5了，而你重抽样10次，得到的结果是10次的平均，这样的结果很明显是更加稳健的。\n模型表现好不好首先是数据原因，一个牛逼的数据不需要复杂的模型也能有很好的结果，数据预处理对数据影响很大，大家可以参考上面的推文。另外还和模型本身的性质有关，比如模型的超参数、模型本身的上限等，这些会在以后陆续介绍。"
  },
  {
    "objectID": "data-split.html#为什么要单独划分出一部分数据",
    "href": "data-split.html#为什么要单独划分出一部分数据",
    "title": "附录 B — 常见的数据划分方法",
    "section": "B.6 为什么要单独划分出一部分数据",
    "text": "B.6 为什么要单独划分出一部分数据\n通常我们建立模型时，会把数据集A划分为A1和A2两份，A1用来训练模型，A2用来测试模型，在训练模型的过程中，完全不用使用到A2这部分数据。有些人不理解，把这种方法和嵌套重抽样混为一谈。其实这两个有着本质的区别。\n嵌套重抽样是在训练模型时使用的，把两份数据集全都用到了，而且两份数据集都会再叠加其他重抽样方法。\n但我们划分数据的目的是什么呢？我们是为了测试最终的模型表现。临床问题数据很珍贵，通常都只有1份，这种情况下我把这份数据全都用于训练模型，那我用什么测试训练出来的模型好坏呢？有的人喜欢把训练好的模型作用于用来训练模型的数据上，发现结果竟然很好，这样是不对的，这叫数据泄露，你的数据模型已经学习过了，这不是作弊吗？这样的模型结果能说明什么问题呢？\n所以一开始把数据就划分为2份是一个很好的解决方法。如果你有很多个数据集，你完全可以在其中1个数据集中使用各种方法建模。"
  },
  {
    "objectID": "data-split.html#方法选择建议",
    "href": "data-split.html#方法选择建议",
    "title": "附录 B — 常见的数据划分方法",
    "section": "B.7 方法选择建议",
    "text": "B.7 方法选择建议\n以上就是一些常见的重抽样方法，可以看到每种方法都强调一个问题，那就是随机！，只有随机，才能保证模型学习到这个数据集中的更多信息，才能获得稳健的模型表现！\n以下是一些方法选择建议：\n\n没有哪一种方法好，哪一种方法不好！！只有合不合适，没有好不好！\n如果样本量较小，建议选择重复10折交叉验证；\n如果样本量足够大，比如几万，几十万这种，随便选，都可以；\n如果目的不是得到最好的模型表现，而是为了在不同模型间进行选择，建议使用bootstrap；\n如果还不知道怎么选，建议都试一试，喜欢哪个选哪个"
  },
  {
    "objectID": "9999-appendix.html#r语言rtoolsrstudio的安装",
    "href": "9999-appendix.html#r语言rtoolsrstudio的安装",
    "title": "附录 C — 其他合集",
    "section": "C.1 R语言、Rtools、Rstudio的安装",
    "text": "C.1 R语言、Rtools、Rstudio的安装\n\n公众号推文：可能是最适合小白的R语言和R包安装教程\nb站播放量超7w的视频教程：适合小白的R语言和Rstudio安装教程"
  },
  {
    "objectID": "9999-appendix.html#r包安装",
    "href": "9999-appendix.html#r包安装",
    "title": "附录 C — 其他合集",
    "section": "C.2 R包安装",
    "text": "C.2 R包安装\n\n公众号推文：可能是最好的R包安装教程\nb站播放量超7w的视频教程：可能是最好用的R包安装教程"
  },
  {
    "objectID": "9999-appendix.html#医学统计",
    "href": "9999-appendix.html#医学统计",
    "title": "附录 C — 其他合集",
    "section": "C.3 医学统计",
    "text": "C.3 医学统计\n\nR语言实战医学统计合集：R语言实战医学统计\n在线版电子书：R语言实战医学统计\ngithub地址：R语言实战医学统计"
  },
  {
    "objectID": "9999-appendix.html#机器学习",
    "href": "9999-appendix.html#机器学习",
    "title": "附录 C — 其他合集",
    "section": "C.4 机器学习",
    "text": "C.4 机器学习\n临床预测模型和机器学习的联系越来越紧密，没有明确的界限，目前来看各种机器学习方法已经充斥在预测建模类文章中，所以大家有必要学习一些机器学习的相关知识了。\n医学和生信笔记后台回复caret即可获取caret包的合集教程；回复tidymodels即可获取tidymodels的合集教程；回复mlr3即可获取mlr3合集教程，回复机器学习即可获取机器学习推文合集。\nR语言机器学习合集：R语言机器学习\n在线版电子书敬请期待，即将上线！\n本号很少涉及理论知识，主要还是R语言实战，所以理论部分大家需要自己多多学习。"
  },
  {
    "objectID": "9999-appendix.html#生信数据挖掘",
    "href": "9999-appendix.html#生信数据挖掘",
    "title": "附录 C — 其他合集",
    "section": "C.5 生信数据挖掘",
    "text": "C.5 生信数据挖掘\n生信数据挖掘合集：生信数据挖掘\n医学和生信笔记公众号所有关于生信数据挖掘的推文都可以免费下载使用，请看：“灌水”生信类文章会用到哪些生信下游分析？（附下载地址）\ngithub地址：R语言生信数据挖掘"
  },
  {
    "objectID": "9999-appendix.html#扫码关注",
    "href": "9999-appendix.html#扫码关注",
    "title": "附录 C — 其他合集",
    "section": "C.6 扫码关注",
    "text": "C.6 扫码关注\n欢迎扫码关注：医学和生信笔记"
  }
]